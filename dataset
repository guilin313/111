    writer = SummaryWriter(log_dir=os.path.join(training_args.output_dir, "tb_samples"))
    log_dataset_samples(train_dataset, writer, tag_prefix="train", num_samples=5)
    log_dataset_samples(val_dataset, writer, tag_prefix="val", num_samples=3)
    writer.close()
    logger.info("✅ Wrote sample slices to TensorBoard.")

def log_dataset_samples(dataset, writer, tag_prefix="train", num_samples=5):
    for idx in range(min(num_samples, len(dataset))):
        data = dataset[idx]["pixel_values"]  # shape: [1, D, H, W]
        image = data[0, data.shape[1] // 2]   # 取中间层 [H, W]
        writer.add_image(f"{tag_prefix}_slice_{idx}", image.unsqueeze(0), global_step=0)



import os
import shutil
import random

# 设置路径
src_dir = 'path/to/your/tif_images'  # 原始文件夹路径，存放所有 .tif 图像
train_dir = 'path/to/save/train'  # 存储训练集的文件夹
val_dir = 'path/to/save/val'  # 存储验证集的文件夹

# 创建训练集和验证集的目录
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)

# 获取所有 .tif 文件
tif_files = [f for f in os.listdir(src_dir) if f.endswith('.tif')]

# 打乱文件顺序
random.shuffle(tif_files)

# 划分训练集和验证集
split_ratio = 0.9
train_size = int(len(tif_files) * split_ratio)

train_files = tif_files[:train_size]
val_files = tif_files[train_size:]

# 复制训练集文件
for file in train_files:
    src_path = os.path.join(src_dir, file)
    dst_path = os.path.join(train_dir, file)
    shutil.copy(src_path, dst_path)

# 复制验证集文件
for file in val_files:
    src_path = os.path.join(src_dir, file)
    dst_path = os.path.join(val_dir, file)
    shutil.copy(src_path, dst_path)

print(f"训练集包含 {len(train_files)} 张图片")
print(f"验证集包含 {len(val_files)} 张图片")





import os
import torch
import random
import numpy as np
import tifffile as tiff
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader

class TIFDataset(Dataset):
    def __init__(self, data_dir, augment=True):
        self.data_dir = data_dir
        self.file_list = sorted(os.listdir(data_dir))
        self.augment = augment

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        file_path = os.path.join(self.data_dir, self.file_list[idx])
        image = tiff.imread(file_path)  # (D, H, W)
        
        # 归一化到 [0,1]
        image = image.astype('float32') / 255.0  

        # 3D 预处理
        image = self.preprocess(image)

        # 转换为 Tensor
        image = torch.tensor(image).unsqueeze(0)  # [C=1, D, H, W]
        return image

    def preprocess(self, image):
        """ 数据预处理：标准化 & 数据增强 """
        # 标准化（减均值，除以标准差）
        image = (image - np.mean(image)) / (np.std(image) + 1e-5)

        if self.augment:
            image = self.augment_data(image)
        
        return image

    def augment_data(self, image):
        """ 数据增强：随机翻转、旋转、高斯噪声 """
        if random.random() > 0.5:
            image = np.flip(image, axis=1)  # 左右翻转
        if random.random() > 0.5:
            image = np.flip(image, axis=2)  # 上下翻转
        if random.random() > 0.5:
            image = np.flip(image, axis=0)  # 深度翻转

        # 旋转 90/180/270 度
        if random.random() > 0.5:
            k = random.choice([1, 2, 3])
            image = np.rot90(image, k=k, axes=(1, 2))  

        # 添加高斯噪声
        if random.random() > 0.7:
            noise = np.random.normal(0, 0.01, image.shape)
            image = np.clip(image + noise, 0, 1)

        return image

def get_dataloader(data_dir, batch_size=4, augment=True):
    dataset = TIFDataset(data_dir, augment=augment)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)


import os
import zipfile
import tarfile
import gzip
import shutil

def extract_archive(archive_path, output_dir):
    try:
        if archive_path.endswith(".zip"):
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
        elif archive_path.endswith((".tar.gz", ".tgz", ".tar")):
            with tarfile.open(archive_path, 'r:*') as tar_ref:
                tar_ref.extractall(output_dir)
        elif archive_path.endswith(".gz") and not archive_path.endswith(".tar.gz"):
            with gzip.open(archive_path, 'rb') as f_in:
                out_path = os.path.join(output_dir, os.path.basename(archive_path)[:-3])
                with open(out_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        else:
            print(f"Unsupported format: {archive_path}")
    except Exception as e:
        print(f"❌ Failed to extract {archive_path}: {e}")

def extract_all_in_dir(target_dir):
    for fname in os.listdir(target_dir):
        fpath = os.path.join(target_dir, fname)
        if not os.path.isfile(fpath):
            continue

        name, ext = os.path.splitext(fname)
        # Handle .tar.gz and .tgz specially
        if fname.endswith(".tar.gz") or fname.endswith(".tgz"):
            name = fname.rsplit('.', 2)[0]
        elif fname.endswith(".tar"):
            name = fname.rsplit('.', 1)[0]
        elif fname.endswith(".gz") and not fname.endswith(".tar.gz"):
            name = fname.rsplit('.', 1)[0]
        elif fname.endswith(".zip"):
            name = fname.rsplit('.', 1)[0]
        else:
            continue

        out_dir = os.path.join(target_dir, name)
        os.makedirs(out_dir, exist_ok=True)
        print(f"Extracting {fname} to {out_dir}...")
        extract_archive(fpath, out_dir)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("dir", help="Path to directory containing archives")
    args = parser.parse_args()

    extract_all_in_dir(args.dir)
