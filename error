{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.508440105755542,
  "eval_steps": 10000,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000508440105755542,
      "grad_norm": 0.18319441378116608,
      "learning_rate": 1.5624920556233477e-05,
      "loss": 1.1219,
      "step": 10
    },
    {
      "epoch": 0.001016880211511084,
      "grad_norm": 0.1554599404335022,
      "learning_rate": 1.5624841112466953e-05,
      "loss": 1.0854,
      "step": 20
    },
    {
      "epoch": 0.001525320317266626,
      "grad_norm": 0.1413077861070633,
      "learning_rate": 1.562476166870043e-05,
      "loss": 1.0636,
      "step": 30
    },
    {
      "epoch": 0.002033760423022168,
      "grad_norm": 0.12589207291603088,
      "learning_rate": 1.5624682224933903e-05,
      "loss": 1.0468,
      "step": 40
    },
    {
      "epoch": 0.00254220052877771,
      "grad_norm": 0.10337094217538834,
      "learning_rate": 1.562460278116738e-05,
      "loss": 1.0339,
      "step": 50
    },
    {
      "epoch": 0.003050640634533252,
      "grad_norm": 0.116363525390625,
      "learning_rate": 1.5624523337400855e-05,
      "loss": 1.0247,
      "step": 60
    },
    {
      "epoch": 0.003559080740288794,
      "grad_norm": 0.07577262073755264,
      "learning_rate": 1.562444389363433e-05,
      "loss": 1.0178,
      "step": 70
    },
    {
      "epoch": 0.004067520846044336,
      "grad_norm": 0.06819525361061096,
      "learning_rate": 1.5624364449867808e-05,
      "loss": 1.013,
      "step": 80
    },
    {
      "epoch": 0.004575960951799878,
      "grad_norm": 0.057117220014333725,
      "learning_rate": 1.562428500610128e-05,
      "loss": 1.0093,
      "step": 90
    },
    {
      "epoch": 0.00508440105755542,
      "grad_norm": 0.050229594111442566,
      "learning_rate": 1.5624205562334757e-05,
      "loss": 1.0073,
      "step": 100
    },
    {
      "epoch": 0.005592841163310962,
      "grad_norm": 0.055162761360406876,
      "learning_rate": 1.5624126118568234e-05,
      "loss": 1.0051,
      "step": 110
    },
    {
      "epoch": 0.006101281269066504,
      "grad_norm": 0.0377536416053772,
      "learning_rate": 1.562404667480171e-05,
      "loss": 1.0041,
      "step": 120
    },
    {
      "epoch": 0.006609721374822046,
      "grad_norm": 0.028559468686580658,
      "learning_rate": 1.5623967231035187e-05,
      "loss": 1.0031,
      "step": 130
    },
    {
      "epoch": 0.007118161480577588,
      "grad_norm": 0.04965333640575409,
      "learning_rate": 1.562388778726866e-05,
      "loss": 1.0025,
      "step": 140
    },
    {
      "epoch": 0.00762660158633313,
      "grad_norm": 0.05176849663257599,
      "learning_rate": 1.5623808343502136e-05,
      "loss": 1.002,
      "step": 150
    },
    {
      "epoch": 0.008135041692088672,
      "grad_norm": 0.02785893902182579,
      "learning_rate": 1.5623728899735612e-05,
      "loss": 1.0016,
      "step": 160
    },
    {
      "epoch": 0.008643481797844214,
      "grad_norm": 0.08040662109851837,
      "learning_rate": 1.562364945596909e-05,
      "loss": 1.0012,
      "step": 170
    },
    {
      "epoch": 0.009151921903599756,
      "grad_norm": 0.054910995066165924,
      "learning_rate": 1.5623570012202562e-05,
      "loss": 1.0007,
      "step": 180
    },
    {
      "epoch": 0.009660362009355298,
      "grad_norm": 0.07873998582363129,
      "learning_rate": 1.5623490568436038e-05,
      "loss": 1.0014,
      "step": 190
    },
    {
      "epoch": 0.01016880211511084,
      "grad_norm": 0.06173648685216904,
      "learning_rate": 1.5623411124669515e-05,
      "loss": 1.001,
      "step": 200
    },
    {
      "epoch": 0.010677242220866382,
      "grad_norm": 0.03272593766450882,
      "learning_rate": 1.562333168090299e-05,
      "loss": 1.0008,
      "step": 210
    },
    {
      "epoch": 0.011185682326621925,
      "grad_norm": 0.02639615908265114,
      "learning_rate": 1.5623252237136467e-05,
      "loss": 1.0009,
      "step": 220
    },
    {
      "epoch": 0.011694122432377467,
      "grad_norm": 0.06583397090435028,
      "learning_rate": 1.562317279336994e-05,
      "loss": 1.0007,
      "step": 230
    },
    {
      "epoch": 0.012202562538133009,
      "grad_norm": 0.03503900766372681,
      "learning_rate": 1.5623093349603417e-05,
      "loss": 1.0005,
      "step": 240
    },
    {
      "epoch": 0.01271100264388855,
      "grad_norm": 0.04276684299111366,
      "learning_rate": 1.5623013905836893e-05,
      "loss": 1.0007,
      "step": 250
    },
    {
      "epoch": 0.013219442749644091,
      "grad_norm": 0.03045549802482128,
      "learning_rate": 1.562293446207037e-05,
      "loss": 1.0007,
      "step": 260
    },
    {
      "epoch": 0.013727882855399633,
      "grad_norm": 0.018303649500012398,
      "learning_rate": 1.5622855018303846e-05,
      "loss": 1.0006,
      "step": 270
    },
    {
      "epoch": 0.014236322961155175,
      "grad_norm": 0.04732902720570564,
      "learning_rate": 1.562277557453732e-05,
      "loss": 1.0006,
      "step": 280
    },
    {
      "epoch": 0.014744763066910718,
      "grad_norm": 0.0315338633954525,
      "learning_rate": 1.5622696130770795e-05,
      "loss": 1.0004,
      "step": 290
    },
    {
      "epoch": 0.01525320317266626,
      "grad_norm": 0.04287277162075043,
      "learning_rate": 1.5622616687004272e-05,
      "loss": 1.0004,
      "step": 300
    },
    {
      "epoch": 0.015761643278421802,
      "grad_norm": 0.04445658624172211,
      "learning_rate": 1.5622537243237748e-05,
      "loss": 1.0003,
      "step": 310
    },
    {
      "epoch": 0.016270083384177344,
      "grad_norm": 0.03519778326153755,
      "learning_rate": 1.5622457799471225e-05,
      "loss": 0.9998,
      "step": 320
    },
    {
      "epoch": 0.016778523489932886,
      "grad_norm": 0.055503569543361664,
      "learning_rate": 1.5622378355704698e-05,
      "loss": 1.0012,
      "step": 330
    },
    {
      "epoch": 0.017286963595688428,
      "grad_norm": 0.03952288627624512,
      "learning_rate": 1.5622298911938174e-05,
      "loss": 1.0005,
      "step": 340
    },
    {
      "epoch": 0.01779540370144397,
      "grad_norm": 0.07893437147140503,
      "learning_rate": 1.562221946817165e-05,
      "loss": 1.0006,
      "step": 350
    },
    {
      "epoch": 0.018303843807199512,
      "grad_norm": 0.075856052339077,
      "learning_rate": 1.5622140024405127e-05,
      "loss": 1.0001,
      "step": 360
    },
    {
      "epoch": 0.018812283912955054,
      "grad_norm": 0.09683562070131302,
      "learning_rate": 1.5622060580638603e-05,
      "loss": 1.0002,
      "step": 370
    },
    {
      "epoch": 0.019320724018710596,
      "grad_norm": 0.023046504706144333,
      "learning_rate": 1.5621981136872076e-05,
      "loss": 1.0007,
      "step": 380
    },
    {
      "epoch": 0.01982916412446614,
      "grad_norm": 0.03330911695957184,
      "learning_rate": 1.5621901693105553e-05,
      "loss": 0.9999,
      "step": 390
    },
    {
      "epoch": 0.02033760423022168,
      "grad_norm": 0.047676924616098404,
      "learning_rate": 1.562182224933903e-05,
      "loss": 1.0002,
      "step": 400
    },
    {
      "epoch": 0.020846044335977223,
      "grad_norm": 0.04139643907546997,
      "learning_rate": 1.5621742805572505e-05,
      "loss": 1.0002,
      "step": 410
    },
    {
      "epoch": 0.021354484441732765,
      "grad_norm": 0.05772620439529419,
      "learning_rate": 1.562166336180598e-05,
      "loss": 0.9997,
      "step": 420
    },
    {
      "epoch": 0.021862924547488307,
      "grad_norm": 0.06686712056398392,
      "learning_rate": 1.5621583918039455e-05,
      "loss": 1.0002,
      "step": 430
    },
    {
      "epoch": 0.02237136465324385,
      "grad_norm": 0.05251016095280647,
      "learning_rate": 1.562150447427293e-05,
      "loss": 1.0001,
      "step": 440
    },
    {
      "epoch": 0.02287980475899939,
      "grad_norm": 0.026928720995783806,
      "learning_rate": 1.5621425030506408e-05,
      "loss": 1.0002,
      "step": 450
    },
    {
      "epoch": 0.023388244864754933,
      "grad_norm": 0.055071666836738586,
      "learning_rate": 1.5621345586739884e-05,
      "loss": 1.0001,
      "step": 460
    },
    {
      "epoch": 0.023896684970510475,
      "grad_norm": 0.06976375728845596,
      "learning_rate": 1.5621266142973357e-05,
      "loss": 0.9995,
      "step": 470
    },
    {
      "epoch": 0.024405125076266018,
      "grad_norm": 0.04821143299341202,
      "learning_rate": 1.5621186699206833e-05,
      "loss": 0.9995,
      "step": 480
    },
    {
      "epoch": 0.024913565182021556,
      "grad_norm": 0.0435267873108387,
      "learning_rate": 1.562110725544031e-05,
      "loss": 0.9999,
      "step": 490
    },
    {
      "epoch": 0.0254220052877771,
      "grad_norm": 0.017657700926065445,
      "learning_rate": 1.5621027811673786e-05,
      "loss": 1.0001,
      "step": 500
    },
    {
      "epoch": 0.02593044539353264,
      "grad_norm": 0.03706173598766327,
      "learning_rate": 1.5620948367907263e-05,
      "loss": 0.9997,
      "step": 510
    },
    {
      "epoch": 0.026438885499288182,
      "grad_norm": 0.027868986129760742,
      "learning_rate": 1.5620868924140736e-05,
      "loss": 0.9999,
      "step": 520
    },
    {
      "epoch": 0.026947325605043725,
      "grad_norm": 0.05958469957113266,
      "learning_rate": 1.5620789480374212e-05,
      "loss": 0.9996,
      "step": 530
    },
    {
      "epoch": 0.027455765710799267,
      "grad_norm": 0.06125790998339653,
      "learning_rate": 1.562071003660769e-05,
      "loss": 0.9998,
      "step": 540
    },
    {
      "epoch": 0.02796420581655481,
      "grad_norm": 0.06712452322244644,
      "learning_rate": 1.5620630592841165e-05,
      "loss": 0.9997,
      "step": 550
    },
    {
      "epoch": 0.02847264592231035,
      "grad_norm": 0.025419099256396294,
      "learning_rate": 1.562055114907464e-05,
      "loss": 0.9995,
      "step": 560
    },
    {
      "epoch": 0.028981086028065893,
      "grad_norm": 0.06695831567049026,
      "learning_rate": 1.5620471705308114e-05,
      "loss": 0.999,
      "step": 570
    },
    {
      "epoch": 0.029489526133821435,
      "grad_norm": 0.05364920198917389,
      "learning_rate": 1.562039226154159e-05,
      "loss": 0.9996,
      "step": 580
    },
    {
      "epoch": 0.029997966239576977,
      "grad_norm": 0.06098681688308716,
      "learning_rate": 1.5620312817775067e-05,
      "loss": 0.9993,
      "step": 590
    },
    {
      "epoch": 0.03050640634533252,
      "grad_norm": 0.029488664120435715,
      "learning_rate": 1.5620233374008543e-05,
      "loss": 0.9992,
      "step": 600
    },
    {
      "epoch": 0.03101484645108806,
      "grad_norm": 0.05065024271607399,
      "learning_rate": 1.562015393024202e-05,
      "loss": 0.9985,
      "step": 610
    },
    {
      "epoch": 0.031523286556843604,
      "grad_norm": 0.05037124827504158,
      "learning_rate": 1.5620074486475493e-05,
      "loss": 0.9997,
      "step": 620
    },
    {
      "epoch": 0.032031726662599146,
      "grad_norm": 0.0762765035033226,
      "learning_rate": 1.561999504270897e-05,
      "loss": 0.9991,
      "step": 630
    },
    {
      "epoch": 0.03254016676835469,
      "grad_norm": 0.02794456109404564,
      "learning_rate": 1.5619915598942445e-05,
      "loss": 0.9991,
      "step": 640
    },
    {
      "epoch": 0.03304860687411023,
      "grad_norm": 0.03379435837268829,
      "learning_rate": 1.5619836155175922e-05,
      "loss": 0.9991,
      "step": 650
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 0.04549172520637512,
      "learning_rate": 1.5619756711409395e-05,
      "loss": 0.9993,
      "step": 660
    },
    {
      "epoch": 0.034065487085621314,
      "grad_norm": 0.04203125089406967,
      "learning_rate": 1.561967726764287e-05,
      "loss": 0.9996,
      "step": 670
    },
    {
      "epoch": 0.034573927191376856,
      "grad_norm": 0.04685080796480179,
      "learning_rate": 1.5619597823876348e-05,
      "loss": 0.9991,
      "step": 680
    },
    {
      "epoch": 0.0350823672971324,
      "grad_norm": 0.029320936650037766,
      "learning_rate": 1.5619518380109824e-05,
      "loss": 0.9993,
      "step": 690
    },
    {
      "epoch": 0.03559080740288794,
      "grad_norm": 0.03141147643327713,
      "learning_rate": 1.56194389363433e-05,
      "loss": 0.9996,
      "step": 700
    },
    {
      "epoch": 0.03609924750864348,
      "grad_norm": 0.019124630838632584,
      "learning_rate": 1.5619359492576773e-05,
      "loss": 0.9999,
      "step": 710
    },
    {
      "epoch": 0.036607687614399025,
      "grad_norm": 0.08810347318649292,
      "learning_rate": 1.561928004881025e-05,
      "loss": 0.9991,
      "step": 720
    },
    {
      "epoch": 0.03711612772015457,
      "grad_norm": 0.10911790281534195,
      "learning_rate": 1.5619200605043726e-05,
      "loss": 0.998,
      "step": 730
    },
    {
      "epoch": 0.03762456782591011,
      "grad_norm": 0.0482732430100441,
      "learning_rate": 1.5619121161277203e-05,
      "loss": 0.9991,
      "step": 740
    },
    {
      "epoch": 0.03813300793166565,
      "grad_norm": 0.05589986965060234,
      "learning_rate": 1.561904171751068e-05,
      "loss": 0.9988,
      "step": 750
    },
    {
      "epoch": 0.03864144803742119,
      "grad_norm": 0.08036225289106369,
      "learning_rate": 1.5618962273744152e-05,
      "loss": 0.9978,
      "step": 760
    },
    {
      "epoch": 0.039149888143176735,
      "grad_norm": 0.045052412897348404,
      "learning_rate": 1.561888282997763e-05,
      "loss": 0.9969,
      "step": 770
    },
    {
      "epoch": 0.03965832824893228,
      "grad_norm": 0.05300574004650116,
      "learning_rate": 1.5618803386211105e-05,
      "loss": 0.9994,
      "step": 780
    },
    {
      "epoch": 0.04016676835468782,
      "grad_norm": 0.051199041306972504,
      "learning_rate": 1.561872394244458e-05,
      "loss": 0.9995,
      "step": 790
    },
    {
      "epoch": 0.04067520846044336,
      "grad_norm": 0.021401038393378258,
      "learning_rate": 1.5618644498678058e-05,
      "loss": 0.9989,
      "step": 800
    },
    {
      "epoch": 0.041183648566198904,
      "grad_norm": 0.01916988007724285,
      "learning_rate": 1.561856505491153e-05,
      "loss": 0.999,
      "step": 810
    },
    {
      "epoch": 0.041692088671954446,
      "grad_norm": 0.021684683859348297,
      "learning_rate": 1.5618485611145007e-05,
      "loss": 0.9992,
      "step": 820
    },
    {
      "epoch": 0.04220052877770999,
      "grad_norm": 0.039386898279190063,
      "learning_rate": 1.5618406167378483e-05,
      "loss": 0.9984,
      "step": 830
    },
    {
      "epoch": 0.04270896888346553,
      "grad_norm": 0.044424869120121,
      "learning_rate": 1.561832672361196e-05,
      "loss": 0.9985,
      "step": 840
    },
    {
      "epoch": 0.04321740898922107,
      "grad_norm": 0.032167915254831314,
      "learning_rate": 1.5618247279845436e-05,
      "loss": 0.9975,
      "step": 850
    },
    {
      "epoch": 0.043725849094976614,
      "grad_norm": 0.057846974581480026,
      "learning_rate": 1.561816783607891e-05,
      "loss": 0.9979,
      "step": 860
    },
    {
      "epoch": 0.044234289200732156,
      "grad_norm": 0.04326024278998375,
      "learning_rate": 1.5618088392312386e-05,
      "loss": 0.998,
      "step": 870
    },
    {
      "epoch": 0.0447427293064877,
      "grad_norm": 0.0483582504093647,
      "learning_rate": 1.5618008948545862e-05,
      "loss": 0.9994,
      "step": 880
    },
    {
      "epoch": 0.04525116941224324,
      "grad_norm": 0.0236026793718338,
      "learning_rate": 1.561792950477934e-05,
      "loss": 0.999,
      "step": 890
    },
    {
      "epoch": 0.04575960951799878,
      "grad_norm": 0.02511686459183693,
      "learning_rate": 1.561785006101281e-05,
      "loss": 0.9984,
      "step": 900
    },
    {
      "epoch": 0.046268049623754325,
      "grad_norm": 0.04741789773106575,
      "learning_rate": 1.5617770617246288e-05,
      "loss": 0.9988,
      "step": 910
    },
    {
      "epoch": 0.04677648972950987,
      "grad_norm": 0.041504934430122375,
      "learning_rate": 1.5617691173479764e-05,
      "loss": 0.9976,
      "step": 920
    },
    {
      "epoch": 0.04728492983526541,
      "grad_norm": 0.03274652734398842,
      "learning_rate": 1.561761172971324e-05,
      "loss": 0.998,
      "step": 930
    },
    {
      "epoch": 0.04779336994102095,
      "grad_norm": 0.11878038197755814,
      "learning_rate": 1.5617532285946717e-05,
      "loss": 0.9975,
      "step": 940
    },
    {
      "epoch": 0.04830181004677649,
      "grad_norm": 0.07596366852521896,
      "learning_rate": 1.561745284218019e-05,
      "loss": 0.9987,
      "step": 950
    },
    {
      "epoch": 0.048810250152532035,
      "grad_norm": 0.035732563585042953,
      "learning_rate": 1.5617373398413666e-05,
      "loss": 0.999,
      "step": 960
    },
    {
      "epoch": 0.04931869025828758,
      "grad_norm": 0.03934929892420769,
      "learning_rate": 1.5617293954647143e-05,
      "loss": 0.9983,
      "step": 970
    },
    {
      "epoch": 0.04982713036404311,
      "grad_norm": 0.06061021238565445,
      "learning_rate": 1.561721451088062e-05,
      "loss": 0.9985,
      "step": 980
    },
    {
      "epoch": 0.050335570469798654,
      "grad_norm": 0.0285806767642498,
      "learning_rate": 1.5617135067114096e-05,
      "loss": 0.9988,
      "step": 990
    },
    {
      "epoch": 0.0508440105755542,
      "grad_norm": 0.05205449089407921,
      "learning_rate": 1.561705562334757e-05,
      "loss": 0.9986,
      "step": 1000
    },
    {
      "epoch": 0.05135245068130974,
      "grad_norm": 0.050315290689468384,
      "learning_rate": 1.5616976179581045e-05,
      "loss": 0.9988,
      "step": 1010
    },
    {
      "epoch": 0.05186089078706528,
      "grad_norm": 0.033929385244846344,
      "learning_rate": 1.561689673581452e-05,
      "loss": 0.9982,
      "step": 1020
    },
    {
      "epoch": 0.05236933089282082,
      "grad_norm": 0.032284218817949295,
      "learning_rate": 1.5616817292047998e-05,
      "loss": 0.997,
      "step": 1030
    },
    {
      "epoch": 0.052877770998576365,
      "grad_norm": 0.03033476136624813,
      "learning_rate": 1.5616737848281474e-05,
      "loss": 0.9991,
      "step": 1040
    },
    {
      "epoch": 0.05338621110433191,
      "grad_norm": 0.037737179547548294,
      "learning_rate": 1.5616658404514947e-05,
      "loss": 0.9981,
      "step": 1050
    },
    {
      "epoch": 0.05389465121008745,
      "grad_norm": 0.014073565602302551,
      "learning_rate": 1.5616578960748424e-05,
      "loss": 0.9973,
      "step": 1060
    },
    {
      "epoch": 0.05440309131584299,
      "grad_norm": 0.03455691412091255,
      "learning_rate": 1.56164995169819e-05,
      "loss": 0.9975,
      "step": 1070
    },
    {
      "epoch": 0.05491153142159853,
      "grad_norm": 0.01606861874461174,
      "learning_rate": 1.5616420073215376e-05,
      "loss": 0.9983,
      "step": 1080
    },
    {
      "epoch": 0.055419971527354076,
      "grad_norm": 0.05892089754343033,
      "learning_rate": 1.561634062944885e-05,
      "loss": 0.9971,
      "step": 1090
    },
    {
      "epoch": 0.05592841163310962,
      "grad_norm": 0.025681588798761368,
      "learning_rate": 1.5616261185682326e-05,
      "loss": 0.9981,
      "step": 1100
    },
    {
      "epoch": 0.05643685173886516,
      "grad_norm": 0.04259573668241501,
      "learning_rate": 1.5616181741915802e-05,
      "loss": 0.9976,
      "step": 1110
    },
    {
      "epoch": 0.0569452918446207,
      "grad_norm": 0.04765443503856659,
      "learning_rate": 1.561610229814928e-05,
      "loss": 0.9982,
      "step": 1120
    },
    {
      "epoch": 0.057453731950376244,
      "grad_norm": 0.05025593936443329,
      "learning_rate": 1.5616022854382755e-05,
      "loss": 0.9955,
      "step": 1130
    },
    {
      "epoch": 0.057962172056131786,
      "grad_norm": 0.08397333323955536,
      "learning_rate": 1.5615943410616228e-05,
      "loss": 0.9963,
      "step": 1140
    },
    {
      "epoch": 0.05847061216188733,
      "grad_norm": 0.038954172283411026,
      "learning_rate": 1.5615863966849704e-05,
      "loss": 0.9976,
      "step": 1150
    },
    {
      "epoch": 0.05897905226764287,
      "grad_norm": 0.035256847739219666,
      "learning_rate": 1.561578452308318e-05,
      "loss": 0.9981,
      "step": 1160
    },
    {
      "epoch": 0.05948749237339841,
      "grad_norm": 0.040112487971782684,
      "learning_rate": 1.5615705079316657e-05,
      "loss": 0.9938,
      "step": 1170
    },
    {
      "epoch": 0.059995932479153954,
      "grad_norm": 0.032988063991069794,
      "learning_rate": 1.5615625635550133e-05,
      "loss": 0.9981,
      "step": 1180
    },
    {
      "epoch": 0.0605043725849095,
      "grad_norm": 0.03645140677690506,
      "learning_rate": 1.5615546191783606e-05,
      "loss": 0.9977,
      "step": 1190
    },
    {
      "epoch": 0.06101281269066504,
      "grad_norm": 0.03710491582751274,
      "learning_rate": 1.5615466748017083e-05,
      "loss": 0.9979,
      "step": 1200
    },
    {
      "epoch": 0.06152125279642058,
      "grad_norm": 0.026789642870426178,
      "learning_rate": 1.561538730425056e-05,
      "loss": 0.9987,
      "step": 1210
    },
    {
      "epoch": 0.06202969290217612,
      "grad_norm": 0.019968880340456963,
      "learning_rate": 1.5615307860484036e-05,
      "loss": 0.9974,
      "step": 1220
    },
    {
      "epoch": 0.06253813300793166,
      "grad_norm": 0.024777701124548912,
      "learning_rate": 1.5615228416717512e-05,
      "loss": 0.9958,
      "step": 1230
    },
    {
      "epoch": 0.06304657311368721,
      "grad_norm": 0.05049775540828705,
      "learning_rate": 1.5615148972950985e-05,
      "loss": 0.9957,
      "step": 1240
    },
    {
      "epoch": 0.06355501321944275,
      "grad_norm": 0.04413950443267822,
      "learning_rate": 1.561506952918446e-05,
      "loss": 0.9967,
      "step": 1250
    },
    {
      "epoch": 0.06406345332519829,
      "grad_norm": 0.04118805751204491,
      "learning_rate": 1.5614990085417938e-05,
      "loss": 0.9968,
      "step": 1260
    },
    {
      "epoch": 0.06457189343095383,
      "grad_norm": 0.046197738498449326,
      "learning_rate": 1.5614910641651414e-05,
      "loss": 0.9956,
      "step": 1270
    },
    {
      "epoch": 0.06508033353670938,
      "grad_norm": 0.028561847284436226,
      "learning_rate": 1.561483119788489e-05,
      "loss": 0.9958,
      "step": 1280
    },
    {
      "epoch": 0.06558877364246492,
      "grad_norm": 0.030348317697644234,
      "learning_rate": 1.5614751754118364e-05,
      "loss": 0.9969,
      "step": 1290
    },
    {
      "epoch": 0.06609721374822046,
      "grad_norm": 0.07145331799983978,
      "learning_rate": 1.561467231035184e-05,
      "loss": 0.9956,
      "step": 1300
    },
    {
      "epoch": 0.066605653853976,
      "grad_norm": 0.054726872593164444,
      "learning_rate": 1.5614592866585316e-05,
      "loss": 0.9972,
      "step": 1310
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 0.03633132576942444,
      "learning_rate": 1.5614513422818793e-05,
      "loss": 0.9954,
      "step": 1320
    },
    {
      "epoch": 0.06762253406548709,
      "grad_norm": 0.042668718844652176,
      "learning_rate": 1.5614433979052266e-05,
      "loss": 0.9935,
      "step": 1330
    },
    {
      "epoch": 0.06813097417124263,
      "grad_norm": 0.033127304166555405,
      "learning_rate": 1.5614354535285742e-05,
      "loss": 0.9948,
      "step": 1340
    },
    {
      "epoch": 0.06863941427699817,
      "grad_norm": 0.04021894931793213,
      "learning_rate": 1.561427509151922e-05,
      "loss": 0.9978,
      "step": 1350
    },
    {
      "epoch": 0.06914785438275371,
      "grad_norm": 0.14243480563163757,
      "learning_rate": 1.5614195647752695e-05,
      "loss": 0.9969,
      "step": 1360
    },
    {
      "epoch": 0.06965629448850925,
      "grad_norm": 0.028305577114224434,
      "learning_rate": 1.561411620398617e-05,
      "loss": 0.9974,
      "step": 1370
    },
    {
      "epoch": 0.0701647345942648,
      "grad_norm": 0.03548452630639076,
      "learning_rate": 1.5614036760219644e-05,
      "loss": 0.9965,
      "step": 1380
    },
    {
      "epoch": 0.07067317470002034,
      "grad_norm": 0.043501898646354675,
      "learning_rate": 1.561395731645312e-05,
      "loss": 0.9946,
      "step": 1390
    },
    {
      "epoch": 0.07118161480577588,
      "grad_norm": 0.05214112251996994,
      "learning_rate": 1.5613877872686597e-05,
      "loss": 0.9968,
      "step": 1400
    },
    {
      "epoch": 0.07169005491153142,
      "grad_norm": 0.02944546937942505,
      "learning_rate": 1.5613798428920074e-05,
      "loss": 0.996,
      "step": 1410
    },
    {
      "epoch": 0.07219849501728696,
      "grad_norm": 0.027857057750225067,
      "learning_rate": 1.561371898515355e-05,
      "loss": 0.9955,
      "step": 1420
    },
    {
      "epoch": 0.07270693512304251,
      "grad_norm": 0.0420515239238739,
      "learning_rate": 1.5613639541387023e-05,
      "loss": 0.9917,
      "step": 1430
    },
    {
      "epoch": 0.07321537522879805,
      "grad_norm": 0.03628384694457054,
      "learning_rate": 1.56135600976205e-05,
      "loss": 0.992,
      "step": 1440
    },
    {
      "epoch": 0.07372381533455359,
      "grad_norm": 0.15296992659568787,
      "learning_rate": 1.5613480653853976e-05,
      "loss": 0.9923,
      "step": 1450
    },
    {
      "epoch": 0.07423225544030913,
      "grad_norm": 0.06660768389701843,
      "learning_rate": 1.5613401210087452e-05,
      "loss": 0.9967,
      "step": 1460
    },
    {
      "epoch": 0.07474069554606468,
      "grad_norm": 0.03161308169364929,
      "learning_rate": 1.561332176632093e-05,
      "loss": 0.9954,
      "step": 1470
    },
    {
      "epoch": 0.07524913565182022,
      "grad_norm": 0.047338780015707016,
      "learning_rate": 1.56132423225544e-05,
      "loss": 0.9954,
      "step": 1480
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.054758843034505844,
      "learning_rate": 1.5613162878787878e-05,
      "loss": 0.9956,
      "step": 1490
    },
    {
      "epoch": 0.0762660158633313,
      "grad_norm": 0.063475102186203,
      "learning_rate": 1.5613083435021354e-05,
      "loss": 0.9957,
      "step": 1500
    },
    {
      "epoch": 0.07677445596908684,
      "grad_norm": 0.04314611107110977,
      "learning_rate": 1.561300399125483e-05,
      "loss": 0.9942,
      "step": 1510
    },
    {
      "epoch": 0.07728289607484239,
      "grad_norm": 0.0714876651763916,
      "learning_rate": 1.5612924547488307e-05,
      "loss": 0.9962,
      "step": 1520
    },
    {
      "epoch": 0.07779133618059793,
      "grad_norm": 0.06790352612733841,
      "learning_rate": 1.561284510372178e-05,
      "loss": 0.9926,
      "step": 1530
    },
    {
      "epoch": 0.07829977628635347,
      "grad_norm": 0.023829128593206406,
      "learning_rate": 1.5612765659955257e-05,
      "loss": 0.997,
      "step": 1540
    },
    {
      "epoch": 0.07880821639210901,
      "grad_norm": 0.024950454011559486,
      "learning_rate": 1.5612686216188733e-05,
      "loss": 0.9957,
      "step": 1550
    },
    {
      "epoch": 0.07931665649786455,
      "grad_norm": 0.030426327139139175,
      "learning_rate": 1.561260677242221e-05,
      "loss": 0.9949,
      "step": 1560
    },
    {
      "epoch": 0.0798250966036201,
      "grad_norm": 0.033193085342645645,
      "learning_rate": 1.5612527328655682e-05,
      "loss": 0.9958,
      "step": 1570
    },
    {
      "epoch": 0.08033353670937564,
      "grad_norm": 0.03855384513735771,
      "learning_rate": 1.561244788488916e-05,
      "loss": 0.9938,
      "step": 1580
    },
    {
      "epoch": 0.08084197681513118,
      "grad_norm": 0.04245024546980858,
      "learning_rate": 1.5612368441122635e-05,
      "loss": 0.9962,
      "step": 1590
    },
    {
      "epoch": 0.08135041692088672,
      "grad_norm": 0.053567275404930115,
      "learning_rate": 1.561228899735611e-05,
      "loss": 0.9888,
      "step": 1600
    },
    {
      "epoch": 0.08185885702664226,
      "grad_norm": 0.03962669149041176,
      "learning_rate": 1.5612209553589588e-05,
      "loss": 0.9963,
      "step": 1610
    },
    {
      "epoch": 0.08236729713239781,
      "grad_norm": 0.06707791239023209,
      "learning_rate": 1.561213010982306e-05,
      "loss": 0.9939,
      "step": 1620
    },
    {
      "epoch": 0.08287573723815335,
      "grad_norm": 0.05543022230267525,
      "learning_rate": 1.5612050666056537e-05,
      "loss": 0.9944,
      "step": 1630
    },
    {
      "epoch": 0.08338417734390889,
      "grad_norm": 0.04690456762909889,
      "learning_rate": 1.5611971222290014e-05,
      "loss": 0.989,
      "step": 1640
    },
    {
      "epoch": 0.08389261744966443,
      "grad_norm": 0.08100345730781555,
      "learning_rate": 1.561189177852349e-05,
      "loss": 0.9951,
      "step": 1650
    },
    {
      "epoch": 0.08440105755541998,
      "grad_norm": 0.07867355644702911,
      "learning_rate": 1.5611812334756966e-05,
      "loss": 0.9921,
      "step": 1660
    },
    {
      "epoch": 0.08490949766117552,
      "grad_norm": 0.04653250426054001,
      "learning_rate": 1.561173289099044e-05,
      "loss": 0.9964,
      "step": 1670
    },
    {
      "epoch": 0.08541793776693106,
      "grad_norm": 0.02212458848953247,
      "learning_rate": 1.561165344722392e-05,
      "loss": 0.9947,
      "step": 1680
    },
    {
      "epoch": 0.0859263778726866,
      "grad_norm": 0.034485023468732834,
      "learning_rate": 1.5611574003457392e-05,
      "loss": 0.9917,
      "step": 1690
    },
    {
      "epoch": 0.08643481797844214,
      "grad_norm": 0.021814651787281036,
      "learning_rate": 1.561149455969087e-05,
      "loss": 0.9946,
      "step": 1700
    },
    {
      "epoch": 0.08694325808419769,
      "grad_norm": 0.039705194532871246,
      "learning_rate": 1.5611415115924345e-05,
      "loss": 0.9962,
      "step": 1710
    },
    {
      "epoch": 0.08745169818995323,
      "grad_norm": 0.03810322657227516,
      "learning_rate": 1.561133567215782e-05,
      "loss": 0.9965,
      "step": 1720
    },
    {
      "epoch": 0.08796013829570877,
      "grad_norm": 0.03137562423944473,
      "learning_rate": 1.5611256228391298e-05,
      "loss": 0.9942,
      "step": 1730
    },
    {
      "epoch": 0.08846857840146431,
      "grad_norm": 0.07801969349384308,
      "learning_rate": 1.561117678462477e-05,
      "loss": 0.9898,
      "step": 1740
    },
    {
      "epoch": 0.08897701850721985,
      "grad_norm": 0.06896845251321793,
      "learning_rate": 1.5611097340858247e-05,
      "loss": 0.9925,
      "step": 1750
    },
    {
      "epoch": 0.0894854586129754,
      "grad_norm": 0.04953717067837715,
      "learning_rate": 1.5611017897091724e-05,
      "loss": 0.9951,
      "step": 1760
    },
    {
      "epoch": 0.08999389871873094,
      "grad_norm": 0.036268990486860275,
      "learning_rate": 1.56109384533252e-05,
      "loss": 0.995,
      "step": 1770
    },
    {
      "epoch": 0.09050233882448648,
      "grad_norm": 0.04003484547138214,
      "learning_rate": 1.5610859009558676e-05,
      "loss": 0.9959,
      "step": 1780
    },
    {
      "epoch": 0.09101077893024202,
      "grad_norm": 0.06410039216279984,
      "learning_rate": 1.561077956579215e-05,
      "loss": 0.9897,
      "step": 1790
    },
    {
      "epoch": 0.09151921903599756,
      "grad_norm": 0.038838375359773636,
      "learning_rate": 1.5610700122025626e-05,
      "loss": 0.9927,
      "step": 1800
    },
    {
      "epoch": 0.09202765914175311,
      "grad_norm": 0.028581740334630013,
      "learning_rate": 1.5610620678259102e-05,
      "loss": 0.9936,
      "step": 1810
    },
    {
      "epoch": 0.09253609924750865,
      "grad_norm": 0.0366685725748539,
      "learning_rate": 1.561054123449258e-05,
      "loss": 0.9949,
      "step": 1820
    },
    {
      "epoch": 0.09304453935326419,
      "grad_norm": 0.06479804217815399,
      "learning_rate": 1.5610461790726055e-05,
      "loss": 0.9937,
      "step": 1830
    },
    {
      "epoch": 0.09355297945901973,
      "grad_norm": 0.05551065132021904,
      "learning_rate": 1.5610382346959528e-05,
      "loss": 0.9933,
      "step": 1840
    },
    {
      "epoch": 0.09406141956477528,
      "grad_norm": 0.07803503423929214,
      "learning_rate": 1.5610302903193004e-05,
      "loss": 0.9925,
      "step": 1850
    },
    {
      "epoch": 0.09456985967053082,
      "grad_norm": 0.05506991595029831,
      "learning_rate": 1.561022345942648e-05,
      "loss": 0.9932,
      "step": 1860
    },
    {
      "epoch": 0.09507829977628636,
      "grad_norm": 0.04233403131365776,
      "learning_rate": 1.5610144015659957e-05,
      "loss": 0.9935,
      "step": 1870
    },
    {
      "epoch": 0.0955867398820419,
      "grad_norm": 0.034347835928201675,
      "learning_rate": 1.5610064571893434e-05,
      "loss": 0.9931,
      "step": 1880
    },
    {
      "epoch": 0.09609517998779744,
      "grad_norm": 0.03289831057190895,
      "learning_rate": 1.5609985128126907e-05,
      "loss": 0.9937,
      "step": 1890
    },
    {
      "epoch": 0.09660362009355299,
      "grad_norm": 0.04455196484923363,
      "learning_rate": 1.5609905684360383e-05,
      "loss": 0.9917,
      "step": 1900
    },
    {
      "epoch": 0.09711206019930853,
      "grad_norm": 0.04805615916848183,
      "learning_rate": 1.560982624059386e-05,
      "loss": 0.9915,
      "step": 1910
    },
    {
      "epoch": 0.09762050030506407,
      "grad_norm": 0.04736807942390442,
      "learning_rate": 1.5609746796827336e-05,
      "loss": 0.991,
      "step": 1920
    },
    {
      "epoch": 0.09812894041081961,
      "grad_norm": 0.07935018092393875,
      "learning_rate": 1.560966735306081e-05,
      "loss": 0.9912,
      "step": 1930
    },
    {
      "epoch": 0.09863738051657515,
      "grad_norm": 0.05541091412305832,
      "learning_rate": 1.5609587909294285e-05,
      "loss": 0.9893,
      "step": 1940
    },
    {
      "epoch": 0.09914582062233068,
      "grad_norm": 0.06722774356603622,
      "learning_rate": 1.560950846552776e-05,
      "loss": 0.9924,
      "step": 1950
    },
    {
      "epoch": 0.09965426072808622,
      "grad_norm": 0.07759469747543335,
      "learning_rate": 1.5609429021761238e-05,
      "loss": 0.9917,
      "step": 1960
    },
    {
      "epoch": 0.10016270083384177,
      "grad_norm": 0.0341569148004055,
      "learning_rate": 1.5609349577994714e-05,
      "loss": 0.9965,
      "step": 1970
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 0.06479421257972717,
      "learning_rate": 1.5609270134228187e-05,
      "loss": 0.9917,
      "step": 1980
    },
    {
      "epoch": 0.10117958104535285,
      "grad_norm": 0.03876463323831558,
      "learning_rate": 1.5609190690461664e-05,
      "loss": 0.995,
      "step": 1990
    },
    {
      "epoch": 0.1016880211511084,
      "grad_norm": 0.039699021726846695,
      "learning_rate": 1.560911124669514e-05,
      "loss": 0.9927,
      "step": 2000
    },
    {
      "epoch": 0.10219646125686394,
      "grad_norm": 0.041277818381786346,
      "learning_rate": 1.5609031802928617e-05,
      "loss": 0.9962,
      "step": 2010
    },
    {
      "epoch": 0.10270490136261948,
      "grad_norm": 0.05072081461548805,
      "learning_rate": 1.5608952359162093e-05,
      "loss": 0.9919,
      "step": 2020
    },
    {
      "epoch": 0.10321334146837502,
      "grad_norm": 0.03399088978767395,
      "learning_rate": 1.5608872915395566e-05,
      "loss": 0.9969,
      "step": 2030
    },
    {
      "epoch": 0.10372178157413056,
      "grad_norm": 0.04382099583745003,
      "learning_rate": 1.5608793471629042e-05,
      "loss": 0.9906,
      "step": 2040
    },
    {
      "epoch": 0.1042302216798861,
      "grad_norm": 0.0579586923122406,
      "learning_rate": 1.560871402786252e-05,
      "loss": 0.9939,
      "step": 2050
    },
    {
      "epoch": 0.10473866178564165,
      "grad_norm": 0.03789537400007248,
      "learning_rate": 1.5608634584095995e-05,
      "loss": 0.988,
      "step": 2060
    },
    {
      "epoch": 0.10524710189139719,
      "grad_norm": 0.06279871612787247,
      "learning_rate": 1.560855514032947e-05,
      "loss": 0.992,
      "step": 2070
    },
    {
      "epoch": 0.10575554199715273,
      "grad_norm": 0.03776942938566208,
      "learning_rate": 1.5608475696562945e-05,
      "loss": 0.9949,
      "step": 2080
    },
    {
      "epoch": 0.10626398210290827,
      "grad_norm": 0.05954955518245697,
      "learning_rate": 1.560839625279642e-05,
      "loss": 0.9916,
      "step": 2090
    },
    {
      "epoch": 0.10677242220866381,
      "grad_norm": 0.03231731057167053,
      "learning_rate": 1.5608316809029897e-05,
      "loss": 0.9908,
      "step": 2100
    },
    {
      "epoch": 0.10728086231441936,
      "grad_norm": 0.039294369518756866,
      "learning_rate": 1.5608237365263374e-05,
      "loss": 0.9888,
      "step": 2110
    },
    {
      "epoch": 0.1077893024201749,
      "grad_norm": 0.025768592953681946,
      "learning_rate": 1.5608157921496847e-05,
      "loss": 0.9926,
      "step": 2120
    },
    {
      "epoch": 0.10829774252593044,
      "grad_norm": 0.031574051827192307,
      "learning_rate": 1.5608078477730323e-05,
      "loss": 0.9942,
      "step": 2130
    },
    {
      "epoch": 0.10880618263168598,
      "grad_norm": 0.05074048414826393,
      "learning_rate": 1.56079990339638e-05,
      "loss": 0.9938,
      "step": 2140
    },
    {
      "epoch": 0.10931462273744152,
      "grad_norm": 0.05496763810515404,
      "learning_rate": 1.5607919590197276e-05,
      "loss": 0.9931,
      "step": 2150
    },
    {
      "epoch": 0.10982306284319707,
      "grad_norm": 0.04895289987325668,
      "learning_rate": 1.5607840146430752e-05,
      "loss": 0.9917,
      "step": 2160
    },
    {
      "epoch": 0.11033150294895261,
      "grad_norm": 0.027061598375439644,
      "learning_rate": 1.5607760702664225e-05,
      "loss": 0.9896,
      "step": 2170
    },
    {
      "epoch": 0.11083994305470815,
      "grad_norm": 0.06937680393457413,
      "learning_rate": 1.56076812588977e-05,
      "loss": 0.9916,
      "step": 2180
    },
    {
      "epoch": 0.1113483831604637,
      "grad_norm": 0.04906342551112175,
      "learning_rate": 1.5607601815131178e-05,
      "loss": 0.9921,
      "step": 2190
    },
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 0.05208858847618103,
      "learning_rate": 1.5607522371364654e-05,
      "loss": 0.9937,
      "step": 2200
    },
    {
      "epoch": 0.11236526337197478,
      "grad_norm": 0.05671447142958641,
      "learning_rate": 1.560744292759813e-05,
      "loss": 0.9875,
      "step": 2210
    },
    {
      "epoch": 0.11287370347773032,
      "grad_norm": 0.053529057651758194,
      "learning_rate": 1.5607363483831604e-05,
      "loss": 0.9874,
      "step": 2220
    },
    {
      "epoch": 0.11338214358348586,
      "grad_norm": 0.044994838535785675,
      "learning_rate": 1.560728404006508e-05,
      "loss": 0.9944,
      "step": 2230
    },
    {
      "epoch": 0.1138905836892414,
      "grad_norm": 0.23162643611431122,
      "learning_rate": 1.5607204596298557e-05,
      "loss": 0.9889,
      "step": 2240
    },
    {
      "epoch": 0.11439902379499695,
      "grad_norm": 0.04578680545091629,
      "learning_rate": 1.5607125152532033e-05,
      "loss": 0.9912,
      "step": 2250
    },
    {
      "epoch": 0.11490746390075249,
      "grad_norm": 0.050073061138391495,
      "learning_rate": 1.560704570876551e-05,
      "loss": 0.9927,
      "step": 2260
    },
    {
      "epoch": 0.11541590400650803,
      "grad_norm": 0.04651496186852455,
      "learning_rate": 1.5606966264998982e-05,
      "loss": 0.9927,
      "step": 2270
    },
    {
      "epoch": 0.11592434411226357,
      "grad_norm": 0.030631106346845627,
      "learning_rate": 1.560688682123246e-05,
      "loss": 0.9937,
      "step": 2280
    },
    {
      "epoch": 0.11643278421801911,
      "grad_norm": 0.042649827897548676,
      "learning_rate": 1.5606807377465935e-05,
      "loss": 0.9948,
      "step": 2290
    },
    {
      "epoch": 0.11694122432377466,
      "grad_norm": 0.05104920640587807,
      "learning_rate": 1.560672793369941e-05,
      "loss": 0.9901,
      "step": 2300
    },
    {
      "epoch": 0.1174496644295302,
      "grad_norm": 0.038390904664993286,
      "learning_rate": 1.5606648489932888e-05,
      "loss": 0.9884,
      "step": 2310
    },
    {
      "epoch": 0.11795810453528574,
      "grad_norm": 0.10825981199741364,
      "learning_rate": 1.560656904616636e-05,
      "loss": 0.9903,
      "step": 2320
    },
    {
      "epoch": 0.11846654464104128,
      "grad_norm": 0.028038673102855682,
      "learning_rate": 1.5606489602399837e-05,
      "loss": 0.9939,
      "step": 2330
    },
    {
      "epoch": 0.11897498474679682,
      "grad_norm": 0.05222094804048538,
      "learning_rate": 1.5606410158633314e-05,
      "loss": 0.9882,
      "step": 2340
    },
    {
      "epoch": 0.11948342485255237,
      "grad_norm": 0.15193307399749756,
      "learning_rate": 1.560633071486679e-05,
      "loss": 0.9885,
      "step": 2350
    },
    {
      "epoch": 0.11999186495830791,
      "grad_norm": 0.04243640974164009,
      "learning_rate": 1.5606251271100263e-05,
      "loss": 0.9927,
      "step": 2360
    },
    {
      "epoch": 0.12050030506406345,
      "grad_norm": 0.04226936027407646,
      "learning_rate": 1.560617182733374e-05,
      "loss": 0.9919,
      "step": 2370
    },
    {
      "epoch": 0.121008745169819,
      "grad_norm": 0.03807546943426132,
      "learning_rate": 1.5606092383567216e-05,
      "loss": 0.9941,
      "step": 2380
    },
    {
      "epoch": 0.12151718527557454,
      "grad_norm": 0.05586467310786247,
      "learning_rate": 1.5606012939800692e-05,
      "loss": 0.9844,
      "step": 2390
    },
    {
      "epoch": 0.12202562538133008,
      "grad_norm": 0.0324520617723465,
      "learning_rate": 1.560593349603417e-05,
      "loss": 0.985,
      "step": 2400
    },
    {
      "epoch": 0.12253406548708562,
      "grad_norm": 0.07080630213022232,
      "learning_rate": 1.5605854052267642e-05,
      "loss": 0.9865,
      "step": 2410
    },
    {
      "epoch": 0.12304250559284116,
      "grad_norm": 0.05717862397432327,
      "learning_rate": 1.5605774608501118e-05,
      "loss": 0.9868,
      "step": 2420
    },
    {
      "epoch": 0.1235509456985967,
      "grad_norm": 0.03359675407409668,
      "learning_rate": 1.5605695164734595e-05,
      "loss": 0.9867,
      "step": 2430
    },
    {
      "epoch": 0.12405938580435225,
      "grad_norm": 0.035287126898765564,
      "learning_rate": 1.560561572096807e-05,
      "loss": 0.9896,
      "step": 2440
    },
    {
      "epoch": 0.12456782591010779,
      "grad_norm": 0.04085094854235649,
      "learning_rate": 1.5605536277201547e-05,
      "loss": 0.9916,
      "step": 2450
    },
    {
      "epoch": 0.12507626601586333,
      "grad_norm": 0.06555156409740448,
      "learning_rate": 1.560545683343502e-05,
      "loss": 0.9927,
      "step": 2460
    },
    {
      "epoch": 0.12558470612161887,
      "grad_norm": 0.04061630368232727,
      "learning_rate": 1.5605377389668497e-05,
      "loss": 0.9907,
      "step": 2470
    },
    {
      "epoch": 0.12609314622737441,
      "grad_norm": 0.03320641443133354,
      "learning_rate": 1.5605297945901973e-05,
      "loss": 0.9931,
      "step": 2480
    },
    {
      "epoch": 0.12660158633312996,
      "grad_norm": 0.04250844940543175,
      "learning_rate": 1.560521850213545e-05,
      "loss": 0.9912,
      "step": 2490
    },
    {
      "epoch": 0.1271100264388855,
      "grad_norm": 0.06473518908023834,
      "learning_rate": 1.5605139058368926e-05,
      "loss": 0.9902,
      "step": 2500
    },
    {
      "epoch": 0.12761846654464104,
      "grad_norm": 0.2433418184518814,
      "learning_rate": 1.56050596146024e-05,
      "loss": 0.9873,
      "step": 2510
    },
    {
      "epoch": 0.12812690665039658,
      "grad_norm": 0.07161853462457657,
      "learning_rate": 1.5604980170835875e-05,
      "loss": 0.9887,
      "step": 2520
    },
    {
      "epoch": 0.12863534675615212,
      "grad_norm": 0.038165513426065445,
      "learning_rate": 1.5604900727069352e-05,
      "loss": 0.9933,
      "step": 2530
    },
    {
      "epoch": 0.12914378686190767,
      "grad_norm": 0.04081125557422638,
      "learning_rate": 1.5604821283302828e-05,
      "loss": 0.9924,
      "step": 2540
    },
    {
      "epoch": 0.1296522269676632,
      "grad_norm": 0.08596862852573395,
      "learning_rate": 1.5604741839536305e-05,
      "loss": 0.9923,
      "step": 2550
    },
    {
      "epoch": 0.13016066707341875,
      "grad_norm": 0.0563557893037796,
      "learning_rate": 1.5604662395769778e-05,
      "loss": 0.9886,
      "step": 2560
    },
    {
      "epoch": 0.1306691071791743,
      "grad_norm": 0.05157705023884773,
      "learning_rate": 1.5604582952003254e-05,
      "loss": 0.9902,
      "step": 2570
    },
    {
      "epoch": 0.13117754728492984,
      "grad_norm": 0.06873397529125214,
      "learning_rate": 1.560450350823673e-05,
      "loss": 0.9935,
      "step": 2580
    },
    {
      "epoch": 0.13168598739068538,
      "grad_norm": 0.0460529550909996,
      "learning_rate": 1.5604424064470207e-05,
      "loss": 0.994,
      "step": 2590
    },
    {
      "epoch": 0.13219442749644092,
      "grad_norm": 0.05182671546936035,
      "learning_rate": 1.560434462070368e-05,
      "loss": 0.9947,
      "step": 2600
    },
    {
      "epoch": 0.13270286760219646,
      "grad_norm": 0.035395219922065735,
      "learning_rate": 1.5604265176937156e-05,
      "loss": 0.9893,
      "step": 2610
    },
    {
      "epoch": 0.133211307707952,
      "grad_norm": 0.059874288737773895,
      "learning_rate": 1.5604185733170633e-05,
      "loss": 0.9902,
      "step": 2620
    },
    {
      "epoch": 0.13371974781370755,
      "grad_norm": 0.020792508497834206,
      "learning_rate": 1.560410628940411e-05,
      "loss": 0.9908,
      "step": 2630
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 0.04048856720328331,
      "learning_rate": 1.5604026845637585e-05,
      "loss": 0.9862,
      "step": 2640
    },
    {
      "epoch": 0.13473662802521863,
      "grad_norm": 0.06144915521144867,
      "learning_rate": 1.5603947401871058e-05,
      "loss": 0.9831,
      "step": 2650
    },
    {
      "epoch": 0.13524506813097417,
      "grad_norm": 0.026608996093273163,
      "learning_rate": 1.5603867958104535e-05,
      "loss": 0.9915,
      "step": 2660
    },
    {
      "epoch": 0.13575350823672971,
      "grad_norm": 0.04775913059711456,
      "learning_rate": 1.560378851433801e-05,
      "loss": 0.9929,
      "step": 2670
    },
    {
      "epoch": 0.13626194834248526,
      "grad_norm": 0.07946902513504028,
      "learning_rate": 1.5603709070571487e-05,
      "loss": 0.9867,
      "step": 2680
    },
    {
      "epoch": 0.1367703884482408,
      "grad_norm": 0.0462302528321743,
      "learning_rate": 1.5603629626804964e-05,
      "loss": 0.9884,
      "step": 2690
    },
    {
      "epoch": 0.13727882855399634,
      "grad_norm": 0.09504169225692749,
      "learning_rate": 1.5603550183038437e-05,
      "loss": 0.9874,
      "step": 2700
    },
    {
      "epoch": 0.13778726865975188,
      "grad_norm": 0.05434801056981087,
      "learning_rate": 1.5603470739271913e-05,
      "loss": 0.9884,
      "step": 2710
    },
    {
      "epoch": 0.13829570876550742,
      "grad_norm": 0.07210832089185715,
      "learning_rate": 1.560339129550539e-05,
      "loss": 0.9907,
      "step": 2720
    },
    {
      "epoch": 0.13880414887126297,
      "grad_norm": 0.04153694584965706,
      "learning_rate": 1.5603311851738866e-05,
      "loss": 0.9926,
      "step": 2730
    },
    {
      "epoch": 0.1393125889770185,
      "grad_norm": 0.03708634525537491,
      "learning_rate": 1.5603232407972342e-05,
      "loss": 0.99,
      "step": 2740
    },
    {
      "epoch": 0.13982102908277405,
      "grad_norm": 0.04274454340338707,
      "learning_rate": 1.5603152964205815e-05,
      "loss": 0.9851,
      "step": 2750
    },
    {
      "epoch": 0.1403294691885296,
      "grad_norm": 0.07895297557115555,
      "learning_rate": 1.5603073520439292e-05,
      "loss": 0.9892,
      "step": 2760
    },
    {
      "epoch": 0.14083790929428514,
      "grad_norm": 0.05323055759072304,
      "learning_rate": 1.5602994076672768e-05,
      "loss": 0.9873,
      "step": 2770
    },
    {
      "epoch": 0.14134634940004068,
      "grad_norm": 0.04796703904867172,
      "learning_rate": 1.5602914632906245e-05,
      "loss": 0.9868,
      "step": 2780
    },
    {
      "epoch": 0.14185478950579622,
      "grad_norm": 0.04061596468091011,
      "learning_rate": 1.560283518913972e-05,
      "loss": 0.9965,
      "step": 2790
    },
    {
      "epoch": 0.14236322961155176,
      "grad_norm": 0.05404657870531082,
      "learning_rate": 1.5602755745373194e-05,
      "loss": 0.991,
      "step": 2800
    },
    {
      "epoch": 0.1428716697173073,
      "grad_norm": 0.024901166558265686,
      "learning_rate": 1.560267630160667e-05,
      "loss": 0.9924,
      "step": 2810
    },
    {
      "epoch": 0.14338010982306285,
      "grad_norm": 0.18058647215366364,
      "learning_rate": 1.5602596857840147e-05,
      "loss": 0.9832,
      "step": 2820
    },
    {
      "epoch": 0.1438885499288184,
      "grad_norm": 0.11929424107074738,
      "learning_rate": 1.5602517414073623e-05,
      "loss": 0.9867,
      "step": 2830
    },
    {
      "epoch": 0.14439699003457393,
      "grad_norm": 0.0497281588613987,
      "learning_rate": 1.5602437970307096e-05,
      "loss": 0.9751,
      "step": 2840
    },
    {
      "epoch": 0.14490543014032947,
      "grad_norm": 0.05954331159591675,
      "learning_rate": 1.5602358526540573e-05,
      "loss": 0.9922,
      "step": 2850
    },
    {
      "epoch": 0.14541387024608501,
      "grad_norm": 0.03341057896614075,
      "learning_rate": 1.560227908277405e-05,
      "loss": 0.9903,
      "step": 2860
    },
    {
      "epoch": 0.14592231035184056,
      "grad_norm": 0.033844172954559326,
      "learning_rate": 1.5602199639007525e-05,
      "loss": 0.9894,
      "step": 2870
    },
    {
      "epoch": 0.1464307504575961,
      "grad_norm": 0.037595827132463455,
      "learning_rate": 1.5602120195241002e-05,
      "loss": 0.9915,
      "step": 2880
    },
    {
      "epoch": 0.14693919056335164,
      "grad_norm": 0.055038932710886,
      "learning_rate": 1.5602040751474475e-05,
      "loss": 0.9859,
      "step": 2890
    },
    {
      "epoch": 0.14744763066910718,
      "grad_norm": 0.08069689571857452,
      "learning_rate": 1.560196130770795e-05,
      "loss": 0.9849,
      "step": 2900
    },
    {
      "epoch": 0.14795607077486272,
      "grad_norm": 0.029698830097913742,
      "learning_rate": 1.5601881863941428e-05,
      "loss": 0.9911,
      "step": 2910
    },
    {
      "epoch": 0.14846451088061827,
      "grad_norm": 0.04211493581533432,
      "learning_rate": 1.5601802420174904e-05,
      "loss": 0.9927,
      "step": 2920
    },
    {
      "epoch": 0.1489729509863738,
      "grad_norm": 0.04077969864010811,
      "learning_rate": 1.560172297640838e-05,
      "loss": 0.9861,
      "step": 2930
    },
    {
      "epoch": 0.14948139109212935,
      "grad_norm": 0.029990313574671745,
      "learning_rate": 1.5601643532641853e-05,
      "loss": 0.994,
      "step": 2940
    },
    {
      "epoch": 0.1499898311978849,
      "grad_norm": 0.0503835491836071,
      "learning_rate": 1.560156408887533e-05,
      "loss": 0.9908,
      "step": 2950
    },
    {
      "epoch": 0.15049827130364044,
      "grad_norm": 0.09292280673980713,
      "learning_rate": 1.5601484645108806e-05,
      "loss": 0.9867,
      "step": 2960
    },
    {
      "epoch": 0.15100671140939598,
      "grad_norm": 0.052805013954639435,
      "learning_rate": 1.5601405201342283e-05,
      "loss": 0.9891,
      "step": 2970
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.06374190747737885,
      "learning_rate": 1.560132575757576e-05,
      "loss": 0.9858,
      "step": 2980
    },
    {
      "epoch": 0.15202359162090706,
      "grad_norm": 0.05466075986623764,
      "learning_rate": 1.5601246313809232e-05,
      "loss": 0.9908,
      "step": 2990
    },
    {
      "epoch": 0.1525320317266626,
      "grad_norm": 0.029824893921613693,
      "learning_rate": 1.560116687004271e-05,
      "loss": 0.9902,
      "step": 3000
    },
    {
      "epoch": 0.15304047183241815,
      "grad_norm": 0.051884934306144714,
      "learning_rate": 1.5601087426276185e-05,
      "loss": 0.993,
      "step": 3010
    },
    {
      "epoch": 0.1535489119381737,
      "grad_norm": 0.021749045699834824,
      "learning_rate": 1.560100798250966e-05,
      "loss": 0.9884,
      "step": 3020
    },
    {
      "epoch": 0.15405735204392923,
      "grad_norm": 0.06897551566362381,
      "learning_rate": 1.5600928538743138e-05,
      "loss": 0.9865,
      "step": 3030
    },
    {
      "epoch": 0.15456579214968477,
      "grad_norm": 0.033566705882549286,
      "learning_rate": 1.560084909497661e-05,
      "loss": 0.9895,
      "step": 3040
    },
    {
      "epoch": 0.15507423225544031,
      "grad_norm": 0.0730113536119461,
      "learning_rate": 1.5600769651210087e-05,
      "loss": 0.9902,
      "step": 3050
    },
    {
      "epoch": 0.15558267236119586,
      "grad_norm": 0.10095278173685074,
      "learning_rate": 1.5600690207443563e-05,
      "loss": 0.9877,
      "step": 3060
    },
    {
      "epoch": 0.1560911124669514,
      "grad_norm": 0.07082834094762802,
      "learning_rate": 1.560061076367704e-05,
      "loss": 0.9854,
      "step": 3070
    },
    {
      "epoch": 0.15659955257270694,
      "grad_norm": 0.04166441038250923,
      "learning_rate": 1.5600531319910513e-05,
      "loss": 0.9865,
      "step": 3080
    },
    {
      "epoch": 0.15710799267846248,
      "grad_norm": 0.19688712060451508,
      "learning_rate": 1.560045187614399e-05,
      "loss": 0.983,
      "step": 3090
    },
    {
      "epoch": 0.15761643278421802,
      "grad_norm": 0.03571546450257301,
      "learning_rate": 1.5600372432377466e-05,
      "loss": 0.9769,
      "step": 3100
    },
    {
      "epoch": 0.15812487288997357,
      "grad_norm": 0.07146046310663223,
      "learning_rate": 1.5600292988610942e-05,
      "loss": 0.9941,
      "step": 3110
    },
    {
      "epoch": 0.1586333129957291,
      "grad_norm": 0.08117101341485977,
      "learning_rate": 1.560021354484442e-05,
      "loss": 0.9923,
      "step": 3120
    },
    {
      "epoch": 0.15914175310148465,
      "grad_norm": 0.10778874158859253,
      "learning_rate": 1.560013410107789e-05,
      "loss": 0.9919,
      "step": 3130
    },
    {
      "epoch": 0.1596501932072402,
      "grad_norm": 0.040109194815158844,
      "learning_rate": 1.5600054657311368e-05,
      "loss": 0.9896,
      "step": 3140
    },
    {
      "epoch": 0.16015863331299574,
      "grad_norm": 0.05270930379629135,
      "learning_rate": 1.5599975213544844e-05,
      "loss": 0.9869,
      "step": 3150
    },
    {
      "epoch": 0.16066707341875128,
      "grad_norm": 0.04174687713384628,
      "learning_rate": 1.559989576977832e-05,
      "loss": 0.9846,
      "step": 3160
    },
    {
      "epoch": 0.16117551352450682,
      "grad_norm": 0.077519491314888,
      "learning_rate": 1.5599816326011797e-05,
      "loss": 0.9898,
      "step": 3170
    },
    {
      "epoch": 0.16168395363026236,
      "grad_norm": 0.07779545336961746,
      "learning_rate": 1.559973688224527e-05,
      "loss": 0.9892,
      "step": 3180
    },
    {
      "epoch": 0.1621923937360179,
      "grad_norm": 0.07071202248334885,
      "learning_rate": 1.5599657438478746e-05,
      "loss": 0.9891,
      "step": 3190
    },
    {
      "epoch": 0.16270083384177345,
      "grad_norm": 0.10095193982124329,
      "learning_rate": 1.5599577994712223e-05,
      "loss": 0.9783,
      "step": 3200
    },
    {
      "epoch": 0.163209273947529,
      "grad_norm": 0.02553052455186844,
      "learning_rate": 1.55994985509457e-05,
      "loss": 0.9966,
      "step": 3210
    },
    {
      "epoch": 0.16371771405328453,
      "grad_norm": 0.05504486337304115,
      "learning_rate": 1.5599419107179175e-05,
      "loss": 0.991,
      "step": 3220
    },
    {
      "epoch": 0.16422615415904007,
      "grad_norm": 0.08607561141252518,
      "learning_rate": 1.559933966341265e-05,
      "loss": 0.989,
      "step": 3230
    },
    {
      "epoch": 0.16473459426479561,
      "grad_norm": 0.0574423186480999,
      "learning_rate": 1.5599260219646125e-05,
      "loss": 0.9836,
      "step": 3240
    },
    {
      "epoch": 0.16524303437055116,
      "grad_norm": 0.07715442776679993,
      "learning_rate": 1.55991807758796e-05,
      "loss": 0.9808,
      "step": 3250
    },
    {
      "epoch": 0.1657514744763067,
      "grad_norm": 0.08275113254785538,
      "learning_rate": 1.5599101332113078e-05,
      "loss": 0.9848,
      "step": 3260
    },
    {
      "epoch": 0.16625991458206224,
      "grad_norm": 0.0783005803823471,
      "learning_rate": 1.559902188834655e-05,
      "loss": 0.9906,
      "step": 3270
    },
    {
      "epoch": 0.16676835468781778,
      "grad_norm": 0.025965914130210876,
      "learning_rate": 1.5598942444580027e-05,
      "loss": 0.9895,
      "step": 3280
    },
    {
      "epoch": 0.16727679479357332,
      "grad_norm": 0.07178384065628052,
      "learning_rate": 1.5598863000813503e-05,
      "loss": 0.9915,
      "step": 3290
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 0.05520344898104668,
      "learning_rate": 1.559878355704698e-05,
      "loss": 0.9831,
      "step": 3300
    },
    {
      "epoch": 0.1682936750050844,
      "grad_norm": 0.06503672897815704,
      "learning_rate": 1.5598704113280456e-05,
      "loss": 0.9855,
      "step": 3310
    },
    {
      "epoch": 0.16880211511083995,
      "grad_norm": 0.029304243624210358,
      "learning_rate": 1.559862466951393e-05,
      "loss": 0.9924,
      "step": 3320
    },
    {
      "epoch": 0.1693105552165955,
      "grad_norm": 0.06932500749826431,
      "learning_rate": 1.5598545225747406e-05,
      "loss": 0.9831,
      "step": 3330
    },
    {
      "epoch": 0.16981899532235104,
      "grad_norm": 0.06156828626990318,
      "learning_rate": 1.5598465781980885e-05,
      "loss": 0.9861,
      "step": 3340
    },
    {
      "epoch": 0.17032743542810658,
      "grad_norm": 0.06753328442573547,
      "learning_rate": 1.559838633821436e-05,
      "loss": 0.9857,
      "step": 3350
    },
    {
      "epoch": 0.17083587553386212,
      "grad_norm": 0.08446066826581955,
      "learning_rate": 1.5598306894447835e-05,
      "loss": 0.9895,
      "step": 3360
    },
    {
      "epoch": 0.17134431563961766,
      "grad_norm": 0.17120490968227386,
      "learning_rate": 1.559822745068131e-05,
      "loss": 0.9839,
      "step": 3370
    },
    {
      "epoch": 0.1718527557453732,
      "grad_norm": 0.11231834441423416,
      "learning_rate": 1.5598148006914788e-05,
      "loss": 0.9882,
      "step": 3380
    },
    {
      "epoch": 0.17236119585112875,
      "grad_norm": 0.10909219831228256,
      "learning_rate": 1.559806856314826e-05,
      "loss": 0.9817,
      "step": 3390
    },
    {
      "epoch": 0.1728696359568843,
      "grad_norm": 0.05886925756931305,
      "learning_rate": 1.5597989119381737e-05,
      "loss": 0.9914,
      "step": 3400
    },
    {
      "epoch": 0.17337807606263983,
      "grad_norm": 0.1033293828368187,
      "learning_rate": 1.5597909675615213e-05,
      "loss": 0.9897,
      "step": 3410
    },
    {
      "epoch": 0.17388651616839537,
      "grad_norm": 0.11491374671459198,
      "learning_rate": 1.559783023184869e-05,
      "loss": 0.978,
      "step": 3420
    },
    {
      "epoch": 0.17439495627415091,
      "grad_norm": 0.14740388095378876,
      "learning_rate": 1.5597750788082166e-05,
      "loss": 0.9821,
      "step": 3430
    },
    {
      "epoch": 0.17490339637990646,
      "grad_norm": 0.14291903376579285,
      "learning_rate": 1.559767134431564e-05,
      "loss": 0.9825,
      "step": 3440
    },
    {
      "epoch": 0.175411836485662,
      "grad_norm": 0.11094755679368973,
      "learning_rate": 1.5597591900549116e-05,
      "loss": 0.9782,
      "step": 3450
    },
    {
      "epoch": 0.17592027659141754,
      "grad_norm": 0.09525375068187714,
      "learning_rate": 1.5597512456782592e-05,
      "loss": 0.9857,
      "step": 3460
    },
    {
      "epoch": 0.17642871669717308,
      "grad_norm": 0.09775818884372711,
      "learning_rate": 1.559743301301607e-05,
      "loss": 0.9806,
      "step": 3470
    },
    {
      "epoch": 0.17693715680292862,
      "grad_norm": 0.05376683175563812,
      "learning_rate": 1.5597353569249545e-05,
      "loss": 0.9855,
      "step": 3480
    },
    {
      "epoch": 0.17744559690868417,
      "grad_norm": 0.18946269154548645,
      "learning_rate": 1.5597274125483018e-05,
      "loss": 0.9882,
      "step": 3490
    },
    {
      "epoch": 0.1779540370144397,
      "grad_norm": 0.2999308109283447,
      "learning_rate": 1.5597194681716494e-05,
      "loss": 0.98,
      "step": 3500
    },
    {
      "epoch": 0.17846247712019525,
      "grad_norm": 0.09009292721748352,
      "learning_rate": 1.559711523794997e-05,
      "loss": 0.9802,
      "step": 3510
    },
    {
      "epoch": 0.1789709172259508,
      "grad_norm": 0.09887783974409103,
      "learning_rate": 1.5597035794183447e-05,
      "loss": 0.9701,
      "step": 3520
    },
    {
      "epoch": 0.17947935733170634,
      "grad_norm": 0.08205171674489975,
      "learning_rate": 1.5596956350416923e-05,
      "loss": 0.9768,
      "step": 3530
    },
    {
      "epoch": 0.17998779743746188,
      "grad_norm": 0.07805591821670532,
      "learning_rate": 1.5596876906650396e-05,
      "loss": 0.9826,
      "step": 3540
    },
    {
      "epoch": 0.18049623754321742,
      "grad_norm": 0.41665029525756836,
      "learning_rate": 1.5596797462883873e-05,
      "loss": 0.9562,
      "step": 3550
    },
    {
      "epoch": 0.18100467764897296,
      "grad_norm": 0.15598975121974945,
      "learning_rate": 1.559671801911735e-05,
      "loss": 0.987,
      "step": 3560
    },
    {
      "epoch": 0.1815131177547285,
      "grad_norm": 0.3351077437400818,
      "learning_rate": 1.5596638575350826e-05,
      "loss": 0.9684,
      "step": 3570
    },
    {
      "epoch": 0.18202155786048405,
      "grad_norm": 0.057492636144161224,
      "learning_rate": 1.5596559131584302e-05,
      "loss": 0.9843,
      "step": 3580
    },
    {
      "epoch": 0.1825299979662396,
      "grad_norm": 0.21443895995616913,
      "learning_rate": 1.5596479687817775e-05,
      "loss": 0.9682,
      "step": 3590
    },
    {
      "epoch": 0.18303843807199513,
      "grad_norm": 0.0868849828839302,
      "learning_rate": 1.559640024405125e-05,
      "loss": 0.97,
      "step": 3600
    },
    {
      "epoch": 0.18354687817775067,
      "grad_norm": 0.10946439206600189,
      "learning_rate": 1.5596320800284728e-05,
      "loss": 0.9802,
      "step": 3610
    },
    {
      "epoch": 0.18405531828350621,
      "grad_norm": 0.447807639837265,
      "learning_rate": 1.5596241356518204e-05,
      "loss": 0.963,
      "step": 3620
    },
    {
      "epoch": 0.18456375838926176,
      "grad_norm": 0.13428160548210144,
      "learning_rate": 1.5596161912751677e-05,
      "loss": 0.9819,
      "step": 3630
    },
    {
      "epoch": 0.1850721984950173,
      "grad_norm": 0.26555585861206055,
      "learning_rate": 1.5596082468985154e-05,
      "loss": 0.9495,
      "step": 3640
    },
    {
      "epoch": 0.18558063860077284,
      "grad_norm": 0.14353519678115845,
      "learning_rate": 1.559600302521863e-05,
      "loss": 0.9833,
      "step": 3650
    },
    {
      "epoch": 0.18608907870652838,
      "grad_norm": 0.10439060628414154,
      "learning_rate": 1.5595923581452106e-05,
      "loss": 0.9747,
      "step": 3660
    },
    {
      "epoch": 0.18659751881228392,
      "grad_norm": 0.09397095441818237,
      "learning_rate": 1.5595844137685583e-05,
      "loss": 0.9763,
      "step": 3670
    },
    {
      "epoch": 0.18710595891803947,
      "grad_norm": 0.26328861713409424,
      "learning_rate": 1.5595764693919056e-05,
      "loss": 0.9596,
      "step": 3680
    },
    {
      "epoch": 0.187614399023795,
      "grad_norm": 0.2412763237953186,
      "learning_rate": 1.5595685250152532e-05,
      "loss": 0.9691,
      "step": 3690
    },
    {
      "epoch": 0.18812283912955055,
      "grad_norm": 0.2025405615568161,
      "learning_rate": 1.559560580638601e-05,
      "loss": 0.9812,
      "step": 3700
    },
    {
      "epoch": 0.1886312792353061,
      "grad_norm": 0.08732935041189194,
      "learning_rate": 1.5595526362619485e-05,
      "loss": 0.9805,
      "step": 3710
    },
    {
      "epoch": 0.18913971934106164,
      "grad_norm": 0.2691952884197235,
      "learning_rate": 1.559544691885296e-05,
      "loss": 0.9793,
      "step": 3720
    },
    {
      "epoch": 0.18964815944681718,
      "grad_norm": 0.352804571390152,
      "learning_rate": 1.5595367475086434e-05,
      "loss": 0.9696,
      "step": 3730
    },
    {
      "epoch": 0.19015659955257272,
      "grad_norm": 0.2403414249420166,
      "learning_rate": 1.559528803131991e-05,
      "loss": 0.9545,
      "step": 3740
    },
    {
      "epoch": 0.19066503965832826,
      "grad_norm": 0.20490118861198425,
      "learning_rate": 1.5595208587553387e-05,
      "loss": 0.9659,
      "step": 3750
    },
    {
      "epoch": 0.1911734797640838,
      "grad_norm": 0.2499958723783493,
      "learning_rate": 1.5595129143786863e-05,
      "loss": 0.9799,
      "step": 3760
    },
    {
      "epoch": 0.19168191986983935,
      "grad_norm": 0.12938956916332245,
      "learning_rate": 1.559504970002034e-05,
      "loss": 0.9749,
      "step": 3770
    },
    {
      "epoch": 0.1921903599755949,
      "grad_norm": 0.2926735281944275,
      "learning_rate": 1.5594970256253813e-05,
      "loss": 0.9551,
      "step": 3780
    },
    {
      "epoch": 0.19269880008135043,
      "grad_norm": 0.20355448126792908,
      "learning_rate": 1.559489081248729e-05,
      "loss": 0.9469,
      "step": 3790
    },
    {
      "epoch": 0.19320724018710597,
      "grad_norm": 0.09084323793649673,
      "learning_rate": 1.5594811368720766e-05,
      "loss": 0.9619,
      "step": 3800
    },
    {
      "epoch": 0.19371568029286151,
      "grad_norm": 0.14571432769298553,
      "learning_rate": 1.5594731924954242e-05,
      "loss": 0.9692,
      "step": 3810
    },
    {
      "epoch": 0.19422412039861706,
      "grad_norm": 0.29230135679244995,
      "learning_rate": 1.559465248118772e-05,
      "loss": 0.9688,
      "step": 3820
    },
    {
      "epoch": 0.1947325605043726,
      "grad_norm": 0.11969779431819916,
      "learning_rate": 1.559457303742119e-05,
      "loss": 0.9778,
      "step": 3830
    },
    {
      "epoch": 0.19524100061012814,
      "grad_norm": 0.19086633622646332,
      "learning_rate": 1.5594493593654668e-05,
      "loss": 0.9628,
      "step": 3840
    },
    {
      "epoch": 0.19574944071588368,
      "grad_norm": 0.05881563946604729,
      "learning_rate": 1.5594414149888144e-05,
      "loss": 0.9658,
      "step": 3850
    },
    {
      "epoch": 0.19625788082163922,
      "grad_norm": 0.47812986373901367,
      "learning_rate": 1.559433470612162e-05,
      "loss": 0.9726,
      "step": 3860
    },
    {
      "epoch": 0.19676632092739477,
      "grad_norm": 0.1781751960515976,
      "learning_rate": 1.5594255262355094e-05,
      "loss": 0.9629,
      "step": 3870
    },
    {
      "epoch": 0.1972747610331503,
      "grad_norm": 0.21771574020385742,
      "learning_rate": 1.559417581858857e-05,
      "loss": 0.96,
      "step": 3880
    },
    {
      "epoch": 0.19778320113890582,
      "grad_norm": 0.1980503350496292,
      "learning_rate": 1.5594096374822046e-05,
      "loss": 0.9656,
      "step": 3890
    },
    {
      "epoch": 0.19829164124466137,
      "grad_norm": 0.2885592579841614,
      "learning_rate": 1.5594016931055523e-05,
      "loss": 0.9626,
      "step": 3900
    },
    {
      "epoch": 0.1988000813504169,
      "grad_norm": 0.12123899906873703,
      "learning_rate": 1.5593937487289e-05,
      "loss": 0.9645,
      "step": 3910
    },
    {
      "epoch": 0.19930852145617245,
      "grad_norm": 0.16395796835422516,
      "learning_rate": 1.5593858043522472e-05,
      "loss": 0.9613,
      "step": 3920
    },
    {
      "epoch": 0.199816961561928,
      "grad_norm": 0.3888380825519562,
      "learning_rate": 1.559377859975595e-05,
      "loss": 0.9595,
      "step": 3930
    },
    {
      "epoch": 0.20032540166768353,
      "grad_norm": 0.31521743535995483,
      "learning_rate": 1.5593699155989425e-05,
      "loss": 0.9668,
      "step": 3940
    },
    {
      "epoch": 0.20083384177343908,
      "grad_norm": 0.21379119157791138,
      "learning_rate": 1.55936197122229e-05,
      "loss": 0.9637,
      "step": 3950
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 0.07438326627016068,
      "learning_rate": 1.5593540268456378e-05,
      "loss": 0.9648,
      "step": 3960
    },
    {
      "epoch": 0.20185072198495016,
      "grad_norm": 0.22347350418567657,
      "learning_rate": 1.559346082468985e-05,
      "loss": 0.9582,
      "step": 3970
    },
    {
      "epoch": 0.2023591620907057,
      "grad_norm": 0.24998719990253448,
      "learning_rate": 1.5593381380923327e-05,
      "loss": 0.9691,
      "step": 3980
    },
    {
      "epoch": 0.20286760219646124,
      "grad_norm": 0.3333250880241394,
      "learning_rate": 1.5593301937156804e-05,
      "loss": 0.9552,
      "step": 3990
    },
    {
      "epoch": 0.2033760423022168,
      "grad_norm": 0.09660473465919495,
      "learning_rate": 1.559322249339028e-05,
      "loss": 0.9673,
      "step": 4000
    },
    {
      "epoch": 0.20388448240797233,
      "grad_norm": 0.547461986541748,
      "learning_rate": 1.5593143049623756e-05,
      "loss": 0.9533,
      "step": 4010
    },
    {
      "epoch": 0.20439292251372787,
      "grad_norm": 0.23913434147834778,
      "learning_rate": 1.559306360585723e-05,
      "loss": 0.9642,
      "step": 4020
    },
    {
      "epoch": 0.2049013626194834,
      "grad_norm": 0.19763138890266418,
      "learning_rate": 1.5592984162090706e-05,
      "loss": 0.9562,
      "step": 4030
    },
    {
      "epoch": 0.20540980272523895,
      "grad_norm": 0.15220187604427338,
      "learning_rate": 1.5592904718324182e-05,
      "loss": 0.9668,
      "step": 4040
    },
    {
      "epoch": 0.2059182428309945,
      "grad_norm": 0.04035644233226776,
      "learning_rate": 1.559282527455766e-05,
      "loss": 0.9697,
      "step": 4050
    },
    {
      "epoch": 0.20642668293675004,
      "grad_norm": 0.12003891170024872,
      "learning_rate": 1.559274583079113e-05,
      "loss": 0.9701,
      "step": 4060
    },
    {
      "epoch": 0.20693512304250558,
      "grad_norm": 0.5479641556739807,
      "learning_rate": 1.5592666387024608e-05,
      "loss": 0.9487,
      "step": 4070
    },
    {
      "epoch": 0.20744356314826112,
      "grad_norm": 0.21703709661960602,
      "learning_rate": 1.5592586943258084e-05,
      "loss": 0.9636,
      "step": 4080
    },
    {
      "epoch": 0.20795200325401667,
      "grad_norm": 0.4301917552947998,
      "learning_rate": 1.559250749949156e-05,
      "loss": 0.9485,
      "step": 4090
    },
    {
      "epoch": 0.2084604433597722,
      "grad_norm": 0.23342232406139374,
      "learning_rate": 1.5592428055725037e-05,
      "loss": 0.9399,
      "step": 4100
    },
    {
      "epoch": 0.20896888346552775,
      "grad_norm": 0.29641908407211304,
      "learning_rate": 1.559234861195851e-05,
      "loss": 0.9427,
      "step": 4110
    },
    {
      "epoch": 0.2094773235712833,
      "grad_norm": 0.6163246035575867,
      "learning_rate": 1.5592269168191987e-05,
      "loss": 0.9552,
      "step": 4120
    },
    {
      "epoch": 0.20998576367703883,
      "grad_norm": 0.18851856887340546,
      "learning_rate": 1.5592189724425463e-05,
      "loss": 0.956,
      "step": 4130
    },
    {
      "epoch": 0.21049420378279438,
      "grad_norm": 0.08262951672077179,
      "learning_rate": 1.559211028065894e-05,
      "loss": 0.9497,
      "step": 4140
    },
    {
      "epoch": 0.21100264388854992,
      "grad_norm": 0.23059314489364624,
      "learning_rate": 1.5592030836892416e-05,
      "loss": 0.9548,
      "step": 4150
    },
    {
      "epoch": 0.21151108399430546,
      "grad_norm": 0.1635764092206955,
      "learning_rate": 1.559195139312589e-05,
      "loss": 0.9468,
      "step": 4160
    },
    {
      "epoch": 0.212019524100061,
      "grad_norm": 0.2973092496395111,
      "learning_rate": 1.5591871949359365e-05,
      "loss": 0.9443,
      "step": 4170
    },
    {
      "epoch": 0.21252796420581654,
      "grad_norm": 0.2742946743965149,
      "learning_rate": 1.559179250559284e-05,
      "loss": 0.9399,
      "step": 4180
    },
    {
      "epoch": 0.2130364043115721,
      "grad_norm": 0.4999537467956543,
      "learning_rate": 1.5591713061826318e-05,
      "loss": 0.9603,
      "step": 4190
    },
    {
      "epoch": 0.21354484441732763,
      "grad_norm": 0.27071285247802734,
      "learning_rate": 1.5591633618059794e-05,
      "loss": 0.9427,
      "step": 4200
    },
    {
      "epoch": 0.21405328452308317,
      "grad_norm": 0.3879777789115906,
      "learning_rate": 1.5591554174293267e-05,
      "loss": 0.9514,
      "step": 4210
    },
    {
      "epoch": 0.2145617246288387,
      "grad_norm": 0.20520153641700745,
      "learning_rate": 1.5591474730526744e-05,
      "loss": 0.9601,
      "step": 4220
    },
    {
      "epoch": 0.21507016473459425,
      "grad_norm": 0.28744637966156006,
      "learning_rate": 1.559139528676022e-05,
      "loss": 0.9549,
      "step": 4230
    },
    {
      "epoch": 0.2155786048403498,
      "grad_norm": 0.2735349237918854,
      "learning_rate": 1.5591315842993696e-05,
      "loss": 0.9573,
      "step": 4240
    },
    {
      "epoch": 0.21608704494610534,
      "grad_norm": 0.4627687633037567,
      "learning_rate": 1.5591236399227173e-05,
      "loss": 0.951,
      "step": 4250
    },
    {
      "epoch": 0.21659548505186088,
      "grad_norm": 0.26233378052711487,
      "learning_rate": 1.5591156955460646e-05,
      "loss": 0.9497,
      "step": 4260
    },
    {
      "epoch": 0.21710392515761642,
      "grad_norm": 0.36437997221946716,
      "learning_rate": 1.5591077511694122e-05,
      "loss": 0.955,
      "step": 4270
    },
    {
      "epoch": 0.21761236526337197,
      "grad_norm": 0.7237149477005005,
      "learning_rate": 1.55909980679276e-05,
      "loss": 0.9504,
      "step": 4280
    },
    {
      "epoch": 0.2181208053691275,
      "grad_norm": 0.22282493114471436,
      "learning_rate": 1.5590918624161075e-05,
      "loss": 0.9557,
      "step": 4290
    },
    {
      "epoch": 0.21862924547488305,
      "grad_norm": 0.16691815853118896,
      "learning_rate": 1.5590839180394548e-05,
      "loss": 0.9583,
      "step": 4300
    },
    {
      "epoch": 0.2191376855806386,
      "grad_norm": 0.2253832221031189,
      "learning_rate": 1.5590759736628024e-05,
      "loss": 0.9411,
      "step": 4310
    },
    {
      "epoch": 0.21964612568639413,
      "grad_norm": 0.2528424561023712,
      "learning_rate": 1.55906802928615e-05,
      "loss": 0.9399,
      "step": 4320
    },
    {
      "epoch": 0.22015456579214968,
      "grad_norm": 0.3792564272880554,
      "learning_rate": 1.5590600849094977e-05,
      "loss": 0.9383,
      "step": 4330
    },
    {
      "epoch": 0.22066300589790522,
      "grad_norm": 0.32536643743515015,
      "learning_rate": 1.5590521405328454e-05,
      "loss": 0.9655,
      "step": 4340
    },
    {
      "epoch": 0.22117144600366076,
      "grad_norm": 0.4934133291244507,
      "learning_rate": 1.5590441961561927e-05,
      "loss": 0.9361,
      "step": 4350
    },
    {
      "epoch": 0.2216798861094163,
      "grad_norm": 0.197525754570961,
      "learning_rate": 1.5590362517795403e-05,
      "loss": 0.9322,
      "step": 4360
    },
    {
      "epoch": 0.22218832621517184,
      "grad_norm": 0.35336560010910034,
      "learning_rate": 1.559028307402888e-05,
      "loss": 0.9257,
      "step": 4370
    },
    {
      "epoch": 0.2226967663209274,
      "grad_norm": 0.37516599893569946,
      "learning_rate": 1.5590203630262356e-05,
      "loss": 0.9386,
      "step": 4380
    },
    {
      "epoch": 0.22320520642668293,
      "grad_norm": 0.21101514995098114,
      "learning_rate": 1.5590124186495832e-05,
      "loss": 0.9303,
      "step": 4390
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 0.32446032762527466,
      "learning_rate": 1.5590044742729305e-05,
      "loss": 0.9435,
      "step": 4400
    },
    {
      "epoch": 0.224222086638194,
      "grad_norm": 0.2513575553894043,
      "learning_rate": 1.558996529896278e-05,
      "loss": 0.9535,
      "step": 4410
    },
    {
      "epoch": 0.22473052674394955,
      "grad_norm": 0.5156847834587097,
      "learning_rate": 1.5589885855196258e-05,
      "loss": 0.9424,
      "step": 4420
    },
    {
      "epoch": 0.2252389668497051,
      "grad_norm": 0.3921329081058502,
      "learning_rate": 1.5589806411429734e-05,
      "loss": 0.9277,
      "step": 4430
    },
    {
      "epoch": 0.22574740695546064,
      "grad_norm": 0.1875317394733429,
      "learning_rate": 1.558972696766321e-05,
      "loss": 0.9457,
      "step": 4440
    },
    {
      "epoch": 0.22625584706121618,
      "grad_norm": 0.4237957000732422,
      "learning_rate": 1.5589647523896684e-05,
      "loss": 0.9372,
      "step": 4450
    },
    {
      "epoch": 0.22676428716697172,
      "grad_norm": 0.3842790424823761,
      "learning_rate": 1.558956808013016e-05,
      "loss": 0.9452,
      "step": 4460
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.2329857349395752,
      "learning_rate": 1.5589488636363637e-05,
      "loss": 0.9432,
      "step": 4470
    },
    {
      "epoch": 0.2277811673784828,
      "grad_norm": 0.46964889764785767,
      "learning_rate": 1.5589409192597113e-05,
      "loss": 0.9474,
      "step": 4480
    },
    {
      "epoch": 0.22828960748423835,
      "grad_norm": 0.3518105149269104,
      "learning_rate": 1.558932974883059e-05,
      "loss": 0.9377,
      "step": 4490
    },
    {
      "epoch": 0.2287980475899939,
      "grad_norm": 0.6131301522254944,
      "learning_rate": 1.5589250305064062e-05,
      "loss": 0.9409,
      "step": 4500
    },
    {
      "epoch": 0.22930648769574943,
      "grad_norm": 0.19985979795455933,
      "learning_rate": 1.558917086129754e-05,
      "loss": 0.9191,
      "step": 4510
    },
    {
      "epoch": 0.22981492780150498,
      "grad_norm": 0.49849313497543335,
      "learning_rate": 1.5589091417531015e-05,
      "loss": 0.9243,
      "step": 4520
    },
    {
      "epoch": 0.23032336790726052,
      "grad_norm": 0.5474237203598022,
      "learning_rate": 1.558901197376449e-05,
      "loss": 0.9356,
      "step": 4530
    },
    {
      "epoch": 0.23083180801301606,
      "grad_norm": 0.6940447092056274,
      "learning_rate": 1.5588932529997965e-05,
      "loss": 0.9529,
      "step": 4540
    },
    {
      "epoch": 0.2313402481187716,
      "grad_norm": 0.5032798051834106,
      "learning_rate": 1.558885308623144e-05,
      "loss": 0.9553,
      "step": 4550
    },
    {
      "epoch": 0.23184868822452714,
      "grad_norm": 0.3678533732891083,
      "learning_rate": 1.5588773642464917e-05,
      "loss": 0.9521,
      "step": 4560
    },
    {
      "epoch": 0.2323571283302827,
      "grad_norm": 0.6588091850280762,
      "learning_rate": 1.5588694198698394e-05,
      "loss": 0.9454,
      "step": 4570
    },
    {
      "epoch": 0.23286556843603823,
      "grad_norm": 0.26309093832969666,
      "learning_rate": 1.558861475493187e-05,
      "loss": 0.946,
      "step": 4580
    },
    {
      "epoch": 0.23337400854179377,
      "grad_norm": 0.1760457307100296,
      "learning_rate": 1.5588535311165343e-05,
      "loss": 0.93,
      "step": 4590
    },
    {
      "epoch": 0.2338824486475493,
      "grad_norm": 0.410163938999176,
      "learning_rate": 1.558845586739882e-05,
      "loss": 0.9345,
      "step": 4600
    },
    {
      "epoch": 0.23439088875330485,
      "grad_norm": 0.3224712014198303,
      "learning_rate": 1.5588376423632296e-05,
      "loss": 0.9515,
      "step": 4610
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 0.36413851380348206,
      "learning_rate": 1.5588296979865772e-05,
      "loss": 0.9154,
      "step": 4620
    },
    {
      "epoch": 0.23540776896481594,
      "grad_norm": 0.6140164136886597,
      "learning_rate": 1.558821753609925e-05,
      "loss": 0.9325,
      "step": 4630
    },
    {
      "epoch": 0.23591620907057148,
      "grad_norm": 0.6371312141418457,
      "learning_rate": 1.5588138092332722e-05,
      "loss": 0.9129,
      "step": 4640
    },
    {
      "epoch": 0.23642464917632702,
      "grad_norm": 0.2974485754966736,
      "learning_rate": 1.5588058648566198e-05,
      "loss": 0.9636,
      "step": 4650
    },
    {
      "epoch": 0.23693308928208257,
      "grad_norm": 0.4497326612472534,
      "learning_rate": 1.5587979204799675e-05,
      "loss": 0.902,
      "step": 4660
    },
    {
      "epoch": 0.2374415293878381,
      "grad_norm": 0.3346790075302124,
      "learning_rate": 1.558789976103315e-05,
      "loss": 0.9295,
      "step": 4670
    },
    {
      "epoch": 0.23794996949359365,
      "grad_norm": 0.3809441924095154,
      "learning_rate": 1.5587820317266627e-05,
      "loss": 0.9185,
      "step": 4680
    },
    {
      "epoch": 0.2384584095993492,
      "grad_norm": 0.37055331468582153,
      "learning_rate": 1.55877408735001e-05,
      "loss": 0.9262,
      "step": 4690
    },
    {
      "epoch": 0.23896684970510473,
      "grad_norm": 0.28009775280952454,
      "learning_rate": 1.5587661429733577e-05,
      "loss": 0.9332,
      "step": 4700
    },
    {
      "epoch": 0.23947528981086028,
      "grad_norm": 0.3188631236553192,
      "learning_rate": 1.5587581985967053e-05,
      "loss": 0.92,
      "step": 4710
    },
    {
      "epoch": 0.23998372991661582,
      "grad_norm": 0.5202596783638,
      "learning_rate": 1.558750254220053e-05,
      "loss": 0.928,
      "step": 4720
    },
    {
      "epoch": 0.24049217002237136,
      "grad_norm": 0.6554806232452393,
      "learning_rate": 1.5587423098434006e-05,
      "loss": 0.9303,
      "step": 4730
    },
    {
      "epoch": 0.2410006101281269,
      "grad_norm": 0.2589765191078186,
      "learning_rate": 1.558734365466748e-05,
      "loss": 0.9374,
      "step": 4740
    },
    {
      "epoch": 0.24150905023388244,
      "grad_norm": 0.3717450797557831,
      "learning_rate": 1.5587264210900955e-05,
      "loss": 0.9266,
      "step": 4750
    },
    {
      "epoch": 0.242017490339638,
      "grad_norm": 0.202347069978714,
      "learning_rate": 1.558718476713443e-05,
      "loss": 0.9352,
      "step": 4760
    },
    {
      "epoch": 0.24252593044539353,
      "grad_norm": 0.3726041615009308,
      "learning_rate": 1.5587105323367908e-05,
      "loss": 0.9235,
      "step": 4770
    },
    {
      "epoch": 0.24303437055114907,
      "grad_norm": 0.39566946029663086,
      "learning_rate": 1.558702587960138e-05,
      "loss": 0.9399,
      "step": 4780
    },
    {
      "epoch": 0.2435428106569046,
      "grad_norm": 0.3867150545120239,
      "learning_rate": 1.5586946435834857e-05,
      "loss": 0.9173,
      "step": 4790
    },
    {
      "epoch": 0.24405125076266015,
      "grad_norm": 0.2562207281589508,
      "learning_rate": 1.5586866992068334e-05,
      "loss": 0.9353,
      "step": 4800
    },
    {
      "epoch": 0.2445596908684157,
      "grad_norm": 0.25869831442832947,
      "learning_rate": 1.558678754830181e-05,
      "loss": 0.9339,
      "step": 4810
    },
    {
      "epoch": 0.24506813097417124,
      "grad_norm": 0.38127487897872925,
      "learning_rate": 1.5586708104535287e-05,
      "loss": 0.9267,
      "step": 4820
    },
    {
      "epoch": 0.24557657107992678,
      "grad_norm": 0.3466262221336365,
      "learning_rate": 1.558662866076876e-05,
      "loss": 0.9069,
      "step": 4830
    },
    {
      "epoch": 0.24608501118568232,
      "grad_norm": 0.3089774250984192,
      "learning_rate": 1.5586549217002236e-05,
      "loss": 0.9222,
      "step": 4840
    },
    {
      "epoch": 0.24659345129143787,
      "grad_norm": 0.3846903145313263,
      "learning_rate": 1.5586469773235712e-05,
      "loss": 0.9454,
      "step": 4850
    },
    {
      "epoch": 0.2471018913971934,
      "grad_norm": 0.47739461064338684,
      "learning_rate": 1.558639032946919e-05,
      "loss": 0.9106,
      "step": 4860
    },
    {
      "epoch": 0.24761033150294895,
      "grad_norm": 0.36029088497161865,
      "learning_rate": 1.5586310885702665e-05,
      "loss": 0.9245,
      "step": 4870
    },
    {
      "epoch": 0.2481187716087045,
      "grad_norm": 0.1664542406797409,
      "learning_rate": 1.5586231441936138e-05,
      "loss": 0.9324,
      "step": 4880
    },
    {
      "epoch": 0.24862721171446003,
      "grad_norm": 0.4079042375087738,
      "learning_rate": 1.5586151998169615e-05,
      "loss": 0.9374,
      "step": 4890
    },
    {
      "epoch": 0.24913565182021558,
      "grad_norm": 0.3875480890274048,
      "learning_rate": 1.558607255440309e-05,
      "loss": 0.9393,
      "step": 4900
    },
    {
      "epoch": 0.24964409192597112,
      "grad_norm": 0.3128310441970825,
      "learning_rate": 1.5585993110636567e-05,
      "loss": 0.9031,
      "step": 4910
    },
    {
      "epoch": 0.25015253203172666,
      "grad_norm": 0.5259705185890198,
      "learning_rate": 1.5585913666870044e-05,
      "loss": 0.919,
      "step": 4920
    },
    {
      "epoch": 0.25066097213748223,
      "grad_norm": 0.42937013506889343,
      "learning_rate": 1.5585834223103517e-05,
      "loss": 0.9448,
      "step": 4930
    },
    {
      "epoch": 0.25116941224323774,
      "grad_norm": 0.29037347435951233,
      "learning_rate": 1.5585754779336993e-05,
      "loss": 0.9196,
      "step": 4940
    },
    {
      "epoch": 0.2516778523489933,
      "grad_norm": 0.4470658600330353,
      "learning_rate": 1.558567533557047e-05,
      "loss": 0.9113,
      "step": 4950
    },
    {
      "epoch": 0.25218629245474883,
      "grad_norm": 0.25486519932746887,
      "learning_rate": 1.5585595891803946e-05,
      "loss": 0.9276,
      "step": 4960
    },
    {
      "epoch": 0.2526947325605044,
      "grad_norm": 0.11358867585659027,
      "learning_rate": 1.5585516448037422e-05,
      "loss": 0.9218,
      "step": 4970
    },
    {
      "epoch": 0.2532031726662599,
      "grad_norm": 0.2585192024707794,
      "learning_rate": 1.5585437004270895e-05,
      "loss": 0.9223,
      "step": 4980
    },
    {
      "epoch": 0.2537116127720155,
      "grad_norm": 0.4146342873573303,
      "learning_rate": 1.5585357560504372e-05,
      "loss": 0.9284,
      "step": 4990
    },
    {
      "epoch": 0.254220052877771,
      "grad_norm": 0.38607028126716614,
      "learning_rate": 1.5585278116737848e-05,
      "loss": 0.9099,
      "step": 5000
    },
    {
      "epoch": 0.25472849298352657,
      "grad_norm": 0.31795641779899597,
      "learning_rate": 1.5585198672971325e-05,
      "loss": 0.9391,
      "step": 5010
    },
    {
      "epoch": 0.2552369330892821,
      "grad_norm": 0.518377959728241,
      "learning_rate": 1.5585119229204798e-05,
      "loss": 0.9133,
      "step": 5020
    },
    {
      "epoch": 0.25574537319503765,
      "grad_norm": 0.2335543930530548,
      "learning_rate": 1.5585039785438277e-05,
      "loss": 0.92,
      "step": 5030
    },
    {
      "epoch": 0.25625381330079317,
      "grad_norm": 0.46409666538238525,
      "learning_rate": 1.5584960341671754e-05,
      "loss": 0.905,
      "step": 5040
    },
    {
      "epoch": 0.25676225340654873,
      "grad_norm": 0.24434645473957062,
      "learning_rate": 1.5584880897905227e-05,
      "loss": 0.9166,
      "step": 5050
    },
    {
      "epoch": 0.25727069351230425,
      "grad_norm": 0.5131525993347168,
      "learning_rate": 1.5584801454138703e-05,
      "loss": 0.9075,
      "step": 5060
    },
    {
      "epoch": 0.2577791336180598,
      "grad_norm": 0.35348573327064514,
      "learning_rate": 1.558472201037218e-05,
      "loss": 0.9179,
      "step": 5070
    },
    {
      "epoch": 0.25828757372381533,
      "grad_norm": 0.46248164772987366,
      "learning_rate": 1.5584642566605656e-05,
      "loss": 0.921,
      "step": 5080
    },
    {
      "epoch": 0.2587960138295709,
      "grad_norm": 0.17062248289585114,
      "learning_rate": 1.558456312283913e-05,
      "loss": 0.923,
      "step": 5090
    },
    {
      "epoch": 0.2593044539353264,
      "grad_norm": 0.6235106587409973,
      "learning_rate": 1.5584483679072605e-05,
      "loss": 0.9312,
      "step": 5100
    },
    {
      "epoch": 0.259812894041082,
      "grad_norm": 0.49664074182510376,
      "learning_rate": 1.5584404235306082e-05,
      "loss": 0.927,
      "step": 5110
    },
    {
      "epoch": 0.2603213341468375,
      "grad_norm": 0.706820011138916,
      "learning_rate": 1.5584324791539558e-05,
      "loss": 0.9296,
      "step": 5120
    },
    {
      "epoch": 0.26082977425259307,
      "grad_norm": 0.8276750445365906,
      "learning_rate": 1.5584245347773035e-05,
      "loss": 0.8991,
      "step": 5130
    },
    {
      "epoch": 0.2613382143583486,
      "grad_norm": 0.7378729581832886,
      "learning_rate": 1.5584165904006508e-05,
      "loss": 0.9049,
      "step": 5140
    },
    {
      "epoch": 0.26184665446410416,
      "grad_norm": 0.6106716394424438,
      "learning_rate": 1.5584086460239984e-05,
      "loss": 0.9063,
      "step": 5150
    },
    {
      "epoch": 0.26235509456985967,
      "grad_norm": 0.7101808190345764,
      "learning_rate": 1.558400701647346e-05,
      "loss": 0.9273,
      "step": 5160
    },
    {
      "epoch": 0.26286353467561524,
      "grad_norm": 0.17281490564346313,
      "learning_rate": 1.5583927572706937e-05,
      "loss": 0.9213,
      "step": 5170
    },
    {
      "epoch": 0.26337197478137075,
      "grad_norm": 0.13130633533000946,
      "learning_rate": 1.5583848128940413e-05,
      "loss": 0.9346,
      "step": 5180
    },
    {
      "epoch": 0.26388041488712627,
      "grad_norm": 0.5196704268455505,
      "learning_rate": 1.5583768685173886e-05,
      "loss": 0.9259,
      "step": 5190
    },
    {
      "epoch": 0.26438885499288184,
      "grad_norm": 0.3630594313144684,
      "learning_rate": 1.5583689241407363e-05,
      "loss": 0.9294,
      "step": 5200
    },
    {
      "epoch": 0.26489729509863735,
      "grad_norm": 0.4572731554508209,
      "learning_rate": 1.558360979764084e-05,
      "loss": 0.935,
      "step": 5210
    },
    {
      "epoch": 0.2654057352043929,
      "grad_norm": 0.5037428140640259,
      "learning_rate": 1.5583530353874315e-05,
      "loss": 0.931,
      "step": 5220
    },
    {
      "epoch": 0.26591417531014844,
      "grad_norm": 0.4893486797809601,
      "learning_rate": 1.5583450910107792e-05,
      "loss": 0.9289,
      "step": 5230
    },
    {
      "epoch": 0.266422615415904,
      "grad_norm": 0.7773874402046204,
      "learning_rate": 1.5583371466341265e-05,
      "loss": 0.9172,
      "step": 5240
    },
    {
      "epoch": 0.2669310555216595,
      "grad_norm": 0.2834521234035492,
      "learning_rate": 1.558329202257474e-05,
      "loss": 0.9348,
      "step": 5250
    },
    {
      "epoch": 0.2674394956274151,
      "grad_norm": 0.48883742094039917,
      "learning_rate": 1.5583212578808217e-05,
      "loss": 0.926,
      "step": 5260
    },
    {
      "epoch": 0.2679479357331706,
      "grad_norm": 0.1575808823108673,
      "learning_rate": 1.5583133135041694e-05,
      "loss": 0.9396,
      "step": 5270
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 0.30931568145751953,
      "learning_rate": 1.558305369127517e-05,
      "loss": 0.9493,
      "step": 5280
    },
    {
      "epoch": 0.2689648159446817,
      "grad_norm": 0.5298383235931396,
      "learning_rate": 1.5582974247508643e-05,
      "loss": 0.8894,
      "step": 5290
    },
    {
      "epoch": 0.26947325605043726,
      "grad_norm": 0.64400315284729,
      "learning_rate": 1.558289480374212e-05,
      "loss": 0.9044,
      "step": 5300
    },
    {
      "epoch": 0.2699816961561928,
      "grad_norm": 0.6602242588996887,
      "learning_rate": 1.5582815359975596e-05,
      "loss": 0.8895,
      "step": 5310
    },
    {
      "epoch": 0.27049013626194834,
      "grad_norm": 0.5283052921295166,
      "learning_rate": 1.5582735916209072e-05,
      "loss": 0.9392,
      "step": 5320
    },
    {
      "epoch": 0.27099857636770386,
      "grad_norm": 0.37336641550064087,
      "learning_rate": 1.5582656472442545e-05,
      "loss": 0.9119,
      "step": 5330
    },
    {
      "epoch": 0.27150701647345943,
      "grad_norm": 0.24837815761566162,
      "learning_rate": 1.5582577028676022e-05,
      "loss": 0.9236,
      "step": 5340
    },
    {
      "epoch": 0.27201545657921494,
      "grad_norm": 0.3294875919818878,
      "learning_rate": 1.5582497584909498e-05,
      "loss": 0.9232,
      "step": 5350
    },
    {
      "epoch": 0.2725238966849705,
      "grad_norm": 0.6911300420761108,
      "learning_rate": 1.5582418141142975e-05,
      "loss": 0.9413,
      "step": 5360
    },
    {
      "epoch": 0.273032336790726,
      "grad_norm": 0.40533867478370667,
      "learning_rate": 1.558233869737645e-05,
      "loss": 0.9111,
      "step": 5370
    },
    {
      "epoch": 0.2735407768964816,
      "grad_norm": 0.5075437426567078,
      "learning_rate": 1.5582259253609924e-05,
      "loss": 0.9189,
      "step": 5380
    },
    {
      "epoch": 0.2740492170022371,
      "grad_norm": 0.2732757031917572,
      "learning_rate": 1.55821798098434e-05,
      "loss": 0.9154,
      "step": 5390
    },
    {
      "epoch": 0.2745576571079927,
      "grad_norm": 0.38043099641799927,
      "learning_rate": 1.5582100366076877e-05,
      "loss": 0.93,
      "step": 5400
    },
    {
      "epoch": 0.2750660972137482,
      "grad_norm": 0.4689781069755554,
      "learning_rate": 1.5582020922310353e-05,
      "loss": 0.9347,
      "step": 5410
    },
    {
      "epoch": 0.27557453731950377,
      "grad_norm": 0.33999770879745483,
      "learning_rate": 1.558194147854383e-05,
      "loss": 0.9163,
      "step": 5420
    },
    {
      "epoch": 0.2760829774252593,
      "grad_norm": 0.3625251352787018,
      "learning_rate": 1.5581862034777303e-05,
      "loss": 0.9267,
      "step": 5430
    },
    {
      "epoch": 0.27659141753101485,
      "grad_norm": 0.24770204722881317,
      "learning_rate": 1.558178259101078e-05,
      "loss": 0.9144,
      "step": 5440
    },
    {
      "epoch": 0.27709985763677036,
      "grad_norm": 0.3591572642326355,
      "learning_rate": 1.5581703147244255e-05,
      "loss": 0.9441,
      "step": 5450
    },
    {
      "epoch": 0.27760829774252593,
      "grad_norm": 0.637691080570221,
      "learning_rate": 1.5581623703477732e-05,
      "loss": 0.904,
      "step": 5460
    },
    {
      "epoch": 0.27811673784828145,
      "grad_norm": 0.4757474362850189,
      "learning_rate": 1.5581544259711208e-05,
      "loss": 0.9095,
      "step": 5470
    },
    {
      "epoch": 0.278625177954037,
      "grad_norm": 0.3989434540271759,
      "learning_rate": 1.558146481594468e-05,
      "loss": 0.9081,
      "step": 5480
    },
    {
      "epoch": 0.27913361805979253,
      "grad_norm": 0.2445446401834488,
      "learning_rate": 1.5581385372178158e-05,
      "loss": 0.9165,
      "step": 5490
    },
    {
      "epoch": 0.2796420581655481,
      "grad_norm": 0.5796127915382385,
      "learning_rate": 1.5581305928411634e-05,
      "loss": 0.927,
      "step": 5500
    },
    {
      "epoch": 0.2801504982713036,
      "grad_norm": 0.532280445098877,
      "learning_rate": 1.558122648464511e-05,
      "loss": 0.9234,
      "step": 5510
    },
    {
      "epoch": 0.2806589383770592,
      "grad_norm": 0.7085835933685303,
      "learning_rate": 1.5581147040878587e-05,
      "loss": 0.9198,
      "step": 5520
    },
    {
      "epoch": 0.2811673784828147,
      "grad_norm": 0.4419439733028412,
      "learning_rate": 1.558106759711206e-05,
      "loss": 0.9085,
      "step": 5530
    },
    {
      "epoch": 0.28167581858857027,
      "grad_norm": 0.37444621324539185,
      "learning_rate": 1.5580988153345536e-05,
      "loss": 0.9234,
      "step": 5540
    },
    {
      "epoch": 0.2821842586943258,
      "grad_norm": 0.42459067702293396,
      "learning_rate": 1.5580908709579013e-05,
      "loss": 0.8906,
      "step": 5550
    },
    {
      "epoch": 0.28269269880008135,
      "grad_norm": 0.18399955332279205,
      "learning_rate": 1.558082926581249e-05,
      "loss": 0.9338,
      "step": 5560
    },
    {
      "epoch": 0.28320113890583687,
      "grad_norm": 0.5627602934837341,
      "learning_rate": 1.5580749822045962e-05,
      "loss": 0.9156,
      "step": 5570
    },
    {
      "epoch": 0.28370957901159244,
      "grad_norm": 0.5650265216827393,
      "learning_rate": 1.558067037827944e-05,
      "loss": 0.9297,
      "step": 5580
    },
    {
      "epoch": 0.28421801911734795,
      "grad_norm": 0.1680743396282196,
      "learning_rate": 1.5580590934512915e-05,
      "loss": 0.9299,
      "step": 5590
    },
    {
      "epoch": 0.2847264592231035,
      "grad_norm": 0.4567338824272156,
      "learning_rate": 1.558051149074639e-05,
      "loss": 0.9236,
      "step": 5600
    },
    {
      "epoch": 0.28523489932885904,
      "grad_norm": 0.403474360704422,
      "learning_rate": 1.5580432046979868e-05,
      "loss": 0.9189,
      "step": 5610
    },
    {
      "epoch": 0.2857433394346146,
      "grad_norm": 0.573032796382904,
      "learning_rate": 1.558035260321334e-05,
      "loss": 0.8888,
      "step": 5620
    },
    {
      "epoch": 0.2862517795403701,
      "grad_norm": 1.0393223762512207,
      "learning_rate": 1.5580273159446817e-05,
      "loss": 0.8971,
      "step": 5630
    },
    {
      "epoch": 0.2867602196461257,
      "grad_norm": 0.3527161478996277,
      "learning_rate": 1.5580193715680293e-05,
      "loss": 0.9092,
      "step": 5640
    },
    {
      "epoch": 0.2872686597518812,
      "grad_norm": 0.4564632475376129,
      "learning_rate": 1.558011427191377e-05,
      "loss": 0.9064,
      "step": 5650
    },
    {
      "epoch": 0.2877770998576368,
      "grad_norm": 0.3577762246131897,
      "learning_rate": 1.5580034828147246e-05,
      "loss": 0.9126,
      "step": 5660
    },
    {
      "epoch": 0.2882855399633923,
      "grad_norm": 0.5342820882797241,
      "learning_rate": 1.557995538438072e-05,
      "loss": 0.9196,
      "step": 5670
    },
    {
      "epoch": 0.28879398006914786,
      "grad_norm": 0.5631869435310364,
      "learning_rate": 1.5579875940614196e-05,
      "loss": 0.8984,
      "step": 5680
    },
    {
      "epoch": 0.2893024201749034,
      "grad_norm": 0.4097402095794678,
      "learning_rate": 1.5579796496847672e-05,
      "loss": 0.9281,
      "step": 5690
    },
    {
      "epoch": 0.28981086028065894,
      "grad_norm": 0.448248952627182,
      "learning_rate": 1.557971705308115e-05,
      "loss": 0.9359,
      "step": 5700
    },
    {
      "epoch": 0.29031930038641446,
      "grad_norm": 0.586561381816864,
      "learning_rate": 1.5579637609314625e-05,
      "loss": 0.9085,
      "step": 5710
    },
    {
      "epoch": 0.29082774049217003,
      "grad_norm": 0.3342472314834595,
      "learning_rate": 1.5579558165548098e-05,
      "loss": 0.9094,
      "step": 5720
    },
    {
      "epoch": 0.29133618059792554,
      "grad_norm": 0.346707284450531,
      "learning_rate": 1.5579478721781574e-05,
      "loss": 0.9185,
      "step": 5730
    },
    {
      "epoch": 0.2918446207036811,
      "grad_norm": 0.27672848105430603,
      "learning_rate": 1.557939927801505e-05,
      "loss": 0.924,
      "step": 5740
    },
    {
      "epoch": 0.2923530608094366,
      "grad_norm": 0.22713018953800201,
      "learning_rate": 1.5579319834248527e-05,
      "loss": 0.9264,
      "step": 5750
    },
    {
      "epoch": 0.2928615009151922,
      "grad_norm": 0.5629589557647705,
      "learning_rate": 1.5579240390482003e-05,
      "loss": 0.9015,
      "step": 5760
    },
    {
      "epoch": 0.2933699410209477,
      "grad_norm": 0.7814351916313171,
      "learning_rate": 1.5579160946715476e-05,
      "loss": 0.9162,
      "step": 5770
    },
    {
      "epoch": 0.2938783811267033,
      "grad_norm": 0.39118748903274536,
      "learning_rate": 1.5579081502948953e-05,
      "loss": 0.9311,
      "step": 5780
    },
    {
      "epoch": 0.2943868212324588,
      "grad_norm": 0.5674227476119995,
      "learning_rate": 1.557900205918243e-05,
      "loss": 0.9364,
      "step": 5790
    },
    {
      "epoch": 0.29489526133821437,
      "grad_norm": 0.3918518126010895,
      "learning_rate": 1.5578922615415905e-05,
      "loss": 0.9056,
      "step": 5800
    },
    {
      "epoch": 0.2954037014439699,
      "grad_norm": 0.7578901052474976,
      "learning_rate": 1.557884317164938e-05,
      "loss": 0.9153,
      "step": 5810
    },
    {
      "epoch": 0.29591214154972545,
      "grad_norm": 0.5420162677764893,
      "learning_rate": 1.5578763727882855e-05,
      "loss": 0.889,
      "step": 5820
    },
    {
      "epoch": 0.29642058165548096,
      "grad_norm": 0.4121125042438507,
      "learning_rate": 1.557868428411633e-05,
      "loss": 0.9109,
      "step": 5830
    },
    {
      "epoch": 0.29692902176123653,
      "grad_norm": 0.3482031226158142,
      "learning_rate": 1.5578604840349808e-05,
      "loss": 0.9191,
      "step": 5840
    },
    {
      "epoch": 0.29743746186699205,
      "grad_norm": 0.554567813873291,
      "learning_rate": 1.5578525396583284e-05,
      "loss": 0.9136,
      "step": 5850
    },
    {
      "epoch": 0.2979459019727476,
      "grad_norm": 0.49978137016296387,
      "learning_rate": 1.5578445952816757e-05,
      "loss": 0.8974,
      "step": 5860
    },
    {
      "epoch": 0.29845434207850313,
      "grad_norm": 0.47792890667915344,
      "learning_rate": 1.5578366509050233e-05,
      "loss": 0.9255,
      "step": 5870
    },
    {
      "epoch": 0.2989627821842587,
      "grad_norm": 0.29827171564102173,
      "learning_rate": 1.557828706528371e-05,
      "loss": 0.9277,
      "step": 5880
    },
    {
      "epoch": 0.2994712222900142,
      "grad_norm": 0.2841752767562866,
      "learning_rate": 1.5578207621517186e-05,
      "loss": 0.9178,
      "step": 5890
    },
    {
      "epoch": 0.2999796623957698,
      "grad_norm": 0.34711048007011414,
      "learning_rate": 1.5578128177750663e-05,
      "loss": 0.9075,
      "step": 5900
    },
    {
      "epoch": 0.3004881025015253,
      "grad_norm": 0.5315037965774536,
      "learning_rate": 1.5578048733984136e-05,
      "loss": 0.9096,
      "step": 5910
    },
    {
      "epoch": 0.30099654260728087,
      "grad_norm": 0.28207629919052124,
      "learning_rate": 1.5577969290217612e-05,
      "loss": 0.911,
      "step": 5920
    },
    {
      "epoch": 0.3015049827130364,
      "grad_norm": 0.4790214002132416,
      "learning_rate": 1.557788984645109e-05,
      "loss": 0.9209,
      "step": 5930
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 0.4102950692176819,
      "learning_rate": 1.5577810402684565e-05,
      "loss": 0.9181,
      "step": 5940
    },
    {
      "epoch": 0.30252186292454747,
      "grad_norm": 0.368898868560791,
      "learning_rate": 1.557773095891804e-05,
      "loss": 0.9285,
      "step": 5950
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.6996492743492126,
      "learning_rate": 1.5577651515151514e-05,
      "loss": 0.9007,
      "step": 5960
    },
    {
      "epoch": 0.30353874313605855,
      "grad_norm": 0.27209949493408203,
      "learning_rate": 1.557757207138499e-05,
      "loss": 0.9274,
      "step": 5970
    },
    {
      "epoch": 0.3040471832418141,
      "grad_norm": 0.19616200029850006,
      "learning_rate": 1.5577492627618467e-05,
      "loss": 0.9109,
      "step": 5980
    },
    {
      "epoch": 0.30455562334756964,
      "grad_norm": 0.29975181818008423,
      "learning_rate": 1.5577413183851943e-05,
      "loss": 0.9121,
      "step": 5990
    },
    {
      "epoch": 0.3050640634533252,
      "grad_norm": 0.34382855892181396,
      "learning_rate": 1.557733374008542e-05,
      "loss": 0.9217,
      "step": 6000
    },
    {
      "epoch": 0.3055725035590807,
      "grad_norm": 0.5597508549690247,
      "learning_rate": 1.5577254296318893e-05,
      "loss": 0.8974,
      "step": 6010
    },
    {
      "epoch": 0.3060809436648363,
      "grad_norm": 0.459071546792984,
      "learning_rate": 1.557717485255237e-05,
      "loss": 0.9109,
      "step": 6020
    },
    {
      "epoch": 0.3065893837705918,
      "grad_norm": 0.5812073945999146,
      "learning_rate": 1.5577095408785846e-05,
      "loss": 0.9261,
      "step": 6030
    },
    {
      "epoch": 0.3070978238763474,
      "grad_norm": 1.0515512228012085,
      "learning_rate": 1.5577015965019322e-05,
      "loss": 0.889,
      "step": 6040
    },
    {
      "epoch": 0.3076062639821029,
      "grad_norm": 0.4127923548221588,
      "learning_rate": 1.5576936521252795e-05,
      "loss": 0.9263,
      "step": 6050
    },
    {
      "epoch": 0.30811470408785846,
      "grad_norm": 0.3456774353981018,
      "learning_rate": 1.557685707748627e-05,
      "loss": 0.9177,
      "step": 6060
    },
    {
      "epoch": 0.308623144193614,
      "grad_norm": 0.8050614595413208,
      "learning_rate": 1.5576777633719748e-05,
      "loss": 0.9136,
      "step": 6070
    },
    {
      "epoch": 0.30913158429936954,
      "grad_norm": 0.2668887674808502,
      "learning_rate": 1.5576698189953224e-05,
      "loss": 0.9156,
      "step": 6080
    },
    {
      "epoch": 0.30964002440512506,
      "grad_norm": 0.3543250560760498,
      "learning_rate": 1.55766187461867e-05,
      "loss": 0.9066,
      "step": 6090
    },
    {
      "epoch": 0.31014846451088063,
      "grad_norm": 0.6837785243988037,
      "learning_rate": 1.5576539302420174e-05,
      "loss": 0.8963,
      "step": 6100
    },
    {
      "epoch": 0.31065690461663614,
      "grad_norm": 0.30703797936439514,
      "learning_rate": 1.557645985865365e-05,
      "loss": 0.9235,
      "step": 6110
    },
    {
      "epoch": 0.3111653447223917,
      "grad_norm": 0.3698139488697052,
      "learning_rate": 1.5576380414887126e-05,
      "loss": 0.8901,
      "step": 6120
    },
    {
      "epoch": 0.3116737848281472,
      "grad_norm": 0.37115389108657837,
      "learning_rate": 1.5576300971120603e-05,
      "loss": 0.8916,
      "step": 6130
    },
    {
      "epoch": 0.3121822249339028,
      "grad_norm": 0.6515184044837952,
      "learning_rate": 1.557622152735408e-05,
      "loss": 0.9093,
      "step": 6140
    },
    {
      "epoch": 0.3126906650396583,
      "grad_norm": 0.5684798359870911,
      "learning_rate": 1.5576142083587552e-05,
      "loss": 0.9161,
      "step": 6150
    },
    {
      "epoch": 0.3131991051454139,
      "grad_norm": 0.29207170009613037,
      "learning_rate": 1.557606263982103e-05,
      "loss": 0.922,
      "step": 6160
    },
    {
      "epoch": 0.3137075452511694,
      "grad_norm": 0.3906685411930084,
      "learning_rate": 1.5575983196054505e-05,
      "loss": 0.9092,
      "step": 6170
    },
    {
      "epoch": 0.31421598535692497,
      "grad_norm": 0.4376322031021118,
      "learning_rate": 1.557590375228798e-05,
      "loss": 0.9237,
      "step": 6180
    },
    {
      "epoch": 0.3147244254626805,
      "grad_norm": 0.4292617738246918,
      "learning_rate": 1.5575824308521458e-05,
      "loss": 0.9039,
      "step": 6190
    },
    {
      "epoch": 0.31523286556843605,
      "grad_norm": 0.4456755816936493,
      "learning_rate": 1.557574486475493e-05,
      "loss": 0.8773,
      "step": 6200
    },
    {
      "epoch": 0.31574130567419156,
      "grad_norm": 0.4345889389514923,
      "learning_rate": 1.5575665420988407e-05,
      "loss": 0.8776,
      "step": 6210
    },
    {
      "epoch": 0.31624974577994713,
      "grad_norm": 0.32598933577537537,
      "learning_rate": 1.5575585977221884e-05,
      "loss": 0.9163,
      "step": 6220
    },
    {
      "epoch": 0.31675818588570265,
      "grad_norm": 0.27743300795555115,
      "learning_rate": 1.557550653345536e-05,
      "loss": 0.9201,
      "step": 6230
    },
    {
      "epoch": 0.3172666259914582,
      "grad_norm": 0.35290634632110596,
      "learning_rate": 1.5575427089688833e-05,
      "loss": 0.918,
      "step": 6240
    },
    {
      "epoch": 0.31777506609721373,
      "grad_norm": 0.4844241738319397,
      "learning_rate": 1.557534764592231e-05,
      "loss": 0.9063,
      "step": 6250
    },
    {
      "epoch": 0.3182835062029693,
      "grad_norm": 0.1956118643283844,
      "learning_rate": 1.5575268202155786e-05,
      "loss": 0.911,
      "step": 6260
    },
    {
      "epoch": 0.3187919463087248,
      "grad_norm": 0.4049876034259796,
      "learning_rate": 1.5575188758389262e-05,
      "loss": 0.9109,
      "step": 6270
    },
    {
      "epoch": 0.3193003864144804,
      "grad_norm": 0.5067012906074524,
      "learning_rate": 1.557510931462274e-05,
      "loss": 0.921,
      "step": 6280
    },
    {
      "epoch": 0.3198088265202359,
      "grad_norm": 0.3093028962612152,
      "learning_rate": 1.557502987085621e-05,
      "loss": 0.9215,
      "step": 6290
    },
    {
      "epoch": 0.32031726662599147,
      "grad_norm": 0.6550117135047913,
      "learning_rate": 1.5574950427089688e-05,
      "loss": 0.9137,
      "step": 6300
    },
    {
      "epoch": 0.320825706731747,
      "grad_norm": 0.6527698636054993,
      "learning_rate": 1.5574870983323164e-05,
      "loss": 0.8801,
      "step": 6310
    },
    {
      "epoch": 0.32133414683750255,
      "grad_norm": 0.5478243231773376,
      "learning_rate": 1.557479153955664e-05,
      "loss": 0.9216,
      "step": 6320
    },
    {
      "epoch": 0.32184258694325807,
      "grad_norm": 0.6398358941078186,
      "learning_rate": 1.5574712095790117e-05,
      "loss": 0.9122,
      "step": 6330
    },
    {
      "epoch": 0.32235102704901364,
      "grad_norm": 0.7597556114196777,
      "learning_rate": 1.557463265202359e-05,
      "loss": 0.932,
      "step": 6340
    },
    {
      "epoch": 0.32285946715476915,
      "grad_norm": 1.1144399642944336,
      "learning_rate": 1.5574553208257066e-05,
      "loss": 0.9067,
      "step": 6350
    },
    {
      "epoch": 0.3233679072605247,
      "grad_norm": 0.9417703747749329,
      "learning_rate": 1.5574473764490543e-05,
      "loss": 0.9011,
      "step": 6360
    },
    {
      "epoch": 0.32387634736628024,
      "grad_norm": 0.47066807746887207,
      "learning_rate": 1.557439432072402e-05,
      "loss": 0.8949,
      "step": 6370
    },
    {
      "epoch": 0.3243847874720358,
      "grad_norm": 0.5167639851570129,
      "learning_rate": 1.5574314876957496e-05,
      "loss": 0.9086,
      "step": 6380
    },
    {
      "epoch": 0.3248932275777913,
      "grad_norm": 0.35859501361846924,
      "learning_rate": 1.557423543319097e-05,
      "loss": 0.8933,
      "step": 6390
    },
    {
      "epoch": 0.3254016676835469,
      "grad_norm": 0.9059486985206604,
      "learning_rate": 1.5574155989424445e-05,
      "loss": 0.919,
      "step": 6400
    },
    {
      "epoch": 0.3259101077893024,
      "grad_norm": 0.5321922302246094,
      "learning_rate": 1.557407654565792e-05,
      "loss": 0.9182,
      "step": 6410
    },
    {
      "epoch": 0.326418547895058,
      "grad_norm": 0.5010356903076172,
      "learning_rate": 1.5573997101891398e-05,
      "loss": 0.903,
      "step": 6420
    },
    {
      "epoch": 0.3269269880008135,
      "grad_norm": 0.29924964904785156,
      "learning_rate": 1.5573917658124874e-05,
      "loss": 0.8893,
      "step": 6430
    },
    {
      "epoch": 0.32743542810656906,
      "grad_norm": 0.6798638105392456,
      "learning_rate": 1.5573838214358347e-05,
      "loss": 0.9122,
      "step": 6440
    },
    {
      "epoch": 0.3279438682123246,
      "grad_norm": 0.3659733533859253,
      "learning_rate": 1.5573758770591824e-05,
      "loss": 0.9138,
      "step": 6450
    },
    {
      "epoch": 0.32845230831808014,
      "grad_norm": 0.22838257253170013,
      "learning_rate": 1.55736793268253e-05,
      "loss": 0.9387,
      "step": 6460
    },
    {
      "epoch": 0.32896074842383566,
      "grad_norm": 0.41642704606056213,
      "learning_rate": 1.5573599883058776e-05,
      "loss": 0.9091,
      "step": 6470
    },
    {
      "epoch": 0.32946918852959123,
      "grad_norm": 0.48042160272598267,
      "learning_rate": 1.557352043929225e-05,
      "loss": 0.9018,
      "step": 6480
    },
    {
      "epoch": 0.32997762863534674,
      "grad_norm": 0.3188575506210327,
      "learning_rate": 1.5573440995525726e-05,
      "loss": 0.9015,
      "step": 6490
    },
    {
      "epoch": 0.3304860687411023,
      "grad_norm": 0.4599981904029846,
      "learning_rate": 1.5573361551759202e-05,
      "loss": 0.9049,
      "step": 6500
    },
    {
      "epoch": 0.3309945088468578,
      "grad_norm": 0.3026394844055176,
      "learning_rate": 1.557328210799268e-05,
      "loss": 0.9072,
      "step": 6510
    },
    {
      "epoch": 0.3315029489526134,
      "grad_norm": 0.43982183933258057,
      "learning_rate": 1.5573202664226155e-05,
      "loss": 0.9006,
      "step": 6520
    },
    {
      "epoch": 0.3320113890583689,
      "grad_norm": 0.4533126652240753,
      "learning_rate": 1.5573123220459628e-05,
      "loss": 0.9201,
      "step": 6530
    },
    {
      "epoch": 0.3325198291641245,
      "grad_norm": 0.3714788258075714,
      "learning_rate": 1.5573043776693104e-05,
      "loss": 0.913,
      "step": 6540
    },
    {
      "epoch": 0.33302826926988,
      "grad_norm": 0.2347366064786911,
      "learning_rate": 1.557296433292658e-05,
      "loss": 0.9198,
      "step": 6550
    },
    {
      "epoch": 0.33353670937563556,
      "grad_norm": 0.528487503528595,
      "learning_rate": 1.5572884889160057e-05,
      "loss": 0.8817,
      "step": 6560
    },
    {
      "epoch": 0.3340451494813911,
      "grad_norm": 0.3180244266986847,
      "learning_rate": 1.5572805445393534e-05,
      "loss": 0.9152,
      "step": 6570
    },
    {
      "epoch": 0.33455358958714665,
      "grad_norm": 0.445928156375885,
      "learning_rate": 1.5572726001627007e-05,
      "loss": 0.9357,
      "step": 6580
    },
    {
      "epoch": 0.33506202969290216,
      "grad_norm": 0.32171571254730225,
      "learning_rate": 1.5572646557860483e-05,
      "loss": 0.8814,
      "step": 6590
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 0.5207514762878418,
      "learning_rate": 1.557256711409396e-05,
      "loss": 0.8951,
      "step": 6600
    },
    {
      "epoch": 0.33607890990441325,
      "grad_norm": 0.2259030044078827,
      "learning_rate": 1.5572487670327436e-05,
      "loss": 0.9273,
      "step": 6610
    },
    {
      "epoch": 0.3365873500101688,
      "grad_norm": 0.34846800565719604,
      "learning_rate": 1.5572408226560912e-05,
      "loss": 0.8924,
      "step": 6620
    },
    {
      "epoch": 0.33709579011592433,
      "grad_norm": 0.6727127432823181,
      "learning_rate": 1.5572328782794385e-05,
      "loss": 0.8855,
      "step": 6630
    },
    {
      "epoch": 0.3376042302216799,
      "grad_norm": 0.535394012928009,
      "learning_rate": 1.557224933902786e-05,
      "loss": 0.8975,
      "step": 6640
    },
    {
      "epoch": 0.3381126703274354,
      "grad_norm": 0.5105283856391907,
      "learning_rate": 1.5572169895261338e-05,
      "loss": 0.9271,
      "step": 6650
    },
    {
      "epoch": 0.338621110433191,
      "grad_norm": 0.386129766702652,
      "learning_rate": 1.5572090451494814e-05,
      "loss": 0.9092,
      "step": 6660
    },
    {
      "epoch": 0.3391295505389465,
      "grad_norm": 0.5011657476425171,
      "learning_rate": 1.557201100772829e-05,
      "loss": 0.9193,
      "step": 6670
    },
    {
      "epoch": 0.33963799064470207,
      "grad_norm": 0.5305030941963196,
      "learning_rate": 1.5571931563961764e-05,
      "loss": 0.8895,
      "step": 6680
    },
    {
      "epoch": 0.3401464307504576,
      "grad_norm": 0.3133974075317383,
      "learning_rate": 1.5571852120195244e-05,
      "loss": 0.9142,
      "step": 6690
    },
    {
      "epoch": 0.34065487085621315,
      "grad_norm": 0.7483896017074585,
      "learning_rate": 1.5571772676428717e-05,
      "loss": 0.928,
      "step": 6700
    },
    {
      "epoch": 0.34116331096196867,
      "grad_norm": 0.6134684681892395,
      "learning_rate": 1.5571693232662193e-05,
      "loss": 0.9018,
      "step": 6710
    },
    {
      "epoch": 0.34167175106772424,
      "grad_norm": 0.47747376561164856,
      "learning_rate": 1.557161378889567e-05,
      "loss": 0.9202,
      "step": 6720
    },
    {
      "epoch": 0.34218019117347975,
      "grad_norm": 0.3655509054660797,
      "learning_rate": 1.5571534345129146e-05,
      "loss": 0.8853,
      "step": 6730
    },
    {
      "epoch": 0.3426886312792353,
      "grad_norm": 0.8544896841049194,
      "learning_rate": 1.5571454901362622e-05,
      "loss": 0.9095,
      "step": 6740
    },
    {
      "epoch": 0.34319707138499084,
      "grad_norm": 0.5468584895133972,
      "learning_rate": 1.5571375457596095e-05,
      "loss": 0.9061,
      "step": 6750
    },
    {
      "epoch": 0.3437055114907464,
      "grad_norm": 0.6532013416290283,
      "learning_rate": 1.557129601382957e-05,
      "loss": 0.8811,
      "step": 6760
    },
    {
      "epoch": 0.3442139515965019,
      "grad_norm": 0.510958731174469,
      "learning_rate": 1.5571216570063048e-05,
      "loss": 0.9282,
      "step": 6770
    },
    {
      "epoch": 0.3447223917022575,
      "grad_norm": 0.8288760781288147,
      "learning_rate": 1.5571137126296524e-05,
      "loss": 0.8866,
      "step": 6780
    },
    {
      "epoch": 0.345230831808013,
      "grad_norm": 0.47577017545700073,
      "learning_rate": 1.557105768253e-05,
      "loss": 0.8951,
      "step": 6790
    },
    {
      "epoch": 0.3457392719137686,
      "grad_norm": 0.5020607709884644,
      "learning_rate": 1.5570978238763474e-05,
      "loss": 0.8873,
      "step": 6800
    },
    {
      "epoch": 0.3462477120195241,
      "grad_norm": 0.4920399785041809,
      "learning_rate": 1.557089879499695e-05,
      "loss": 0.9069,
      "step": 6810
    },
    {
      "epoch": 0.34675615212527966,
      "grad_norm": 0.326558917760849,
      "learning_rate": 1.5570819351230426e-05,
      "loss": 0.9155,
      "step": 6820
    },
    {
      "epoch": 0.3472645922310352,
      "grad_norm": 0.3678770065307617,
      "learning_rate": 1.5570739907463903e-05,
      "loss": 0.9196,
      "step": 6830
    },
    {
      "epoch": 0.34777303233679074,
      "grad_norm": 0.1599654257297516,
      "learning_rate": 1.5570660463697376e-05,
      "loss": 0.9053,
      "step": 6840
    },
    {
      "epoch": 0.34828147244254626,
      "grad_norm": 0.26559510827064514,
      "learning_rate": 1.5570581019930852e-05,
      "loss": 0.9165,
      "step": 6850
    },
    {
      "epoch": 0.34878991254830183,
      "grad_norm": 0.4608396589756012,
      "learning_rate": 1.557050157616433e-05,
      "loss": 0.9044,
      "step": 6860
    },
    {
      "epoch": 0.34929835265405734,
      "grad_norm": 0.28558018803596497,
      "learning_rate": 1.5570422132397805e-05,
      "loss": 0.9328,
      "step": 6870
    },
    {
      "epoch": 0.3498067927598129,
      "grad_norm": 0.44556713104248047,
      "learning_rate": 1.557034268863128e-05,
      "loss": 0.9134,
      "step": 6880
    },
    {
      "epoch": 0.3503152328655684,
      "grad_norm": 0.3355732560157776,
      "learning_rate": 1.5570263244864754e-05,
      "loss": 0.9171,
      "step": 6890
    },
    {
      "epoch": 0.350823672971324,
      "grad_norm": 0.4081451892852783,
      "learning_rate": 1.557018380109823e-05,
      "loss": 0.9216,
      "step": 6900
    },
    {
      "epoch": 0.3513321130770795,
      "grad_norm": 0.6288921236991882,
      "learning_rate": 1.5570104357331707e-05,
      "loss": 0.9063,
      "step": 6910
    },
    {
      "epoch": 0.3518405531828351,
      "grad_norm": 0.4307505786418915,
      "learning_rate": 1.5570024913565184e-05,
      "loss": 0.9187,
      "step": 6920
    },
    {
      "epoch": 0.3523489932885906,
      "grad_norm": 0.4711000621318817,
      "learning_rate": 1.556994546979866e-05,
      "loss": 0.9093,
      "step": 6930
    },
    {
      "epoch": 0.35285743339434616,
      "grad_norm": 0.17400908470153809,
      "learning_rate": 1.5569866026032133e-05,
      "loss": 0.9192,
      "step": 6940
    },
    {
      "epoch": 0.3533658735001017,
      "grad_norm": 0.5086269974708557,
      "learning_rate": 1.556978658226561e-05,
      "loss": 0.8934,
      "step": 6950
    },
    {
      "epoch": 0.35387431360585725,
      "grad_norm": 0.4172177016735077,
      "learning_rate": 1.5569707138499086e-05,
      "loss": 0.9038,
      "step": 6960
    },
    {
      "epoch": 0.35438275371161276,
      "grad_norm": 0.6726349592208862,
      "learning_rate": 1.5569627694732562e-05,
      "loss": 0.8787,
      "step": 6970
    },
    {
      "epoch": 0.35489119381736833,
      "grad_norm": 0.7627915740013123,
      "learning_rate": 1.556954825096604e-05,
      "loss": 0.9107,
      "step": 6980
    },
    {
      "epoch": 0.35539963392312385,
      "grad_norm": 0.4402106702327728,
      "learning_rate": 1.556946880719951e-05,
      "loss": 0.8958,
      "step": 6990
    },
    {
      "epoch": 0.3559080740288794,
      "grad_norm": 0.43452635407447815,
      "learning_rate": 1.5569389363432988e-05,
      "loss": 0.9113,
      "step": 7000
    },
    {
      "epoch": 0.35641651413463493,
      "grad_norm": 0.5096470713615417,
      "learning_rate": 1.5569309919666464e-05,
      "loss": 0.8898,
      "step": 7010
    },
    {
      "epoch": 0.3569249542403905,
      "grad_norm": 0.4415799081325531,
      "learning_rate": 1.556923047589994e-05,
      "loss": 0.9198,
      "step": 7020
    },
    {
      "epoch": 0.357433394346146,
      "grad_norm": 0.3185791075229645,
      "learning_rate": 1.5569151032133417e-05,
      "loss": 0.9128,
      "step": 7030
    },
    {
      "epoch": 0.3579418344519016,
      "grad_norm": 0.3427080810070038,
      "learning_rate": 1.556907158836689e-05,
      "loss": 0.9254,
      "step": 7040
    },
    {
      "epoch": 0.3584502745576571,
      "grad_norm": 0.4866602420806885,
      "learning_rate": 1.5568992144600367e-05,
      "loss": 0.9218,
      "step": 7050
    },
    {
      "epoch": 0.35895871466341267,
      "grad_norm": 0.5919674634933472,
      "learning_rate": 1.5568912700833843e-05,
      "loss": 0.8728,
      "step": 7060
    },
    {
      "epoch": 0.3594671547691682,
      "grad_norm": 0.42644697427749634,
      "learning_rate": 1.556883325706732e-05,
      "loss": 0.8893,
      "step": 7070
    },
    {
      "epoch": 0.35997559487492375,
      "grad_norm": 0.5014341473579407,
      "learning_rate": 1.5568753813300792e-05,
      "loss": 0.9015,
      "step": 7080
    },
    {
      "epoch": 0.36048403498067927,
      "grad_norm": 1.0039699077606201,
      "learning_rate": 1.556867436953427e-05,
      "loss": 0.9345,
      "step": 7090
    },
    {
      "epoch": 0.36099247508643484,
      "grad_norm": 0.6480543613433838,
      "learning_rate": 1.5568594925767745e-05,
      "loss": 0.9025,
      "step": 7100
    },
    {
      "epoch": 0.36150091519219035,
      "grad_norm": 0.8737342953681946,
      "learning_rate": 1.556851548200122e-05,
      "loss": 0.8997,
      "step": 7110
    },
    {
      "epoch": 0.3620093552979459,
      "grad_norm": 0.5766962766647339,
      "learning_rate": 1.5568436038234698e-05,
      "loss": 0.9015,
      "step": 7120
    },
    {
      "epoch": 0.36251779540370144,
      "grad_norm": 0.6648457646369934,
      "learning_rate": 1.556835659446817e-05,
      "loss": 0.8779,
      "step": 7130
    },
    {
      "epoch": 0.363026235509457,
      "grad_norm": 0.6315212249755859,
      "learning_rate": 1.5568277150701647e-05,
      "loss": 0.921,
      "step": 7140
    },
    {
      "epoch": 0.3635346756152125,
      "grad_norm": 0.8292899131774902,
      "learning_rate": 1.5568197706935124e-05,
      "loss": 0.8988,
      "step": 7150
    },
    {
      "epoch": 0.3640431157209681,
      "grad_norm": 0.365803986787796,
      "learning_rate": 1.55681182631686e-05,
      "loss": 0.903,
      "step": 7160
    },
    {
      "epoch": 0.3645515558267236,
      "grad_norm": 0.4109044373035431,
      "learning_rate": 1.5568038819402077e-05,
      "loss": 0.8751,
      "step": 7170
    },
    {
      "epoch": 0.3650599959324792,
      "grad_norm": 0.4762786328792572,
      "learning_rate": 1.556795937563555e-05,
      "loss": 0.9121,
      "step": 7180
    },
    {
      "epoch": 0.3655684360382347,
      "grad_norm": 0.5582879781723022,
      "learning_rate": 1.5567879931869026e-05,
      "loss": 0.9172,
      "step": 7190
    },
    {
      "epoch": 0.36607687614399026,
      "grad_norm": 0.40326935052871704,
      "learning_rate": 1.5567800488102502e-05,
      "loss": 0.9281,
      "step": 7200
    },
    {
      "epoch": 0.3665853162497458,
      "grad_norm": 0.4904876947402954,
      "learning_rate": 1.556772104433598e-05,
      "loss": 0.8824,
      "step": 7210
    },
    {
      "epoch": 0.36709375635550134,
      "grad_norm": 0.2503425180912018,
      "learning_rate": 1.5567641600569455e-05,
      "loss": 0.8983,
      "step": 7220
    },
    {
      "epoch": 0.36760219646125686,
      "grad_norm": 0.6332087516784668,
      "learning_rate": 1.5567562156802928e-05,
      "loss": 0.9036,
      "step": 7230
    },
    {
      "epoch": 0.36811063656701243,
      "grad_norm": 0.43944939970970154,
      "learning_rate": 1.5567482713036405e-05,
      "loss": 0.9047,
      "step": 7240
    },
    {
      "epoch": 0.36861907667276794,
      "grad_norm": 0.35324496030807495,
      "learning_rate": 1.556740326926988e-05,
      "loss": 0.8859,
      "step": 7250
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 0.21015392243862152,
      "learning_rate": 1.5567323825503357e-05,
      "loss": 0.9206,
      "step": 7260
    },
    {
      "epoch": 0.369635956884279,
      "grad_norm": 0.41054558753967285,
      "learning_rate": 1.556724438173683e-05,
      "loss": 0.8863,
      "step": 7270
    },
    {
      "epoch": 0.3701443969900346,
      "grad_norm": 0.3393065631389618,
      "learning_rate": 1.5567164937970307e-05,
      "loss": 0.9304,
      "step": 7280
    },
    {
      "epoch": 0.3706528370957901,
      "grad_norm": 0.48199132084846497,
      "learning_rate": 1.5567085494203783e-05,
      "loss": 0.8944,
      "step": 7290
    },
    {
      "epoch": 0.3711612772015457,
      "grad_norm": 0.4952508211135864,
      "learning_rate": 1.556700605043726e-05,
      "loss": 0.8922,
      "step": 7300
    },
    {
      "epoch": 0.3716697173073012,
      "grad_norm": 0.40247952938079834,
      "learning_rate": 1.5566926606670736e-05,
      "loss": 0.887,
      "step": 7310
    },
    {
      "epoch": 0.37217815741305676,
      "grad_norm": 0.4244934618473053,
      "learning_rate": 1.556684716290421e-05,
      "loss": 0.8998,
      "step": 7320
    },
    {
      "epoch": 0.3726865975188123,
      "grad_norm": 0.5585946440696716,
      "learning_rate": 1.5566767719137685e-05,
      "loss": 0.9066,
      "step": 7330
    },
    {
      "epoch": 0.37319503762456785,
      "grad_norm": 0.4858635365962982,
      "learning_rate": 1.556668827537116e-05,
      "loss": 0.9133,
      "step": 7340
    },
    {
      "epoch": 0.37370347773032336,
      "grad_norm": 0.4004499614238739,
      "learning_rate": 1.5566608831604638e-05,
      "loss": 0.9155,
      "step": 7350
    },
    {
      "epoch": 0.37421191783607893,
      "grad_norm": 0.5406991839408875,
      "learning_rate": 1.5566529387838114e-05,
      "loss": 0.8992,
      "step": 7360
    },
    {
      "epoch": 0.37472035794183445,
      "grad_norm": 0.7548460960388184,
      "learning_rate": 1.5566449944071587e-05,
      "loss": 0.902,
      "step": 7370
    },
    {
      "epoch": 0.37522879804759,
      "grad_norm": 0.4347168505191803,
      "learning_rate": 1.5566370500305064e-05,
      "loss": 0.8874,
      "step": 7380
    },
    {
      "epoch": 0.37573723815334553,
      "grad_norm": 0.24685300886631012,
      "learning_rate": 1.556629105653854e-05,
      "loss": 0.9362,
      "step": 7390
    },
    {
      "epoch": 0.3762456782591011,
      "grad_norm": 0.3789605498313904,
      "learning_rate": 1.5566211612772017e-05,
      "loss": 0.912,
      "step": 7400
    },
    {
      "epoch": 0.3767541183648566,
      "grad_norm": 0.46319547295570374,
      "learning_rate": 1.5566132169005493e-05,
      "loss": 0.9195,
      "step": 7410
    },
    {
      "epoch": 0.3772625584706122,
      "grad_norm": 0.3613191843032837,
      "learning_rate": 1.5566052725238966e-05,
      "loss": 0.9087,
      "step": 7420
    },
    {
      "epoch": 0.3777709985763677,
      "grad_norm": 0.5031083226203918,
      "learning_rate": 1.5565973281472442e-05,
      "loss": 0.913,
      "step": 7430
    },
    {
      "epoch": 0.37827943868212327,
      "grad_norm": 0.4105933606624603,
      "learning_rate": 1.556589383770592e-05,
      "loss": 0.8668,
      "step": 7440
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.39598771929740906,
      "learning_rate": 1.5565814393939395e-05,
      "loss": 0.8957,
      "step": 7450
    },
    {
      "epoch": 0.37929631889363435,
      "grad_norm": 0.28484630584716797,
      "learning_rate": 1.556573495017287e-05,
      "loss": 0.9156,
      "step": 7460
    },
    {
      "epoch": 0.37980475899938987,
      "grad_norm": 0.41224542260169983,
      "learning_rate": 1.5565655506406345e-05,
      "loss": 0.9053,
      "step": 7470
    },
    {
      "epoch": 0.38031319910514544,
      "grad_norm": 0.43663427233695984,
      "learning_rate": 1.556557606263982e-05,
      "loss": 0.9096,
      "step": 7480
    },
    {
      "epoch": 0.38082163921090095,
      "grad_norm": 0.5376822352409363,
      "learning_rate": 1.5565496618873297e-05,
      "loss": 0.9009,
      "step": 7490
    },
    {
      "epoch": 0.3813300793166565,
      "grad_norm": 0.541960597038269,
      "learning_rate": 1.5565417175106774e-05,
      "loss": 0.8976,
      "step": 7500
    },
    {
      "epoch": 0.38183851942241204,
      "grad_norm": 0.6035566329956055,
      "learning_rate": 1.5565337731340247e-05,
      "loss": 0.8724,
      "step": 7510
    },
    {
      "epoch": 0.3823469595281676,
      "grad_norm": 0.6116796731948853,
      "learning_rate": 1.5565258287573723e-05,
      "loss": 0.913,
      "step": 7520
    },
    {
      "epoch": 0.3828553996339231,
      "grad_norm": 0.4149698317050934,
      "learning_rate": 1.55651788438072e-05,
      "loss": 0.9083,
      "step": 7530
    },
    {
      "epoch": 0.3833638397396787,
      "grad_norm": 0.48159897327423096,
      "learning_rate": 1.5565099400040676e-05,
      "loss": 0.9015,
      "step": 7540
    },
    {
      "epoch": 0.3838722798454342,
      "grad_norm": 1.1359233856201172,
      "learning_rate": 1.5565019956274152e-05,
      "loss": 0.8826,
      "step": 7550
    },
    {
      "epoch": 0.3843807199511898,
      "grad_norm": 0.2938132882118225,
      "learning_rate": 1.5564940512507625e-05,
      "loss": 0.9099,
      "step": 7560
    },
    {
      "epoch": 0.3848891600569453,
      "grad_norm": 0.5430982708930969,
      "learning_rate": 1.5564861068741102e-05,
      "loss": 0.8808,
      "step": 7570
    },
    {
      "epoch": 0.38539760016270086,
      "grad_norm": 0.3786256015300751,
      "learning_rate": 1.5564781624974578e-05,
      "loss": 0.905,
      "step": 7580
    },
    {
      "epoch": 0.3859060402684564,
      "grad_norm": 0.5734114646911621,
      "learning_rate": 1.5564702181208055e-05,
      "loss": 0.9164,
      "step": 7590
    },
    {
      "epoch": 0.38641448037421194,
      "grad_norm": 0.4079771637916565,
      "learning_rate": 1.556462273744153e-05,
      "loss": 0.8901,
      "step": 7600
    },
    {
      "epoch": 0.38692292047996746,
      "grad_norm": 0.3299253582954407,
      "learning_rate": 1.5564543293675004e-05,
      "loss": 0.9142,
      "step": 7610
    },
    {
      "epoch": 0.38743136058572303,
      "grad_norm": 0.2914481461048126,
      "learning_rate": 1.556446384990848e-05,
      "loss": 0.9158,
      "step": 7620
    },
    {
      "epoch": 0.38793980069147854,
      "grad_norm": 0.6485885381698608,
      "learning_rate": 1.5564384406141957e-05,
      "loss": 0.9008,
      "step": 7630
    },
    {
      "epoch": 0.3884482407972341,
      "grad_norm": 0.30291375517845154,
      "learning_rate": 1.5564304962375433e-05,
      "loss": 0.9287,
      "step": 7640
    },
    {
      "epoch": 0.3889566809029896,
      "grad_norm": 0.48405998945236206,
      "learning_rate": 1.556422551860891e-05,
      "loss": 0.9154,
      "step": 7650
    },
    {
      "epoch": 0.3894651210087452,
      "grad_norm": 1.1105753183364868,
      "learning_rate": 1.5564146074842383e-05,
      "loss": 0.8936,
      "step": 7660
    },
    {
      "epoch": 0.3899735611145007,
      "grad_norm": 0.2713133990764618,
      "learning_rate": 1.556406663107586e-05,
      "loss": 0.9045,
      "step": 7670
    },
    {
      "epoch": 0.3904820012202563,
      "grad_norm": 0.6291927695274353,
      "learning_rate": 1.5563987187309335e-05,
      "loss": 0.8881,
      "step": 7680
    },
    {
      "epoch": 0.3909904413260118,
      "grad_norm": 0.451884001493454,
      "learning_rate": 1.5563907743542812e-05,
      "loss": 0.8913,
      "step": 7690
    },
    {
      "epoch": 0.39149888143176736,
      "grad_norm": 0.45651108026504517,
      "learning_rate": 1.5563828299776288e-05,
      "loss": 0.9098,
      "step": 7700
    },
    {
      "epoch": 0.3920073215375229,
      "grad_norm": 0.41946959495544434,
      "learning_rate": 1.556374885600976e-05,
      "loss": 0.9224,
      "step": 7710
    },
    {
      "epoch": 0.39251576164327845,
      "grad_norm": 0.391487181186676,
      "learning_rate": 1.5563669412243238e-05,
      "loss": 0.8889,
      "step": 7720
    },
    {
      "epoch": 0.39302420174903396,
      "grad_norm": 0.717958927154541,
      "learning_rate": 1.5563589968476714e-05,
      "loss": 0.8717,
      "step": 7730
    },
    {
      "epoch": 0.39353264185478953,
      "grad_norm": 0.49640271067619324,
      "learning_rate": 1.556351052471019e-05,
      "loss": 0.9059,
      "step": 7740
    },
    {
      "epoch": 0.39404108196054505,
      "grad_norm": 0.4755710959434509,
      "learning_rate": 1.5563431080943663e-05,
      "loss": 0.8817,
      "step": 7750
    },
    {
      "epoch": 0.3945495220663006,
      "grad_norm": 0.4278283417224884,
      "learning_rate": 1.556335163717714e-05,
      "loss": 0.9113,
      "step": 7760
    },
    {
      "epoch": 0.39505796217205613,
      "grad_norm": 0.5238533020019531,
      "learning_rate": 1.5563272193410616e-05,
      "loss": 0.8879,
      "step": 7770
    },
    {
      "epoch": 0.39556640227781165,
      "grad_norm": 0.6800866723060608,
      "learning_rate": 1.5563192749644093e-05,
      "loss": 0.8945,
      "step": 7780
    },
    {
      "epoch": 0.3960748423835672,
      "grad_norm": 0.30043232440948486,
      "learning_rate": 1.556311330587757e-05,
      "loss": 0.8787,
      "step": 7790
    },
    {
      "epoch": 0.39658328248932273,
      "grad_norm": 0.6662976145744324,
      "learning_rate": 1.5563033862111042e-05,
      "loss": 0.8983,
      "step": 7800
    },
    {
      "epoch": 0.3970917225950783,
      "grad_norm": 0.42238226532936096,
      "learning_rate": 1.556295441834452e-05,
      "loss": 0.913,
      "step": 7810
    },
    {
      "epoch": 0.3976001627008338,
      "grad_norm": 0.5815345048904419,
      "learning_rate": 1.5562874974577995e-05,
      "loss": 0.888,
      "step": 7820
    },
    {
      "epoch": 0.3981086028065894,
      "grad_norm": 0.5866250395774841,
      "learning_rate": 1.556279553081147e-05,
      "loss": 0.8876,
      "step": 7830
    },
    {
      "epoch": 0.3986170429123449,
      "grad_norm": 0.3366759717464447,
      "learning_rate": 1.5562716087044947e-05,
      "loss": 0.9012,
      "step": 7840
    },
    {
      "epoch": 0.39912548301810047,
      "grad_norm": 0.5838202834129333,
      "learning_rate": 1.556263664327842e-05,
      "loss": 0.906,
      "step": 7850
    },
    {
      "epoch": 0.399633923123856,
      "grad_norm": 0.7386900782585144,
      "learning_rate": 1.5562557199511897e-05,
      "loss": 0.898,
      "step": 7860
    },
    {
      "epoch": 0.40014236322961155,
      "grad_norm": 0.6385647058486938,
      "learning_rate": 1.5562477755745373e-05,
      "loss": 0.8921,
      "step": 7870
    },
    {
      "epoch": 0.40065080333536707,
      "grad_norm": 0.4865420162677765,
      "learning_rate": 1.556239831197885e-05,
      "loss": 0.8921,
      "step": 7880
    },
    {
      "epoch": 0.40115924344112264,
      "grad_norm": 0.788725733757019,
      "learning_rate": 1.5562318868212326e-05,
      "loss": 0.8904,
      "step": 7890
    },
    {
      "epoch": 0.40166768354687815,
      "grad_norm": 0.5454933643341064,
      "learning_rate": 1.55622394244458e-05,
      "loss": 0.9063,
      "step": 7900
    },
    {
      "epoch": 0.4021761236526337,
      "grad_norm": 0.320424884557724,
      "learning_rate": 1.5562159980679275e-05,
      "loss": 0.9085,
      "step": 7910
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 0.4746381938457489,
      "learning_rate": 1.5562080536912752e-05,
      "loss": 0.9019,
      "step": 7920
    },
    {
      "epoch": 0.4031930038641448,
      "grad_norm": 0.3476712703704834,
      "learning_rate": 1.5562001093146228e-05,
      "loss": 0.9165,
      "step": 7930
    },
    {
      "epoch": 0.4037014439699003,
      "grad_norm": 0.6250954866409302,
      "learning_rate": 1.5561921649379705e-05,
      "loss": 0.9025,
      "step": 7940
    },
    {
      "epoch": 0.4042098840756559,
      "grad_norm": 0.5620383024215698,
      "learning_rate": 1.5561842205613178e-05,
      "loss": 0.8924,
      "step": 7950
    },
    {
      "epoch": 0.4047183241814114,
      "grad_norm": 0.49624842405319214,
      "learning_rate": 1.5561762761846654e-05,
      "loss": 0.892,
      "step": 7960
    },
    {
      "epoch": 0.405226764287167,
      "grad_norm": 0.3326711058616638,
      "learning_rate": 1.556168331808013e-05,
      "loss": 0.901,
      "step": 7970
    },
    {
      "epoch": 0.4057352043929225,
      "grad_norm": 0.4461955428123474,
      "learning_rate": 1.5561603874313607e-05,
      "loss": 0.9014,
      "step": 7980
    },
    {
      "epoch": 0.40624364449867806,
      "grad_norm": 0.4989566206932068,
      "learning_rate": 1.556152443054708e-05,
      "loss": 0.8843,
      "step": 7990
    },
    {
      "epoch": 0.4067520846044336,
      "grad_norm": 0.5399632453918457,
      "learning_rate": 1.5561444986780556e-05,
      "loss": 0.8855,
      "step": 8000
    },
    {
      "epoch": 0.40726052471018914,
      "grad_norm": 0.4727599322795868,
      "learning_rate": 1.5561365543014033e-05,
      "loss": 0.8918,
      "step": 8010
    },
    {
      "epoch": 0.40776896481594466,
      "grad_norm": 0.3903658390045166,
      "learning_rate": 1.556128609924751e-05,
      "loss": 0.8787,
      "step": 8020
    },
    {
      "epoch": 0.4082774049217002,
      "grad_norm": 0.3906916677951813,
      "learning_rate": 1.5561206655480985e-05,
      "loss": 0.9026,
      "step": 8030
    },
    {
      "epoch": 0.40878584502745574,
      "grad_norm": 0.5030580163002014,
      "learning_rate": 1.556112721171446e-05,
      "loss": 0.8739,
      "step": 8040
    },
    {
      "epoch": 0.4092942851332113,
      "grad_norm": 0.569125771522522,
      "learning_rate": 1.5561047767947935e-05,
      "loss": 0.8852,
      "step": 8050
    },
    {
      "epoch": 0.4098027252389668,
      "grad_norm": 0.6003820300102234,
      "learning_rate": 1.556096832418141e-05,
      "loss": 0.89,
      "step": 8060
    },
    {
      "epoch": 0.4103111653447224,
      "grad_norm": 0.4827611744403839,
      "learning_rate": 1.5560888880414888e-05,
      "loss": 0.8981,
      "step": 8070
    },
    {
      "epoch": 0.4108196054504779,
      "grad_norm": 0.4371829330921173,
      "learning_rate": 1.5560809436648364e-05,
      "loss": 0.8929,
      "step": 8080
    },
    {
      "epoch": 0.4113280455562335,
      "grad_norm": 0.5047562122344971,
      "learning_rate": 1.5560729992881837e-05,
      "loss": 0.8815,
      "step": 8090
    },
    {
      "epoch": 0.411836485661989,
      "grad_norm": 0.8523045778274536,
      "learning_rate": 1.5560650549115313e-05,
      "loss": 0.878,
      "step": 8100
    },
    {
      "epoch": 0.41234492576774456,
      "grad_norm": 0.5840510129928589,
      "learning_rate": 1.556057110534879e-05,
      "loss": 0.8888,
      "step": 8110
    },
    {
      "epoch": 0.4128533658735001,
      "grad_norm": 1.14028799533844,
      "learning_rate": 1.5560491661582266e-05,
      "loss": 0.8798,
      "step": 8120
    },
    {
      "epoch": 0.41336180597925565,
      "grad_norm": 0.6860716938972473,
      "learning_rate": 1.5560412217815743e-05,
      "loss": 0.8984,
      "step": 8130
    },
    {
      "epoch": 0.41387024608501116,
      "grad_norm": 0.24667152762413025,
      "learning_rate": 1.5560332774049216e-05,
      "loss": 0.8974,
      "step": 8140
    },
    {
      "epoch": 0.41437868619076673,
      "grad_norm": 0.30777472257614136,
      "learning_rate": 1.5560253330282692e-05,
      "loss": 0.9,
      "step": 8150
    },
    {
      "epoch": 0.41488712629652225,
      "grad_norm": 0.7400256991386414,
      "learning_rate": 1.556017388651617e-05,
      "loss": 0.8585,
      "step": 8160
    },
    {
      "epoch": 0.4153955664022778,
      "grad_norm": 0.5865442752838135,
      "learning_rate": 1.5560094442749645e-05,
      "loss": 0.8822,
      "step": 8170
    },
    {
      "epoch": 0.41590400650803333,
      "grad_norm": 0.557927668094635,
      "learning_rate": 1.5560014998983118e-05,
      "loss": 0.8902,
      "step": 8180
    },
    {
      "epoch": 0.4164124466137889,
      "grad_norm": 0.46025365591049194,
      "learning_rate": 1.5559935555216594e-05,
      "loss": 0.9122,
      "step": 8190
    },
    {
      "epoch": 0.4169208867195444,
      "grad_norm": 0.3575854003429413,
      "learning_rate": 1.555985611145007e-05,
      "loss": 0.8884,
      "step": 8200
    },
    {
      "epoch": 0.4174293268253,
      "grad_norm": 0.4380410313606262,
      "learning_rate": 1.5559776667683547e-05,
      "loss": 0.8932,
      "step": 8210
    },
    {
      "epoch": 0.4179377669310555,
      "grad_norm": 0.3823632597923279,
      "learning_rate": 1.5559697223917023e-05,
      "loss": 0.9125,
      "step": 8220
    },
    {
      "epoch": 0.41844620703681107,
      "grad_norm": 0.41570308804512024,
      "learning_rate": 1.5559617780150496e-05,
      "loss": 0.8623,
      "step": 8230
    },
    {
      "epoch": 0.4189546471425666,
      "grad_norm": 0.4822801947593689,
      "learning_rate": 1.5559538336383973e-05,
      "loss": 0.906,
      "step": 8240
    },
    {
      "epoch": 0.41946308724832215,
      "grad_norm": 0.62651526927948,
      "learning_rate": 1.555945889261745e-05,
      "loss": 0.8846,
      "step": 8250
    },
    {
      "epoch": 0.41997152735407767,
      "grad_norm": 0.3125244379043579,
      "learning_rate": 1.5559379448850926e-05,
      "loss": 0.869,
      "step": 8260
    },
    {
      "epoch": 0.42047996745983324,
      "grad_norm": 0.4249716103076935,
      "learning_rate": 1.5559300005084402e-05,
      "loss": 0.8989,
      "step": 8270
    },
    {
      "epoch": 0.42098840756558875,
      "grad_norm": 0.8310549259185791,
      "learning_rate": 1.5559220561317875e-05,
      "loss": 0.9003,
      "step": 8280
    },
    {
      "epoch": 0.4214968476713443,
      "grad_norm": 0.3800405263900757,
      "learning_rate": 1.555914111755135e-05,
      "loss": 0.8744,
      "step": 8290
    },
    {
      "epoch": 0.42200528777709984,
      "grad_norm": 0.5635166764259338,
      "learning_rate": 1.5559061673784828e-05,
      "loss": 0.8946,
      "step": 8300
    },
    {
      "epoch": 0.4225137278828554,
      "grad_norm": 0.6522371768951416,
      "learning_rate": 1.5558982230018304e-05,
      "loss": 0.9004,
      "step": 8310
    },
    {
      "epoch": 0.4230221679886109,
      "grad_norm": 0.5548301339149475,
      "learning_rate": 1.555890278625178e-05,
      "loss": 0.9021,
      "step": 8320
    },
    {
      "epoch": 0.4235306080943665,
      "grad_norm": 0.46062925457954407,
      "learning_rate": 1.5558823342485254e-05,
      "loss": 0.8708,
      "step": 8330
    },
    {
      "epoch": 0.424039048200122,
      "grad_norm": 0.6385177969932556,
      "learning_rate": 1.555874389871873e-05,
      "loss": 0.8724,
      "step": 8340
    },
    {
      "epoch": 0.4245474883058776,
      "grad_norm": 0.4915219247341156,
      "learning_rate": 1.5558664454952206e-05,
      "loss": 0.8831,
      "step": 8350
    },
    {
      "epoch": 0.4250559284116331,
      "grad_norm": 0.515029788017273,
      "learning_rate": 1.5558585011185683e-05,
      "loss": 0.9003,
      "step": 8360
    },
    {
      "epoch": 0.42556436851738866,
      "grad_norm": 0.49185407161712646,
      "learning_rate": 1.555850556741916e-05,
      "loss": 0.9134,
      "step": 8370
    },
    {
      "epoch": 0.4260728086231442,
      "grad_norm": 0.456432044506073,
      "learning_rate": 1.5558426123652635e-05,
      "loss": 0.8867,
      "step": 8380
    },
    {
      "epoch": 0.42658124872889974,
      "grad_norm": 1.152931809425354,
      "learning_rate": 1.5558346679886112e-05,
      "loss": 0.9005,
      "step": 8390
    },
    {
      "epoch": 0.42708968883465526,
      "grad_norm": 0.4528123736381531,
      "learning_rate": 1.5558267236119585e-05,
      "loss": 0.9001,
      "step": 8400
    },
    {
      "epoch": 0.4275981289404108,
      "grad_norm": 0.36936187744140625,
      "learning_rate": 1.555818779235306e-05,
      "loss": 0.8673,
      "step": 8410
    },
    {
      "epoch": 0.42810656904616634,
      "grad_norm": 0.4163762629032135,
      "learning_rate": 1.5558108348586538e-05,
      "loss": 0.869,
      "step": 8420
    },
    {
      "epoch": 0.4286150091519219,
      "grad_norm": 0.3676823377609253,
      "learning_rate": 1.5558028904820014e-05,
      "loss": 0.8749,
      "step": 8430
    },
    {
      "epoch": 0.4291234492576774,
      "grad_norm": 0.24949762225151062,
      "learning_rate": 1.555794946105349e-05,
      "loss": 0.9011,
      "step": 8440
    },
    {
      "epoch": 0.429631889363433,
      "grad_norm": 1.008177399635315,
      "learning_rate": 1.5557870017286963e-05,
      "loss": 0.8976,
      "step": 8450
    },
    {
      "epoch": 0.4301403294691885,
      "grad_norm": 0.4893701672554016,
      "learning_rate": 1.555779057352044e-05,
      "loss": 0.9132,
      "step": 8460
    },
    {
      "epoch": 0.4306487695749441,
      "grad_norm": 0.6461503505706787,
      "learning_rate": 1.5557711129753916e-05,
      "loss": 0.9014,
      "step": 8470
    },
    {
      "epoch": 0.4311572096806996,
      "grad_norm": 0.8135714530944824,
      "learning_rate": 1.5557631685987393e-05,
      "loss": 0.8833,
      "step": 8480
    },
    {
      "epoch": 0.43166564978645516,
      "grad_norm": 0.5714644193649292,
      "learning_rate": 1.555755224222087e-05,
      "loss": 0.8953,
      "step": 8490
    },
    {
      "epoch": 0.4321740898922107,
      "grad_norm": 0.5984715819358826,
      "learning_rate": 1.5557472798454342e-05,
      "loss": 0.8745,
      "step": 8500
    },
    {
      "epoch": 0.43268252999796625,
      "grad_norm": 0.799326479434967,
      "learning_rate": 1.555739335468782e-05,
      "loss": 0.8961,
      "step": 8510
    },
    {
      "epoch": 0.43319097010372176,
      "grad_norm": 0.6842963099479675,
      "learning_rate": 1.5557313910921295e-05,
      "loss": 0.9021,
      "step": 8520
    },
    {
      "epoch": 0.43369941020947733,
      "grad_norm": 0.4354289770126343,
      "learning_rate": 1.555723446715477e-05,
      "loss": 0.8818,
      "step": 8530
    },
    {
      "epoch": 0.43420785031523285,
      "grad_norm": 0.5064815282821655,
      "learning_rate": 1.5557155023388244e-05,
      "loss": 0.8674,
      "step": 8540
    },
    {
      "epoch": 0.4347162904209884,
      "grad_norm": 0.4605139493942261,
      "learning_rate": 1.555707557962172e-05,
      "loss": 0.8996,
      "step": 8550
    },
    {
      "epoch": 0.43522473052674393,
      "grad_norm": 0.456632137298584,
      "learning_rate": 1.5556996135855197e-05,
      "loss": 0.8703,
      "step": 8560
    },
    {
      "epoch": 0.4357331706324995,
      "grad_norm": 0.7084226012229919,
      "learning_rate": 1.5556916692088673e-05,
      "loss": 0.8889,
      "step": 8570
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 0.45915499329566956,
      "learning_rate": 1.555683724832215e-05,
      "loss": 0.8953,
      "step": 8580
    },
    {
      "epoch": 0.4367500508440106,
      "grad_norm": 0.5568912029266357,
      "learning_rate": 1.5556757804555623e-05,
      "loss": 0.893,
      "step": 8590
    },
    {
      "epoch": 0.4372584909497661,
      "grad_norm": 0.8610081076622009,
      "learning_rate": 1.55566783607891e-05,
      "loss": 0.897,
      "step": 8600
    },
    {
      "epoch": 0.43776693105552167,
      "grad_norm": 0.334501713514328,
      "learning_rate": 1.5556598917022576e-05,
      "loss": 0.891,
      "step": 8610
    },
    {
      "epoch": 0.4382753711612772,
      "grad_norm": 0.5255313515663147,
      "learning_rate": 1.5556519473256052e-05,
      "loss": 0.8731,
      "step": 8620
    },
    {
      "epoch": 0.43878381126703275,
      "grad_norm": 0.45021066069602966,
      "learning_rate": 1.555644002948953e-05,
      "loss": 0.889,
      "step": 8630
    },
    {
      "epoch": 0.43929225137278827,
      "grad_norm": 0.7233249545097351,
      "learning_rate": 1.5556360585723e-05,
      "loss": 0.8796,
      "step": 8640
    },
    {
      "epoch": 0.43980069147854384,
      "grad_norm": 0.2799410820007324,
      "learning_rate": 1.5556281141956478e-05,
      "loss": 0.8951,
      "step": 8650
    },
    {
      "epoch": 0.44030913158429935,
      "grad_norm": 0.5714049339294434,
      "learning_rate": 1.5556201698189954e-05,
      "loss": 0.8864,
      "step": 8660
    },
    {
      "epoch": 0.4408175716900549,
      "grad_norm": 0.7580434083938599,
      "learning_rate": 1.555612225442343e-05,
      "loss": 0.8745,
      "step": 8670
    },
    {
      "epoch": 0.44132601179581044,
      "grad_norm": 0.5783735513687134,
      "learning_rate": 1.5556042810656907e-05,
      "loss": 0.8905,
      "step": 8680
    },
    {
      "epoch": 0.441834451901566,
      "grad_norm": 0.555739164352417,
      "learning_rate": 1.555596336689038e-05,
      "loss": 0.8633,
      "step": 8690
    },
    {
      "epoch": 0.4423428920073215,
      "grad_norm": 0.8125318288803101,
      "learning_rate": 1.5555883923123856e-05,
      "loss": 0.8966,
      "step": 8700
    },
    {
      "epoch": 0.4428513321130771,
      "grad_norm": 0.34482893347740173,
      "learning_rate": 1.5555804479357333e-05,
      "loss": 0.8973,
      "step": 8710
    },
    {
      "epoch": 0.4433597722188326,
      "grad_norm": 0.5124861598014832,
      "learning_rate": 1.555572503559081e-05,
      "loss": 0.8903,
      "step": 8720
    },
    {
      "epoch": 0.4438682123245882,
      "grad_norm": 0.33596524596214294,
      "learning_rate": 1.5555645591824286e-05,
      "loss": 0.8998,
      "step": 8730
    },
    {
      "epoch": 0.4443766524303437,
      "grad_norm": 0.6743530631065369,
      "learning_rate": 1.555556614805776e-05,
      "loss": 0.8993,
      "step": 8740
    },
    {
      "epoch": 0.44488509253609926,
      "grad_norm": 0.7857143878936768,
      "learning_rate": 1.5555486704291235e-05,
      "loss": 0.8737,
      "step": 8750
    },
    {
      "epoch": 0.4453935326418548,
      "grad_norm": 0.5717474818229675,
      "learning_rate": 1.555540726052471e-05,
      "loss": 0.8814,
      "step": 8760
    },
    {
      "epoch": 0.44590197274761034,
      "grad_norm": 0.5912643671035767,
      "learning_rate": 1.5555327816758188e-05,
      "loss": 0.8906,
      "step": 8770
    },
    {
      "epoch": 0.44641041285336586,
      "grad_norm": 0.4306499660015106,
      "learning_rate": 1.555524837299166e-05,
      "loss": 0.91,
      "step": 8780
    },
    {
      "epoch": 0.4469188529591214,
      "grad_norm": 0.5680279731750488,
      "learning_rate": 1.5555168929225137e-05,
      "loss": 0.8715,
      "step": 8790
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 0.5811041593551636,
      "learning_rate": 1.5555089485458614e-05,
      "loss": 0.8814,
      "step": 8800
    },
    {
      "epoch": 0.4479357331706325,
      "grad_norm": 0.4641321301460266,
      "learning_rate": 1.555501004169209e-05,
      "loss": 0.8539,
      "step": 8810
    },
    {
      "epoch": 0.448444173276388,
      "grad_norm": 0.5977451801300049,
      "learning_rate": 1.5554930597925566e-05,
      "loss": 0.8498,
      "step": 8820
    },
    {
      "epoch": 0.4489526133821436,
      "grad_norm": 0.4915403127670288,
      "learning_rate": 1.555485115415904e-05,
      "loss": 0.8622,
      "step": 8830
    },
    {
      "epoch": 0.4494610534878991,
      "grad_norm": 0.3626928925514221,
      "learning_rate": 1.5554771710392516e-05,
      "loss": 0.889,
      "step": 8840
    },
    {
      "epoch": 0.4499694935936547,
      "grad_norm": 0.7393519282341003,
      "learning_rate": 1.5554692266625992e-05,
      "loss": 0.8589,
      "step": 8850
    },
    {
      "epoch": 0.4504779336994102,
      "grad_norm": 0.4507325291633606,
      "learning_rate": 1.555461282285947e-05,
      "loss": 0.8918,
      "step": 8860
    },
    {
      "epoch": 0.45098637380516576,
      "grad_norm": 0.9944936037063599,
      "learning_rate": 1.5554533379092945e-05,
      "loss": 0.8739,
      "step": 8870
    },
    {
      "epoch": 0.4514948139109213,
      "grad_norm": 0.7376550436019897,
      "learning_rate": 1.5554453935326418e-05,
      "loss": 0.8855,
      "step": 8880
    },
    {
      "epoch": 0.45200325401667685,
      "grad_norm": 0.49634405970573425,
      "learning_rate": 1.5554374491559894e-05,
      "loss": 0.9011,
      "step": 8890
    },
    {
      "epoch": 0.45251169412243236,
      "grad_norm": 0.5104061961174011,
      "learning_rate": 1.555429504779337e-05,
      "loss": 0.905,
      "step": 8900
    },
    {
      "epoch": 0.45302013422818793,
      "grad_norm": 0.7389527559280396,
      "learning_rate": 1.5554215604026847e-05,
      "loss": 0.8659,
      "step": 8910
    },
    {
      "epoch": 0.45352857433394345,
      "grad_norm": 0.6580770015716553,
      "learning_rate": 1.5554136160260323e-05,
      "loss": 0.8917,
      "step": 8920
    },
    {
      "epoch": 0.454037014439699,
      "grad_norm": 0.5304046869277954,
      "learning_rate": 1.5554056716493796e-05,
      "loss": 0.876,
      "step": 8930
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.7102521061897278,
      "learning_rate": 1.5553977272727273e-05,
      "loss": 0.8826,
      "step": 8940
    },
    {
      "epoch": 0.4550538946512101,
      "grad_norm": 0.40919890999794006,
      "learning_rate": 1.555389782896075e-05,
      "loss": 0.8687,
      "step": 8950
    },
    {
      "epoch": 0.4555623347569656,
      "grad_norm": 0.450982928276062,
      "learning_rate": 1.5553818385194226e-05,
      "loss": 0.905,
      "step": 8960
    },
    {
      "epoch": 0.4560707748627212,
      "grad_norm": 0.45639845728874207,
      "learning_rate": 1.5553738941427702e-05,
      "loss": 0.8855,
      "step": 8970
    },
    {
      "epoch": 0.4565792149684767,
      "grad_norm": 0.6213773488998413,
      "learning_rate": 1.5553659497661175e-05,
      "loss": 0.8761,
      "step": 8980
    },
    {
      "epoch": 0.45708765507423227,
      "grad_norm": 0.5974772572517395,
      "learning_rate": 1.555358005389465e-05,
      "loss": 0.8974,
      "step": 8990
    },
    {
      "epoch": 0.4575960951799878,
      "grad_norm": 0.7155593633651733,
      "learning_rate": 1.5553500610128128e-05,
      "loss": 0.8928,
      "step": 9000
    },
    {
      "epoch": 0.45810453528574335,
      "grad_norm": 0.6176056265830994,
      "learning_rate": 1.5553421166361604e-05,
      "loss": 0.8895,
      "step": 9010
    },
    {
      "epoch": 0.45861297539149887,
      "grad_norm": 0.5158056020736694,
      "learning_rate": 1.5553341722595077e-05,
      "loss": 0.8875,
      "step": 9020
    },
    {
      "epoch": 0.45912141549725444,
      "grad_norm": 0.3735297620296478,
      "learning_rate": 1.5553262278828554e-05,
      "loss": 0.884,
      "step": 9030
    },
    {
      "epoch": 0.45962985560300995,
      "grad_norm": 0.5542728900909424,
      "learning_rate": 1.555318283506203e-05,
      "loss": 0.869,
      "step": 9040
    },
    {
      "epoch": 0.4601382957087655,
      "grad_norm": 0.3441436290740967,
      "learning_rate": 1.5553103391295506e-05,
      "loss": 0.8921,
      "step": 9050
    },
    {
      "epoch": 0.46064673581452104,
      "grad_norm": 0.26027724146842957,
      "learning_rate": 1.5553023947528983e-05,
      "loss": 0.8934,
      "step": 9060
    },
    {
      "epoch": 0.4611551759202766,
      "grad_norm": 0.6698914170265198,
      "learning_rate": 1.5552944503762456e-05,
      "loss": 0.88,
      "step": 9070
    },
    {
      "epoch": 0.4616636160260321,
      "grad_norm": 0.6491151452064514,
      "learning_rate": 1.5552865059995932e-05,
      "loss": 0.8732,
      "step": 9080
    },
    {
      "epoch": 0.4621720561317877,
      "grad_norm": 0.5174745321273804,
      "learning_rate": 1.555278561622941e-05,
      "loss": 0.8657,
      "step": 9090
    },
    {
      "epoch": 0.4626804962375432,
      "grad_norm": 0.8288640975952148,
      "learning_rate": 1.5552706172462885e-05,
      "loss": 0.877,
      "step": 9100
    },
    {
      "epoch": 0.4631889363432988,
      "grad_norm": 0.5523834824562073,
      "learning_rate": 1.555262672869636e-05,
      "loss": 0.8804,
      "step": 9110
    },
    {
      "epoch": 0.4636973764490543,
      "grad_norm": 1.0838675498962402,
      "learning_rate": 1.5552547284929834e-05,
      "loss": 0.8799,
      "step": 9120
    },
    {
      "epoch": 0.46420581655480986,
      "grad_norm": 0.3921869099140167,
      "learning_rate": 1.555246784116331e-05,
      "loss": 0.8836,
      "step": 9130
    },
    {
      "epoch": 0.4647142566605654,
      "grad_norm": 0.4558073878288269,
      "learning_rate": 1.5552388397396787e-05,
      "loss": 0.8881,
      "step": 9140
    },
    {
      "epoch": 0.46522269676632094,
      "grad_norm": 0.4745161533355713,
      "learning_rate": 1.5552308953630264e-05,
      "loss": 0.8731,
      "step": 9150
    },
    {
      "epoch": 0.46573113687207646,
      "grad_norm": 0.5989388823509216,
      "learning_rate": 1.555222950986374e-05,
      "loss": 0.9015,
      "step": 9160
    },
    {
      "epoch": 0.466239576977832,
      "grad_norm": 0.6687324643135071,
      "learning_rate": 1.5552150066097213e-05,
      "loss": 0.914,
      "step": 9170
    },
    {
      "epoch": 0.46674801708358754,
      "grad_norm": 0.649952232837677,
      "learning_rate": 1.555207062233069e-05,
      "loss": 0.9052,
      "step": 9180
    },
    {
      "epoch": 0.4672564571893431,
      "grad_norm": 0.42170727252960205,
      "learning_rate": 1.5551991178564166e-05,
      "loss": 0.8956,
      "step": 9190
    },
    {
      "epoch": 0.4677648972950986,
      "grad_norm": 0.4618629813194275,
      "learning_rate": 1.5551911734797642e-05,
      "loss": 0.876,
      "step": 9200
    },
    {
      "epoch": 0.4682733374008542,
      "grad_norm": 0.8035610914230347,
      "learning_rate": 1.5551832291031115e-05,
      "loss": 0.8546,
      "step": 9210
    },
    {
      "epoch": 0.4687817775066097,
      "grad_norm": 0.5847440958023071,
      "learning_rate": 1.555175284726459e-05,
      "loss": 0.8613,
      "step": 9220
    },
    {
      "epoch": 0.4692902176123653,
      "grad_norm": 0.3884533941745758,
      "learning_rate": 1.5551673403498068e-05,
      "loss": 0.8742,
      "step": 9230
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 0.30028489232063293,
      "learning_rate": 1.5551593959731544e-05,
      "loss": 0.8871,
      "step": 9240
    },
    {
      "epoch": 0.47030709782387636,
      "grad_norm": 0.5156345367431641,
      "learning_rate": 1.555151451596502e-05,
      "loss": 0.8675,
      "step": 9250
    },
    {
      "epoch": 0.4708155379296319,
      "grad_norm": 0.37967878580093384,
      "learning_rate": 1.5551435072198494e-05,
      "loss": 0.8882,
      "step": 9260
    },
    {
      "epoch": 0.47132397803538745,
      "grad_norm": 0.6790665984153748,
      "learning_rate": 1.555135562843197e-05,
      "loss": 0.8979,
      "step": 9270
    },
    {
      "epoch": 0.47183241814114296,
      "grad_norm": 0.8856755495071411,
      "learning_rate": 1.5551276184665447e-05,
      "loss": 0.8761,
      "step": 9280
    },
    {
      "epoch": 0.47234085824689853,
      "grad_norm": 0.5705188512802124,
      "learning_rate": 1.5551196740898923e-05,
      "loss": 0.8917,
      "step": 9290
    },
    {
      "epoch": 0.47284929835265405,
      "grad_norm": 0.8981109857559204,
      "learning_rate": 1.55511172971324e-05,
      "loss": 0.8757,
      "step": 9300
    },
    {
      "epoch": 0.4733577384584096,
      "grad_norm": 0.45017924904823303,
      "learning_rate": 1.5551037853365872e-05,
      "loss": 0.8811,
      "step": 9310
    },
    {
      "epoch": 0.47386617856416513,
      "grad_norm": 0.3951369524002075,
      "learning_rate": 1.555095840959935e-05,
      "loss": 0.885,
      "step": 9320
    },
    {
      "epoch": 0.4743746186699207,
      "grad_norm": 0.7386234998703003,
      "learning_rate": 1.5550878965832825e-05,
      "loss": 0.8532,
      "step": 9330
    },
    {
      "epoch": 0.4748830587756762,
      "grad_norm": 0.4968377649784088,
      "learning_rate": 1.55507995220663e-05,
      "loss": 0.8777,
      "step": 9340
    },
    {
      "epoch": 0.4753914988814318,
      "grad_norm": 0.4542001485824585,
      "learning_rate": 1.5550720078299778e-05,
      "loss": 0.8757,
      "step": 9350
    },
    {
      "epoch": 0.4758999389871873,
      "grad_norm": 0.3090551793575287,
      "learning_rate": 1.555064063453325e-05,
      "loss": 0.904,
      "step": 9360
    },
    {
      "epoch": 0.47640837909294287,
      "grad_norm": 0.4496021270751953,
      "learning_rate": 1.5550561190766727e-05,
      "loss": 0.9064,
      "step": 9370
    },
    {
      "epoch": 0.4769168191986984,
      "grad_norm": 0.463047057390213,
      "learning_rate": 1.5550481747000204e-05,
      "loss": 0.9022,
      "step": 9380
    },
    {
      "epoch": 0.47742525930445395,
      "grad_norm": 0.34317776560783386,
      "learning_rate": 1.555040230323368e-05,
      "loss": 0.8667,
      "step": 9390
    },
    {
      "epoch": 0.47793369941020947,
      "grad_norm": 0.37828826904296875,
      "learning_rate": 1.5550322859467156e-05,
      "loss": 0.8593,
      "step": 9400
    },
    {
      "epoch": 0.47844213951596504,
      "grad_norm": 0.6857767105102539,
      "learning_rate": 1.555024341570063e-05,
      "loss": 0.8802,
      "step": 9410
    },
    {
      "epoch": 0.47895057962172055,
      "grad_norm": 0.41101381182670593,
      "learning_rate": 1.5550163971934106e-05,
      "loss": 0.9013,
      "step": 9420
    },
    {
      "epoch": 0.4794590197274761,
      "grad_norm": 0.3982313871383667,
      "learning_rate": 1.5550084528167582e-05,
      "loss": 0.8689,
      "step": 9430
    },
    {
      "epoch": 0.47996745983323164,
      "grad_norm": 0.5266314744949341,
      "learning_rate": 1.555000508440106e-05,
      "loss": 0.8966,
      "step": 9440
    },
    {
      "epoch": 0.4804758999389872,
      "grad_norm": 0.6585624814033508,
      "learning_rate": 1.554992564063453e-05,
      "loss": 0.8599,
      "step": 9450
    },
    {
      "epoch": 0.4809843400447427,
      "grad_norm": 0.4485238492488861,
      "learning_rate": 1.5549846196868008e-05,
      "loss": 0.8836,
      "step": 9460
    },
    {
      "epoch": 0.4814927801504983,
      "grad_norm": 0.5691471695899963,
      "learning_rate": 1.5549766753101484e-05,
      "loss": 0.8601,
      "step": 9470
    },
    {
      "epoch": 0.4820012202562538,
      "grad_norm": 0.4149019718170166,
      "learning_rate": 1.554968730933496e-05,
      "loss": 0.8772,
      "step": 9480
    },
    {
      "epoch": 0.4825096603620094,
      "grad_norm": 0.4353499412536621,
      "learning_rate": 1.5549607865568437e-05,
      "loss": 0.8431,
      "step": 9490
    },
    {
      "epoch": 0.4830181004677649,
      "grad_norm": 0.5515820384025574,
      "learning_rate": 1.554952842180191e-05,
      "loss": 0.8972,
      "step": 9500
    },
    {
      "epoch": 0.48352654057352046,
      "grad_norm": 0.3939908444881439,
      "learning_rate": 1.5549448978035387e-05,
      "loss": 0.8654,
      "step": 9510
    },
    {
      "epoch": 0.484034980679276,
      "grad_norm": 0.48983439803123474,
      "learning_rate": 1.5549369534268863e-05,
      "loss": 0.8762,
      "step": 9520
    },
    {
      "epoch": 0.48454342078503154,
      "grad_norm": 0.43963080644607544,
      "learning_rate": 1.554929009050234e-05,
      "loss": 0.8689,
      "step": 9530
    },
    {
      "epoch": 0.48505186089078706,
      "grad_norm": 1.128641128540039,
      "learning_rate": 1.5549210646735816e-05,
      "loss": 0.8922,
      "step": 9540
    },
    {
      "epoch": 0.4855603009965426,
      "grad_norm": 0.5563018321990967,
      "learning_rate": 1.554913120296929e-05,
      "loss": 0.8968,
      "step": 9550
    },
    {
      "epoch": 0.48606874110229814,
      "grad_norm": 0.8193150758743286,
      "learning_rate": 1.5549051759202765e-05,
      "loss": 0.8886,
      "step": 9560
    },
    {
      "epoch": 0.4865771812080537,
      "grad_norm": 0.6540278792381287,
      "learning_rate": 1.554897231543624e-05,
      "loss": 0.8721,
      "step": 9570
    },
    {
      "epoch": 0.4870856213138092,
      "grad_norm": 0.31329959630966187,
      "learning_rate": 1.5548892871669718e-05,
      "loss": 0.8563,
      "step": 9580
    },
    {
      "epoch": 0.4875940614195648,
      "grad_norm": 0.29318907856941223,
      "learning_rate": 1.5548813427903194e-05,
      "loss": 0.9141,
      "step": 9590
    },
    {
      "epoch": 0.4881025015253203,
      "grad_norm": 0.9410451650619507,
      "learning_rate": 1.5548733984136667e-05,
      "loss": 0.848,
      "step": 9600
    },
    {
      "epoch": 0.4886109416310759,
      "grad_norm": 0.5613561868667603,
      "learning_rate": 1.5548654540370144e-05,
      "loss": 0.8834,
      "step": 9610
    },
    {
      "epoch": 0.4891193817368314,
      "grad_norm": 1.0199247598648071,
      "learning_rate": 1.554857509660362e-05,
      "loss": 0.8662,
      "step": 9620
    },
    {
      "epoch": 0.48962782184258696,
      "grad_norm": 0.4791473150253296,
      "learning_rate": 1.5548495652837097e-05,
      "loss": 0.9024,
      "step": 9630
    },
    {
      "epoch": 0.4901362619483425,
      "grad_norm": 0.832241952419281,
      "learning_rate": 1.5548416209070573e-05,
      "loss": 0.8502,
      "step": 9640
    },
    {
      "epoch": 0.49064470205409805,
      "grad_norm": 0.49094855785369873,
      "learning_rate": 1.5548336765304046e-05,
      "loss": 0.8654,
      "step": 9650
    },
    {
      "epoch": 0.49115314215985356,
      "grad_norm": 0.4229884743690491,
      "learning_rate": 1.5548257321537522e-05,
      "loss": 0.881,
      "step": 9660
    },
    {
      "epoch": 0.49166158226560913,
      "grad_norm": 0.5795981884002686,
      "learning_rate": 1.5548177877771e-05,
      "loss": 0.8884,
      "step": 9670
    },
    {
      "epoch": 0.49217002237136465,
      "grad_norm": 0.5230070948600769,
      "learning_rate": 1.5548098434004475e-05,
      "loss": 0.8735,
      "step": 9680
    },
    {
      "epoch": 0.4926784624771202,
      "grad_norm": 0.5329338908195496,
      "learning_rate": 1.5548018990237948e-05,
      "loss": 0.8767,
      "step": 9690
    },
    {
      "epoch": 0.49318690258287573,
      "grad_norm": 0.5934232473373413,
      "learning_rate": 1.5547939546471425e-05,
      "loss": 0.894,
      "step": 9700
    },
    {
      "epoch": 0.4936953426886313,
      "grad_norm": 0.6028817892074585,
      "learning_rate": 1.55478601027049e-05,
      "loss": 0.8657,
      "step": 9710
    },
    {
      "epoch": 0.4942037827943868,
      "grad_norm": 0.36456939578056335,
      "learning_rate": 1.5547780658938377e-05,
      "loss": 0.8743,
      "step": 9720
    },
    {
      "epoch": 0.4947122229001424,
      "grad_norm": 0.736472487449646,
      "learning_rate": 1.5547701215171854e-05,
      "loss": 0.871,
      "step": 9730
    },
    {
      "epoch": 0.4952206630058979,
      "grad_norm": 0.3093014657497406,
      "learning_rate": 1.5547621771405327e-05,
      "loss": 0.8951,
      "step": 9740
    },
    {
      "epoch": 0.49572910311165347,
      "grad_norm": 0.37080085277557373,
      "learning_rate": 1.5547542327638803e-05,
      "loss": 0.8754,
      "step": 9750
    },
    {
      "epoch": 0.496237543217409,
      "grad_norm": 0.4524206817150116,
      "learning_rate": 1.554746288387228e-05,
      "loss": 0.8748,
      "step": 9760
    },
    {
      "epoch": 0.49674598332316455,
      "grad_norm": 0.47312456369400024,
      "learning_rate": 1.5547383440105756e-05,
      "loss": 0.892,
      "step": 9770
    },
    {
      "epoch": 0.49725442342892007,
      "grad_norm": 0.7589121460914612,
      "learning_rate": 1.5547303996339232e-05,
      "loss": 0.8541,
      "step": 9780
    },
    {
      "epoch": 0.49776286353467564,
      "grad_norm": 0.5551259517669678,
      "learning_rate": 1.5547224552572705e-05,
      "loss": 0.871,
      "step": 9790
    },
    {
      "epoch": 0.49827130364043115,
      "grad_norm": 0.6646981835365295,
      "learning_rate": 1.5547145108806182e-05,
      "loss": 0.8916,
      "step": 9800
    },
    {
      "epoch": 0.4987797437461867,
      "grad_norm": 0.3782578408718109,
      "learning_rate": 1.5547065665039658e-05,
      "loss": 0.8971,
      "step": 9810
    },
    {
      "epoch": 0.49928818385194224,
      "grad_norm": 0.4520461857318878,
      "learning_rate": 1.5546986221273135e-05,
      "loss": 0.8934,
      "step": 9820
    },
    {
      "epoch": 0.4997966239576978,
      "grad_norm": 0.5661274194717407,
      "learning_rate": 1.554690677750661e-05,
      "loss": 0.8813,
      "step": 9830
    },
    {
      "epoch": 0.5003050640634533,
      "grad_norm": 0.5380527377128601,
      "learning_rate": 1.5546827333740084e-05,
      "loss": 0.8762,
      "step": 9840
    },
    {
      "epoch": 0.5008135041692089,
      "grad_norm": 0.6099157929420471,
      "learning_rate": 1.554674788997356e-05,
      "loss": 0.8597,
      "step": 9850
    },
    {
      "epoch": 0.5013219442749645,
      "grad_norm": 0.48886704444885254,
      "learning_rate": 1.5546668446207037e-05,
      "loss": 0.894,
      "step": 9860
    },
    {
      "epoch": 0.5018303843807199,
      "grad_norm": 0.747870147228241,
      "learning_rate": 1.5546589002440513e-05,
      "loss": 0.874,
      "step": 9870
    },
    {
      "epoch": 0.5023388244864755,
      "grad_norm": 0.5687048435211182,
      "learning_rate": 1.554650955867399e-05,
      "loss": 0.8781,
      "step": 9880
    },
    {
      "epoch": 0.5028472645922311,
      "grad_norm": 0.6925323605537415,
      "learning_rate": 1.5546430114907463e-05,
      "loss": 0.8597,
      "step": 9890
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 0.573189377784729,
      "learning_rate": 1.554635067114094e-05,
      "loss": 0.8482,
      "step": 9900
    },
    {
      "epoch": 0.5038641448037421,
      "grad_norm": 0.5136175155639648,
      "learning_rate": 1.5546271227374415e-05,
      "loss": 0.8542,
      "step": 9910
    },
    {
      "epoch": 0.5043725849094977,
      "grad_norm": 0.5575923323631287,
      "learning_rate": 1.554619178360789e-05,
      "loss": 0.8723,
      "step": 9920
    },
    {
      "epoch": 0.5048810250152532,
      "grad_norm": 0.5278564691543579,
      "learning_rate": 1.5546112339841365e-05,
      "loss": 0.871,
      "step": 9930
    },
    {
      "epoch": 0.5053894651210088,
      "grad_norm": 0.47067761421203613,
      "learning_rate": 1.554603289607484e-05,
      "loss": 0.8755,
      "step": 9940
    },
    {
      "epoch": 0.5058979052267643,
      "grad_norm": 0.4635362923145294,
      "learning_rate": 1.5545953452308317e-05,
      "loss": 0.8811,
      "step": 9950
    },
    {
      "epoch": 0.5064063453325198,
      "grad_norm": 0.422576367855072,
      "learning_rate": 1.5545874008541794e-05,
      "loss": 0.8777,
      "step": 9960
    },
    {
      "epoch": 0.5069147854382754,
      "grad_norm": 0.6234725713729858,
      "learning_rate": 1.554579456477527e-05,
      "loss": 0.8704,
      "step": 9970
    },
    {
      "epoch": 0.507423225544031,
      "grad_norm": 0.3765734136104584,
      "learning_rate": 1.5545715121008743e-05,
      "loss": 0.8862,
      "step": 9980
    },
    {
      "epoch": 0.5079316656497864,
      "grad_norm": 0.5479907989501953,
      "learning_rate": 1.554563567724222e-05,
      "loss": 0.8764,
      "step": 9990
    },
    {
      "epoch": 0.508440105755542,
      "grad_norm": 0.5836743116378784,
      "learning_rate": 1.5545556233475696e-05,
      "loss": 0.8833,
      "step": 10000
    },
    {
      "epoch": 0.508440105755542,
      "eval_runtime": 239.5178,
      "eval_samples_per_second": 36.436,
      "eval_steps_per_second": 9.11,
      "step": 10000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1966800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.8789571944448e+19,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}


Traceback (most recent call last):                                                                                                                                                                                         
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3162, in _determine_best_metric
    metric_value = metrics[metric_to_check]
                   ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'eval_accuracy'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 216, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2620, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3094, in _maybe_log_save_evaluate
    is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3164, in _determine_best_metric
    raise KeyError(
KeyError: "The `metric_for_best_model` training argument is set to 'eval_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments."




Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 197, in main
    f.write(f"{name:60s} | shape: {tuple(param.shape):20s} | requires_grad={param.requires_grad}\n")
                                  ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: unsupported format string passed to tuple.__format__


warnings.warn(
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 133, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/hf_argparser.py", line 358, in parse_args_into_dataclasses
    obj = dtype(**inputs)
          ^^^^^^^^^^^^^^^
  File "<string>", line 136, in __init__
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/training_args.py", line 1678, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.STEPS
- Save strategy: SaveStrategy.EPOCH


tensorboard --logdir ./output_dir/tb_samples


AssertionError: size of input tensor and input format are different.         tensor shape: (1, 320), input_format: CHW



0%|                                                                                                                                                                            | 499/4176700 [01:03<148:04:00,  7.83it/s](pt12) guilin@guilin-System-Product-Name:~/data_proccess$ python unzip.py ./EM_pretrain_data
Extracting FAFB_crop_hdf_4.zip to ./EM_pretrain_data/FAFB_crop_hdf_4...
Extracting Kasthuri2015_hdf_5.zip to ./EM_pretrain_data/Kasthuri2015_hdf_5...
Extracting Kasthuri2015_hdf_9.zip to ./EM_pretrain_data/Kasthuri2015_hdf_9...
Extracting Kasthuri2015_hdf_8.zip to ./EM_pretrain_data/Kasthuri2015_hdf_8...
Extracting Kasthuri2015_hdf_2.zip to ./EM_pretrain_data/Kasthuri2015_hdf_2...
Extracting FIB-25_hdf_6.zip to ./EM_pretrain_data/FIB-25_hdf_6...
Extracting Kasthuri2015_hdf_3.zip to ./EM_pretrain_data/Kasthuri2015_hdf_3...
Traceback (most recent call last):
  File "/home/guilin/data_proccess/unzip.py", line 52, in <module>
    extract_all_in_dir(args.dir)
  File "/home/guilin/data_proccess/unzip.py", line 44, in extract_all_in_dir
    extract_archive(fpath, out_dir)
  File "/home/guilin/data_proccess/unzip.py", line 9, in extract_archive
    with zipfile.ZipFile(archive_path, 'r') as zip_ref:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1349, in __init__
    self._RealGetContents()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1416, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
{'loss': 0.9422, 'grad_norm': 0.2779630422592163, 'learning_rate': 3.906246120865152e-06, 'epoch': 0.0}                                                                                                                    
{'eval_runtime': 380.9952, 'eval_samples_per_second': 29.368, 'eval_steps_per_second': 29.368, 'epoch': 0.0}                                                                                                               
{'loss': 0.9468, 'grad_norm': 0.21853122115135193, 'learning_rate': 3.906242241730305e-06, 'epoch': 0.0}                                                                                                                   
{'eval_runtime': 388.9377, 'eval_samples_per_second': 28.768, 'eval_steps_per_second': 28.768, 'epoch': 0.0}                                                                                                               
{'loss': 1.018, 'grad_norm': 0.20022796094417572, 'learning_rate': 3.906238362595458e-06, 'epoch': 0.0}                                                                                                                    
  0%|                                                                                                                                                                          | 30/10069900 [13:10<51206:24:19, 18.31s/it]
 36%|███████████████████████████████████████████████████████████████▊                                                                                                                 | 4037/11189 [02:15<04:14, 28.07it/s]



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
  0%|                                                                                                                                                                                         | 0/10069900 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 144, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 135, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3718, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3783, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1216, in forward
    loss = self.forward_loss(pixel_values, logits, mask, interpolate_pos_encoding=interpolate_pos_encoding)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1152, in forward_loss
    target = self.patchify(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1084, in patchify
    patchified_pixel_values = torch.einsum("ncdhwpq->ndhwpqc", patchified_pixel_values)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (7) does not match the number of dimensions (8) for operand 0 and no ellipsis was given
  0%| 
