/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

Process finished with exit code 1




æŠ¥é”™ï¼š/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

# train_affinity_3d.pyï¼š
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_affinity_3d_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 320, 320)
BATCH_SIZE = 2
NUM_EPOCHS = 100
NUM_CLASSES = 3  # z+, y+, x+ affinity
LR = 1e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return (focal_weight * bce_loss).mean()

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()

# mae3d_unet_finetune.pyï¼š
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit

        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)

# cremi_affinity_3d_dataset.pyï¼š
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
import random

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 320, 320), mean=127.91 / 255, std=28 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.D, self.H, self.W = self.raw.shape

    def __len__(self):
        return 10000  # number of random crops per epoch

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        zd, yh, xw = self.crop_size
        z = random.randint(0, self.D - zd)
        y = random.randint(0, self.H - yh)
        x = random.randint(0, self.W - xw)

        raw_crop = self.raw[z:z+zd, y:y+yh, x:x+xw].astype(np.float32)
        label_crop = self.labels[z:z+zd, y:y+yh, x:x+xw].astype(np.int64)

        # å½’ä¸€åŒ– + æ ‡å‡†åŒ–
        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

Process finished with exit code 1


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 48, in forward
    x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[2, 768, 2, 20, 20]' is invalid for input of size 307200



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 45, in forward
    features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 823, in forward
    embedding_output, mask, ids_restore = self.embeddings(
                                          ^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 322, in forward
    embeddings = embeddings + position_embeddings[:, 1:, :]
                 ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (200) must match the size of tensor b (800) at non-singleton dimension 1




final_reconstruction[mask_cpu] = reconstructed[mask_cpu]
                                     ~~~~~~~~~~~~~^^^^^^^^^^
IndexError: boolean index did not match indexed array along dimension 0; dimension is 32 but corresponding boolean dimension is 800
(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1780000
yes
Reconstruction loss: 0.4297
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 63, in <module>
    final_reconstruction[mask_cpu] = reconstructed[mask_cpu]
                                     ~~~~~~~~~~~~~^^^^^^^^^^
IndexError: boolean index did not match indexed array along dimension 0; dimension is 32 but corresponding boolean dimension is 800




(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1670000
yes
Reconstruction loss: 0.4615
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 58, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1129, in unpatchify
    patchified_pixel_values = torch.einsum("ndhwpqc->ncdhwpq", patchified_pixel_values)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (7) does not match the number of dimensions (8) for operand 0 and no ellipsis was given


(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1670000
yes
Reconstruction loss: 0.4652
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 58, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1112, in unpatchify
    num_patches_d = original_depth // pd
                    ~~~~~~~~~~~~~~~^^~~~
TypeError: unsupported operand type(s) for //: 'tuple' and 'int'



(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-50000
yes
Reconstruction loss: 0.6117
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 57, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1112, in unpatchify
    num_patches_d = original_depth // pd
                    ~~~~~~~~~~~~~~~^^~~~
TypeError: unsupported operand type(s) for //: 'list' and 'int'



(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-50000
yes
Reconstruction loss: 0.6272
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 53, in <module>
    masked_patchified[0][mask.bool()] = 0  # å°†è¢« mask çš„ patch ç½®ä¸º 0
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
IndexError: The shape of the mask [1, 800] at index 0 does not match the shape of the indexed tensor [800, 4096] at index 0


Traceback (most recent call last):                                                                                                                                                                                         
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3162, in _determine_best_metric
    metric_value = metrics[metric_to_check]
                   ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'eval_accuracy'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 216, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2620, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3094, in _maybe_log_save_evaluate
    is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3164, in _determine_best_metric
    raise KeyError(
KeyError: "The `metric_for_best_model` training argument is set to 'eval_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments."




Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 197, in main
    f.write(f"{name:60s} | shape: {tuple(param.shape):20s} | requires_grad={param.requires_grad}\n")
                                  ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: unsupported format string passed to tuple.__format__


warnings.warn(
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 133, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/hf_argparser.py", line 358, in parse_args_into_dataclasses
    obj = dtype(**inputs)
          ^^^^^^^^^^^^^^^
  File "<string>", line 136, in __init__
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/training_args.py", line 1678, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.STEPS
- Save strategy: SaveStrategy.EPOCH


tensorboard --logdir ./output_dir/tb_samples


AssertionError: size of input tensor and input format are different.         tensor shape: (1, 320), input_format: CHW



0%|                                                                                                                                                                            | 499/4176700 [01:03<148:04:00,  7.83it/s](pt12) guilin@guilin-System-Product-Name:~/data_proccess$ python unzip.py ./EM_pretrain_data
Extracting FAFB_crop_hdf_4.zip to ./EM_pretrain_data/FAFB_crop_hdf_4...
Extracting Kasthuri2015_hdf_5.zip to ./EM_pretrain_data/Kasthuri2015_hdf_5...
Extracting Kasthuri2015_hdf_9.zip to ./EM_pretrain_data/Kasthuri2015_hdf_9...
Extracting Kasthuri2015_hdf_8.zip to ./EM_pretrain_data/Kasthuri2015_hdf_8...
Extracting Kasthuri2015_hdf_2.zip to ./EM_pretrain_data/Kasthuri2015_hdf_2...
Extracting FIB-25_hdf_6.zip to ./EM_pretrain_data/FIB-25_hdf_6...
Extracting Kasthuri2015_hdf_3.zip to ./EM_pretrain_data/Kasthuri2015_hdf_3...
Traceback (most recent call last):
  File "/home/guilin/data_proccess/unzip.py", line 52, in <module>
    extract_all_in_dir(args.dir)
  File "/home/guilin/data_proccess/unzip.py", line 44, in extract_all_in_dir
    extract_archive(fpath, out_dir)
  File "/home/guilin/data_proccess/unzip.py", line 9, in extract_archive
    with zipfile.ZipFile(archive_path, 'r') as zip_ref:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1349, in __init__
    self._RealGetContents()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1416, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
{'loss': 0.9422, 'grad_norm': 0.2779630422592163, 'learning_rate': 3.906246120865152e-06, 'epoch': 0.0}                                                                                                                    
{'eval_runtime': 380.9952, 'eval_samples_per_second': 29.368, 'eval_steps_per_second': 29.368, 'epoch': 0.0}                                                                                                               
{'loss': 0.9468, 'grad_norm': 0.21853122115135193, 'learning_rate': 3.906242241730305e-06, 'epoch': 0.0}                                                                                                                   
{'eval_runtime': 388.9377, 'eval_samples_per_second': 28.768, 'eval_steps_per_second': 28.768, 'epoch': 0.0}                                                                                                               
{'loss': 1.018, 'grad_norm': 0.20022796094417572, 'learning_rate': 3.906238362595458e-06, 'epoch': 0.0}                                                                                                                    
  0%|                                                                                                                                                                          | 30/10069900 [13:10<51206:24:19, 18.31s/it]
 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                 | 4037/11189 [02:15<04:14, 28.07it/s]



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
  0%|                                                                                                                                                                                         | 0/10069900 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 144, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 135, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3718, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3783, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1216, in forward
    loss = self.forward_loss(pixel_values, logits, mask, interpolate_pos_encoding=interpolate_pos_encoding)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1152, in forward_loss
    target = self.patchify(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1084, in patchify
    patchified_pixel_values = torch.einsum("ncdhwpq->ndhwpqc", patchified_pixel_values)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (7) does not match the number of dimensions (8) for operand 0 and no ellipsis was given
  0%| 
