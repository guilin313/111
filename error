findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
findfont: Font family 'Times New Roman' not found.
✅ 已保存：/home/guilin/PycharmProjects/UNetR/scalar2/Metric_AffinityAcc.png




(base) guilin@guilin-System-Product-Name:~$ sudo fc-cache -fv
/usr/share/fonts: caching, new cache contents: 0 fonts, 6 dirs
/usr/share/fonts/X11: caching, new cache contents: 0 fonts, 4 dirs
/usr/share/fonts/X11/Type1: caching, new cache contents: 8 fonts, 0 dirs
/usr/share/fonts/X11/encodings: caching, new cache contents: 0 fonts, 1 dirs
/usr/share/fonts/X11/encodings/large: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/X11/misc: caching, new cache contents: 89 fonts, 0 dirs
/usr/share/fonts/X11/util: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cMap: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cmap: caching, new cache contents: 0 fonts, 5 dirs
/usr/share/fonts/cmap/adobe-cns1: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cmap/adobe-gb1: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cmap/adobe-japan1: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cmap/adobe-japan2: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/cmap/adobe-korea1: caching, new cache contents: 0 fonts, 0 dirs
/usr/share/fonts/opentype: caching, new cache contents: 0 fonts, 3 dirs
/usr/share/fonts/opentype/malayalam: caching, new cache contents: 7 fonts, 0 dirs
/usr/share/fonts/opentype/noto: caching, new cache contents: 80 fonts, 0 dirs
/usr/share/fonts/opentype/urw-base35: caching, new cache contents: 35 fonts, 0 dirs
/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 49 dirs
/usr/share/fonts/truetype/Gargi: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/Gubbi: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/Nakula: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/Navilu: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/Sahadeva: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/Sarai: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/abyssinica: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/arphic: caching, new cache contents: 8 fonts, 0 dirs
/usr/share/fonts/truetype/dejavu: caching, new cache contents: 6 fonts, 0 dirs
/usr/share/fonts/truetype/droid: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-beng-extra: caching, new cache contents: 6 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-deva-extra: caching, new cache contents: 3 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-gujr-extra: caching, new cache contents: 5 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-guru-extra: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-kalapi: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-orya-extra: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-telu-extra: caching, new cache contents: 2 fonts, 0 dirs
/usr/share/fonts/truetype/fonts-yrsa-rasa: caching, new cache contents: 15 fonts, 0 dirs
/usr/share/fonts/truetype/freefont: caching, new cache contents: 12 fonts, 0 dirs
/usr/share/fonts/truetype/kacst: caching, new cache contents: 15 fonts, 0 dirs
/usr/share/fonts/truetype/kacst-one: caching, new cache contents: 2 fonts, 0 dirs
/usr/share/fonts/truetype/lao: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs
/usr/share/fonts/truetype/liberation2: caching, new cache contents: 12 fonts, 0 dirs
/usr/share/fonts/truetype/libreoffice: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-assamese: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-bengali: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-devanagari: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-gujarati: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-kannada: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-malayalam: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-oriya: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-punjabi: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-tamil: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-tamil-classical: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/lohit-telugu: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/malayalam: caching, new cache contents: 10 fonts, 0 dirs
/usr/share/fonts/truetype/msttcorefonts: caching, new cache contents: 60 fonts, 0 dirs
/usr/share/fonts/truetype/noto: caching, new cache contents: 4 fonts, 0 dirs
/usr/share/fonts/truetype/padauk: caching, new cache contents: 4 fonts, 0 dirs
/usr/share/fonts/truetype/pagul: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/samyak: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/samyak-fonts: caching, new cache contents: 3 fonts, 0 dirs
/usr/share/fonts/truetype/sinhala: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/teluguvijayam: caching, new cache contents: 20 fonts, 0 dirs
/usr/share/fonts/truetype/tibetan-machine: caching, new cache contents: 1 fonts, 0 dirs
/usr/share/fonts/truetype/tlwg: caching, new cache contents: 58 fonts, 0 dirs
/usr/share/fonts/truetype/ttf-khmeros-core: caching, new cache contents: 2 fonts, 0 dirs
/usr/share/fonts/truetype/ubuntu: caching, new cache contents: 14 fonts, 0 dirs
/usr/share/fonts/type1: caching, new cache contents: 0 fonts, 1 dirs
/usr/share/fonts/type1/urw-base35: caching, new cache contents: 35 fonts, 0 dirs
/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs
/root/.local/share/fonts: skipping, no such directory
/root/.fonts: skipping, no such directory
/usr/share/fonts/X11: skipping, looped directory detected
/usr/share/fonts/cMap: skipping, looped directory detected
/usr/share/fonts/cmap: skipping, looped directory detected
/usr/share/fonts/opentype: skipping, looped directory detected
/usr/share/fonts/truetype: skipping, looped directory detected
/usr/share/fonts/type1: skipping, looped directory detected
/usr/share/fonts/X11/Type1: skipping, looped directory detected
/usr/share/fonts/X11/encodings: skipping, looped directory detected
/usr/share/fonts/X11/misc: skipping, looped directory detected
/usr/share/fonts/X11/util: skipping, looped directory detected
/usr/share/fonts/cmap/adobe-cns1: skipping, looped directory detected
/usr/share/fonts/cmap/adobe-gb1: skipping, looped directory detected
/usr/share/fonts/cmap/adobe-japan1: skipping, looped directory detected
/usr/share/fonts/cmap/adobe-japan2: skipping, looped directory detected
/usr/share/fonts/cmap/adobe-korea1: skipping, looped directory detected
/usr/share/fonts/opentype/malayalam: skipping, looped directory detected
/usr/share/fonts/opentype/noto: skipping, looped directory detected
/usr/share/fonts/opentype/urw-base35: skipping, looped directory detected
/usr/share/fonts/truetype/Gargi: skipping, looped directory detected
/usr/share/fonts/truetype/Gubbi: skipping, looped directory detected
/usr/share/fonts/truetype/Nakula: skipping, looped directory detected
/usr/share/fonts/truetype/Navilu: skipping, looped directory detected
/usr/share/fonts/truetype/Sahadeva: skipping, looped directory detected
/usr/share/fonts/truetype/Sarai: skipping, looped directory detected
/usr/share/fonts/truetype/abyssinica: skipping, looped directory detected
/usr/share/fonts/truetype/arphic: skipping, looped directory detected
/usr/share/fonts/truetype/dejavu: skipping, looped directory detected
/usr/share/fonts/truetype/droid: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-beng-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-deva-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-gujr-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-guru-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-kalapi: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-orya-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-telu-extra: skipping, looped directory detected
/usr/share/fonts/truetype/fonts-yrsa-rasa: skipping, looped directory detected
/usr/share/fonts/truetype/freefont: skipping, looped directory detected
/usr/share/fonts/truetype/kacst: skipping, looped directory detected
/usr/share/fonts/truetype/kacst-one: skipping, looped directory detected
/usr/share/fonts/truetype/lao: skipping, looped directory detected
/usr/share/fonts/truetype/liberation: skipping, looped directory detected
/usr/share/fonts/truetype/liberation2: skipping, looped directory detected
/usr/share/fonts/truetype/libreoffice: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-assamese: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-bengali: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-devanagari: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-gujarati: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-kannada: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-malayalam: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-oriya: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-punjabi: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-tamil: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-tamil-classical: skipping, looped directory detected
/usr/share/fonts/truetype/lohit-telugu: skipping, looped directory detected
/usr/share/fonts/truetype/malayalam: skipping, looped directory detected
/usr/share/fonts/truetype/msttcorefonts: skipping, looped directory detected
/usr/share/fonts/truetype/noto: skipping, looped directory detected
/usr/share/fonts/truetype/padauk: skipping, looped directory detected
/usr/share/fonts/truetype/pagul: skipping, looped directory detected
/usr/share/fonts/truetype/samyak: skipping, looped directory detected
/usr/share/fonts/truetype/samyak-fonts: skipping, looped directory detected
/usr/share/fonts/truetype/sinhala: skipping, looped directory detected
/usr/share/fonts/truetype/teluguvijayam: skipping, looped directory detected
/usr/share/fonts/truetype/tibetan-machine: skipping, looped directory detected
/usr/share/fonts/truetype/tlwg: skipping, looped directory detected
/usr/share/fonts/truetype/ttf-khmeros-core: skipping, looped directory detected
/usr/share/fonts/truetype/ubuntu: skipping, looped directory detected
/usr/share/fonts/type1/urw-base35: skipping, looped directory detected
/usr/share/fonts/X11/encodings/large: skipping, looped directory detected
/var/cache/fontconfig: cleaning cache directory
/root/.cache/fontconfig: not cleaning non-existent cache directory
/root/.fontconfig: not cleaning non-existent cache directory
fc-cache: succeeded
(base) guilin@guilin-System-Product-Name:~$ fc -list | grep "Times New Roman"
bash: fc: -i: invalid option
fc: usage: fc [-e ename] [-lnr] [first] [last] or fc -s [pat=rep] [command]




/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/UNetR/draw.py 
🔍 正在加载 run: ./logs
✅ 找到 12 个 scalar: ['train/loss', 'train/grad_norm', 'train/learning_rate', 'train/epoch', 'eval/runtime', 'eval/samples_per_second', 'eval/steps_per_second', 'train/train_runtime', 'train/train_samples_per_second', 'train/train_steps_per_second', 'train/total_flos', 'train/train_loss']

📋 所有可选 scalar 名：
[1] eval/runtime
[2] eval/samples_per_second
[3] eval/steps_per_second
[4] train/epoch
[5] train/grad_norm
[6] train/learning_rate
[7] train/loss
[8] train/total_flos
[9] train/train_loss
[10] train/train_runtime
[11] train/train_samples_per_second
[12] train/train_steps_per_second




/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_3d2.py 
yes
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_3d2.py", line 49, in <module>
    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=True))
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for MAEUNETRSkipSegmentation:
	Missing key(s) in state_dict: "mae.vit.embeddings.cls_token", "mae.vit.embeddings.position_embeddings", "mae.vit.embeddings.patch_embeddings.projection.weight", "mae.vit.embeddings.patch_embeddings.projection.bias", "mae.vit.encoder.layer.0.attention.attention.query.weight", "mae.vit.encoder.layer.0.attention.attention.query.bias", "mae.vit.encoder.layer.0.attention.attention.key.weight", "mae.vit.encoder.layer.0.attention.attention.key.bias", "mae.vit.encoder.layer.0.attention.attention.value.weight", "mae.vit.encoder.layer.0.attention.attention.value.bias", "mae.vit.encoder.layer.0.attention.output.dense.weight", "mae.vit.encoder.layer.0.attention.output.dense.bias", "mae.vit.encoder.layer.0.intermediate.dense.weight", "mae.vit.encoder.layer.0.intermediate.dense.bias", "mae.vit.encoder.layer.0.output.dense.weight", "mae.vit.encoder.layer.0.output.dense.bias", "mae.vit.encoder.layer.0.layernorm_before.weight", "mae.vit.encoder.layer.0.layernorm_before.bias", "mae.vit.encoder.layer.0.layernorm_after.weight", "mae.vit.encoder.layer.0.layernorm_after.bias", "mae.vit.encoder.layer.1.attention.attention.query.weight", "mae.vit.encoder.layer.1.attention.attention.query.bias", "mae.vit.encoder.layer.1.attention.attention.key.weight", "mae.vit.encoder.layer.1.attention.attention.key.bias", "mae.vit.encoder.layer.1.attention.attention.value.weight", "mae.vit.encoder.layer.1.attention.attention.value.bias", "mae.vit.encoder.layer.1.attention.output.dense.weight", "mae.vit.encoder.layer.1.attention.output.dense.bias", "mae.vit.encoder.layer.1.intermediate.dense.weight", "mae.vit.encoder.layer.1.intermediate.dense.bias", "mae.vit.encoder.layer.1.output.dense.weight", "mae.vit.encoder.layer.1.output.dense.bias", "mae.vit.encoder.layer.1.layernorm_before.weight", "mae.vit.encoder.layer.1.layernorm_before.bias", "mae.vit.encoder.layer.1.layernorm_after.weight", "mae.vit.encoder.layer.1.layernorm_after.bias", "mae.vit.encoder.layer.2.attention.attention.query.weight", "mae.vit.encoder.layer.2.attention.attention.query.bias", "mae.vit.encoder.layer.2.attention.attention.key.weight", "mae.vit.encoder.layer.2.attention.attention.key.bias", "mae.vit.encoder.layer.2.attention.attention.value.weight", "mae.vit.encoder.layer.2.attention.attention.value.bias", "mae.vit.encoder.layer.2.attention.output.dense.weight", "mae.vit.encoder.layer.2.attention.output.dense.bias", "mae.vit.encoder.layer.2.intermediate.dense.weight", "mae.vit.encoder.layer.2.intermediate.dense.bias", "mae.vit.encoder.layer.2.output.dense.weight", "mae.vit.encoder.layer.2.output.dense.bias", "mae.vit.encoder.layer.2.layernorm_before.weight", "mae.vit.encoder.layer.2.layernorm_before.bias", "mae.vit.encoder.layer.2.layernorm_after.weight", "mae.vit.encoder.layer.2.layernorm_after.bias", "mae.vit.encoder.layer.3.attention.attention.query.weight", "mae.vit.encoder.layer.3.attention.attention.query.bias", "mae.vit.encoder.layer.3.attention.attention.key.weight", "mae.vit.encoder.layer.3.attention.attention.key.bias", "mae.vit.encoder.layer.3.attention.attention.value.weight", "mae.vit.encoder.layer.3.attention.attention.value.bias", "mae.vit.encoder.layer.3.attention.output.dense.weight", "mae.vit.encoder.layer.3.attention.output.dense.bias", "mae.vit.encoder.layer.3.intermediate.dense.weight", "mae.vit.encoder.layer.3.intermediate.dense.bias", "mae.vit.encoder.layer.3.output.dense.weight", "mae.vit.encoder.layer.3.output.dense.bias", "mae.vit.encoder.layer.3.layernorm_before.weight", "mae.vit.encoder.layer.3.layernorm_before.bias", "mae.vit.encoder.layer.3.layernorm_after.weight", "mae.vit.encoder.layer.3.layernorm_after.bias", "mae.vit.encoder.layer.4.attention.attention.query.weight", "mae.vit.encoder.layer.4.attention.attention.query.bias", "mae.vit.encoder.layer.4.attention.attention.key.weight", "mae.vit.encoder.layer.4.attention.attention.key.bias", "mae.vit.encoder.layer.4.attention.attention.value.weight", "mae.vit.encoder.layer.4.attention.attention.value.bias", "mae.vit.encoder.layer.4.attention.output.dense.weight", "mae.vit.encoder.layer.4.attention.output.dense.bias", "mae.vit.encoder.layer.4.intermediate.dense.weight", "mae.vit.encoder.layer.4.intermediate.dense.bias", "mae.vit.encoder.layer.4.output.dense.weight", "mae.vit.encoder.layer.4.output.dense.bias", "mae.vit.encoder.layer.4.layernorm_before.weight", "mae.vit.encoder.layer.4.layernorm_before.bias", "mae.vit.encoder.layer.4.layernorm_after.weight", "mae.vit.encoder.layer.4.layernorm_after.bias", "mae.vit.encoder.layer.5.attention.attention.query.weight", "mae.vit.encoder.layer.5.attention.attention.query.bias", "mae.vit.encoder.layer.5.attention.attention.key.weight", "mae.vit.encoder.layer.5.attention.attention.key.bias", "mae.vit.encoder.layer.5.attention.attention.value.weight", "mae.vit.encoder.layer.5.attention.attention.value.bias", "mae.vit.encoder.layer.5.attention.output.dense.weight", "mae.vit.encoder.layer.5.attention.output.dense.bias", "mae.vit.encoder.layer.5.intermediate.dense.weight", "mae.vit.encoder.layer.5.intermediate.dense.bias", "mae.vit.encoder.layer.5.output.dense.weight", "mae.vit.encoder.layer.5.output.dense.bias", "mae.vit.encoder.layer.5.layernorm_before.weight", "mae.vit.encoder.layer.5.layernorm_before.bias", "mae.vit.encoder.layer.5.layernorm_after.weight", "mae.vit.encoder.layer.5.layernorm_after.bias", "mae.vit.encoder.layer.6.attention.attention.query.weight", "mae.vit.encoder.layer.6.attention.attention.query.bias", "mae.vit.encoder.layer.6.attention.attention.key.weight", "mae.vit.encoder.layer.6.attention.attention.key.bias", "mae.vit.encoder.layer.6.attention.attention.value.weight", "mae.vit.encoder.layer.6.attention.attention.value.bias", "mae.vit.encoder.layer.6.attention.output.dense.weight", "mae.vit.encoder.layer.6.attention.output.dense.bias", "mae.vit.encoder.layer.6.intermediate.dense.weight", "mae.vit.encoder.layer.6.intermediate.dense.bias", "mae.vit.encoder.layer.6.output.dense.weight", "mae.vit.encoder.layer.6.output.dense.bias", "mae.vit.encoder.layer.6.layernorm_before.weight", "mae.vit.encoder.layer.6.layernorm_before.bias", "mae.vit.encoder.layer.6.layernorm_after.weight", "mae.vit.encoder.layer.6.layernorm_after.bias", "mae.vit.encoder.layer.7.attention.attention.query.weight", "mae.vit.encoder.layer.7.attention.attention.query.bias", "mae.vit.encoder.layer.7.attention.attention.key.weight", "mae.vit.encoder.layer.7.attention.attention.key.bias", "mae.vit.encoder.layer.7.attention.attention.value.weight", "mae.vit.encoder.layer.7.attention.attention.value.bias", "mae.vit.encoder.layer.7.attention.output.dense.weight", "mae.vit.encoder.layer.7.attention.output.dense.bias", "mae.vit.encoder.layer.7.intermediate.dense.weight", "mae.vit.encoder.layer.7.intermediate.dense.bias", "mae.vit.encoder.layer.7.output.dense.weight", "mae.vit.encoder.layer.7.output.dense.bias", "mae.vit.encoder.layer.7.layernorm_before.weight", "mae.vit.encoder.layer.7.layernorm_before.bias", "mae.vit.encoder.layer.7.layernorm_after.weight", "mae.vit.encoder.layer.7.layernorm_after.bias", "mae.vit.encoder.layer.8.attention.attention.query.weight", "mae.vit.encoder.layer.8.attention.attention.query.bias", "mae.vit.encoder.layer.8.attention.attention.key.weight", "mae.vit.encoder.layer.8.attention.attention.key.bias", "mae.vit.encoder.layer.8.attention.attention.value.weight", "mae.vit.encoder.layer.8.attention.attention.value.bias", "mae.vit.encoder.layer.8.attention.output.dense.weight", "mae.vit.encoder.layer.8.attention.output.dense.bias", "mae.vit.encoder.layer.8.intermediate.dense.weight", "mae.vit.encoder.layer.8.intermediate.dense.bias", "mae.vit.encoder.layer.8.output.dense.weight", "mae.vit.encoder.layer.8.output.dense.bias", "mae.vit.encoder.layer.8.layernorm_before.weight", "mae.vit.encoder.layer.8.layernorm_before.bias", "mae.vit.encoder.layer.8.layernorm_after.weight", "mae.vit.encoder.layer.8.layernorm_after.bias", "mae.vit.encoder.layer.9.attention.attention.query.weight", "mae.vit.encoder.layer.9.attention.attention.query.bias", "mae.vit.encoder.layer.9.attention.attention.key.weight", "mae.vit.encoder.layer.9.attention.attention.key.bias", "mae.vit.encoder.layer.9.attention.attention.value.weight", "mae.vit.encoder.layer.9.attention.attention.value.bias", "mae.vit.encoder.layer.9.attention.output.dense.weight", "mae.vit.encoder.layer.9.attention.output.dense.bias", "mae.vit.encoder.layer.9.intermediate.dense.weight", "mae.vit.encoder.layer.9.intermediate.dense.bias", "mae.vit.encoder.layer.9.output.dense.weight", "mae.vit.encoder.layer.9.output.dense.bias", "mae.vit.encoder.layer.9.layernorm_before.weight", "mae.vit.encoder.layer.9.layernorm_before.bias", "mae.vit.encoder.layer.9.layernorm_after.weight", "mae.vit.encoder.layer.9.layernorm_after.bias", "mae.vit.layernorm.weight", "mae.vit.layernorm.bias", "mae.decoder.mask_token", "mae.decoder.decoder_pos_embed", "mae.decoder.decoder_embed.weight", "mae.decoder.decoder_embed.bias", "mae.decoder.decoder_layers.0.attention.attention.query.weight", "mae.decoder.decoder_layers.0.attention.attention.query.bias", "mae.decoder.decoder_layers.0.attention.attention.key.weight", "mae.decoder.decoder_layers.0.attention.attention.key.bias", "mae.decoder.decoder_layers.0.attention.attention.value.weight", "mae.decoder.decoder_layers.0.attention.attention.value.bias", "mae.decoder.decoder_layers.0.attention.output.dense.weight", "mae.decoder.decoder_layers.0.attention.output.dense.bias", "mae.decoder.decoder_layers.0.intermediate.dense.weight", "mae.decoder.decoder_layers.0.intermediate.dense.bias", "mae.decoder.decoder_layers.0.output.dense.weight", "mae.decoder.decoder_layers.0.output.dense.bias", "mae.decoder.decoder_layers.0.layernorm_before.weight", "mae.decoder.decoder_layers.0.layernorm_before.bias", "mae.decoder.decoder_layers.0.layernorm_after.weight", "mae.decoder.decoder_layers.0.layernorm_after.bias", "mae.decoder.decoder_layers.1.attention.attention.query.weight", "mae.decoder.decoder_layers.1.attention.attention.query.bias", "mae.decoder.decoder_layers.1.attention.attention.key.weight", "mae.decoder.decoder_layers.1.attention.attention.key.bias", "mae.decoder.decoder_layers.1.attention.attention.value.weight", "mae.decoder.decoder_layers.1.attention.attention.value.bias", "mae.decoder.decoder_layers.1.attention.output.dense.weight", "mae.decoder.decoder_layers.1.attention.output.dense.bias", "mae.decoder.decoder_layers.1.intermediate.dense.weight", "mae.decoder.decoder_layers.1.intermediate.dense.bias", "mae.decoder.decoder_layers.1.output.dense.weight", "mae.decoder.decoder_layers.1.output.dense.bias", "mae.decoder.decoder_layers.1.layernorm_before.weight", "mae.decoder.decoder_layers.1.layernorm_before.bias", "mae.decoder.decoder_layers.1.layernorm_after.weight", "mae.decoder.decoder_layers.1.layernorm_after.bias", "mae.decoder.decoder_layers.2.attention.attention.query.weight", "mae.decoder.decoder_layers.2.attention.attention.query.bias", "mae.decoder.decoder_layers.2.attention.attention.key.weight", "mae.decoder.decoder_layers.2.attention.attention.key.bias", "mae.decoder.decoder_layers.2.attention.attention.value.weight", "mae.decoder.decoder_layers.2.attention.attention.value.bias", "mae.decoder.decoder_layers.2.attention.output.dense.weight", "mae.decoder.decoder_layers.2.attention.output.dense.bias", "mae.decoder.decoder_layers.2.intermediate.dense.weight", "mae.decoder.decoder_layers.2.intermediate.dense.bias", "mae.decoder.decoder_layers.2.output.dense.weight", "mae.decoder.decoder_layers.2.output.dense.bias", "mae.decoder.decoder_layers.2.layernorm_before.weight", "mae.decoder.decoder_layers.2.layernorm_before.bias", "mae.decoder.decoder_layers.2.layernorm_after.weight", "mae.decoder.decoder_layers.2.layernorm_after.bias", "mae.decoder.decoder_layers.3.attention.attention.query.weight", "mae.decoder.decoder_layers.3.attention.attention.query.bias", "mae.decoder.decoder_layers.3.attention.attention.key.weight", "mae.decoder.decoder_layers.3.attention.attention.key.bias", "mae.decoder.decoder_layers.3.attention.attention.value.weight", "mae.decoder.decoder_layers.3.attention.attention.value.bias", "mae.decoder.decoder_layers.3.attention.output.dense.weight", "mae.decoder.decoder_layers.3.attention.output.dense.bias", "mae.decoder.decoder_layers.3.intermediate.dense.weight", "mae.decoder.decoder_layers.3.intermediate.dense.bias", "mae.decoder.decoder_layers.3.output.dense.weight", "mae.decoder.decoder_layers.3.output.dense.bias", "mae.decoder.decoder_layers.3.layernorm_before.weight", "mae.decoder.decoder_layers.3.layernorm_before.bias", "mae.decoder.decoder_layers.3.layernorm_after.weight", "mae.decoder.decoder_layers.3.layernorm_after.bias", "mae.decoder.decoder_layers.4.attention.attention.query.weight", "mae.decoder.decoder_layers.4.attention.attention.query.bias", "mae.decoder.decoder_layers.4.attention.attention.key.weight", "mae.decoder.decoder_layers.4.attention.attention.key.bias", "mae.decoder.decoder_layers.4.attention.attention.value.weight", "mae.decoder.decoder_layers.4.attention.attention.value.bias", "mae.decoder.decoder_layers.4.attention.output.dense.weight", "mae.decoder.decoder_layers.4.attention.output.dense.bias", "mae.decoder.decoder_layers.4.intermediate.dense.weight", "mae.decoder.decoder_layers.4.intermediate.dense.bias", "mae.decoder.decoder_layers.4.output.dense.weight", "mae.decoder.decoder_layers.4.output.dense.bias", "mae.decoder.decoder_layers.4.layernorm_before.weight", "mae.decoder.decoder_layers.4.layernorm_before.bias", "mae.decoder.decoder_layers.4.layernorm_after.weight", "mae.decoder.decoder_layers.4.layernorm_after.bias", "mae.decoder.decoder_layers.5.attention.attention.query.weight", "mae.decoder.decoder_layers.5.attention.attention.query.bias", "mae.decoder.decoder_layers.5.attention.attention.key.weight", "mae.decoder.decoder_layers.5.attention.attention.key.bias", "mae.decoder.decoder_layers.5.attention.attention.value.weight", "mae.decoder.decoder_layers.5.attention.attention.value.bias", "mae.decoder.decoder_layers.5.attention.output.dense.weight", "mae.decoder.decoder_layers.5.attention.output.dense.bias", "mae.decoder.decoder_layers.5.intermediate.dense.weight", "mae.decoder.decoder_layers.5.intermediate.dense.bias", "mae.decoder.decoder_layers.5.output.dense.weight", "mae.decoder.decoder_layers.5.output.dense.bias", "mae.decoder.decoder_layers.5.layernorm_before.weight", "mae.decoder.decoder_layers.5.layernorm_before.bias", "mae.decoder.decoder_layers.5.layernorm_after.weight", "mae.decoder.decoder_layers.5.layernorm_after.bias", "mae.decoder.decoder_norm.weight", "mae.decoder.decoder_norm.bias", "mae.decoder.decoder_pred.weight", "mae.decoder.decoder_pred.bias", "encoder.embeddings.cls_token", "encoder.embeddings.position_embeddings", "encoder.embeddings.patch_embeddings.projection.weight", "encoder.embeddings.patch_embeddings.projection.bias", "encoder.encoder.layer.0.attention.attention.query.weight", "encoder.encoder.layer.0.attention.attention.query.bias", "encoder.encoder.layer.0.attention.attention.key.weight", "encoder.encoder.layer.0.attention.attention.key.bias", "encoder.encoder.layer.0.attention.attention.value.weight", "encoder.encoder.layer.0.attention.attention.value.bias", "encoder.encoder.layer.0.attention.output.dense.weight", "encoder.encoder.layer.0.attention.output.dense.bias", "encoder.encoder.layer.0.intermediate.dense.weight", "encoder.encoder.layer.0.intermediate.dense.bias", "encoder.encoder.layer.0.output.dense.weight", "encoder.encoder.layer.0.output.dense.bias", "encoder.encoder.layer.0.layernorm_before.weight", "encoder.encoder.layer.0.layernorm_before.bias", "encoder.encoder.layer.0.layernorm_after.weight", "encoder.encoder.layer.0.layernorm_after.bias", "encoder.encoder.layer.1.attention.attention.query.weight", "encoder.encoder.layer.1.attention.attention.query.bias", "encoder.encoder.layer.1.attention.attention.key.weight", "encoder.encoder.layer.1.attention.attention.key.bias", "encoder.encoder.layer.1.attention.attention.value.weight", "encoder.encoder.layer.1.attention.attention.value.bias", "encoder.encoder.layer.1.attention.output.dense.weight", "encoder.encoder.layer.1.attention.output.dense.bias", "encoder.encoder.layer.1.intermediate.dense.weight", "encoder.encoder.layer.1.intermediate.dense.bias", "encoder.encoder.layer.1.output.dense.weight", "encoder.encoder.layer.1.output.dense.bias", "encoder.encoder.layer.1.layernorm_before.weight", "encoder.encoder.layer.1.layernorm_before.bias", "encoder.encoder.layer.1.layernorm_after.weight", "encoder.encoder.layer.1.layernorm_after.bias", "encoder.encoder.layer.2.attention.attention.query.weight", "encoder.encoder.layer.2.attention.attention.query.bias", "encoder.encoder.layer.2.attention.attention.key.weight", "encoder.encoder.layer.2.attention.attention.key.bias", "encoder.encoder.layer.2.attention.attention.value.weight", "encoder.encoder.layer.2.attention.attention.value.bias", "encoder.encoder.layer.2.attention.output.dense.weight", "encoder.encoder.layer.2.attention.output.dense.bias", "encoder.encoder.layer.2.intermediate.dense.weight", "encoder.encoder.layer.2.intermediate.dense.bias", "encoder.encoder.layer.2.output.dense.weight", "encoder.encoder.layer.2.output.dense.bias", "encoder.encoder.layer.2.layernorm_before.weight", "encoder.encoder.layer.2.layernorm_before.bias", "encoder.encoder.layer.2.layernorm_after.weight", "encoder.encoder.layer.2.layernorm_after.bias", "encoder.encoder.layer.3.attention.attention.query.weight", "encoder.encoder.layer.3.attention.attention.query.bias", "encoder.encoder.layer.3.attention.attention.key.weight", "encoder.encoder.layer.3.attention.attention.key.bias", "encoder.encoder.layer.3.attention.attention.value.weight", "encoder.encoder.layer.3.attention.attention.value.bias", "encoder.encoder.layer.3.attention.output.dense.weight", "encoder.encoder.layer.3.attention.output.dense.bias", "encoder.encoder.layer.3.intermediate.dense.weight", "encoder.encoder.layer.3.intermediate.dense.bias", "encoder.encoder.layer.3.output.dense.weight", "encoder.encoder.layer.3.output.dense.bias", "encoder.encoder.layer.3.layernorm_before.weight", "encoder.encoder.layer.3.layernorm_before.bias", "encoder.encoder.layer.3.layernorm_after.weight", "encoder.encoder.layer.3.layernorm_after.bias", "encoder.encoder.layer.4.attention.attention.query.weight", "encoder.encoder.layer.4.attention.attention.query.bias", "encoder.encoder.layer.4.attention.attention.key.weight", "encoder.encoder.layer.4.attention.attention.key.bias", "encoder.encoder.layer.4.attention.attention.value.weight", "encoder.encoder.layer.4.attention.attention.value.bias", "encoder.encoder.layer.4.attention.output.dense.weight", "encoder.encoder.layer.4.attention.output.dense.bias", "encoder.encoder.layer.4.intermediate.dense.weight", "encoder.encoder.layer.4.intermediate.dense.bias", "encoder.encoder.layer.4.output.dense.weight", "encoder.encoder.layer.4.output.dense.bias", "encoder.encoder.layer.4.layernorm_before.weight", "encoder.encoder.layer.4.layernorm_before.bias", "encoder.encoder.layer.4.layernorm_after.weight", "encoder.encoder.layer.4.layernorm_after.bias", "encoder.encoder.layer.5.attention.attention.query.weight", "encoder.encoder.layer.5.attention.attention.query.bias", "encoder.encoder.layer.5.attention.attention.key.weight", "encoder.encoder.layer.5.attention.attention.key.bias", "encoder.encoder.layer.5.attention.attention.value.weight", "encoder.encoder.layer.5.attention.attention.value.bias", "encoder.encoder.layer.5.attention.output.dense.weight", "encoder.encoder.layer.5.attention.output.dense.bias", "encoder.encoder.layer.5.intermediate.dense.weight", "encoder.encoder.layer.5.intermediate.dense.bias", "encoder.encoder.layer.5.output.dense.weight", "encoder.encoder.layer.5.output.dense.bias", "encoder.encoder.layer.5.layernorm_before.weight", "encoder.encoder.layer.5.layernorm_before.bias", "encoder.encoder.layer.5.layernorm_after.weight", "encoder.encoder.layer.5.layernorm_after.bias", "encoder.encoder.layer.6.attention.attention.query.weight", "encoder.encoder.layer.6.attention.attention.query.bias", "encoder.encoder.layer.6.attention.attention.key.weight", "encoder.encoder.layer.6.attention.attention.key.bias", "encoder.encoder.layer.6.attention.attention.value.weight", "encoder.encoder.layer.6.attention.attention.value.bias", "encoder.encoder.layer.6.attention.output.dense.weight", "encoder.encoder.layer.6.attention.output.dense.bias", "encoder.encoder.layer.6.intermediate.dense.weight", "encoder.encoder.layer.6.intermediate.dense.bias", "encoder.encoder.layer.6.output.dense.weight", "encoder.encoder.layer.6.output.dense.bias", "encoder.encoder.layer.6.layernorm_before.weight", "encoder.encoder.layer.6.layernorm_before.bias", "encoder.encoder.layer.6.layernorm_after.weight", "encoder.encoder.layer.6.layernorm_after.bias", "encoder.encoder.layer.7.attention.attention.query.weight", "encoder.encoder.layer.7.attention.attention.query.bias", "encoder.encoder.layer.7.attention.attention.key.weight", "encoder.encoder.layer.7.attention.attention.key.bias", "encoder.encoder.layer.7.attention.attention.value.weight", "encoder.encoder.layer.7.attention.attention.value.bias", "encoder.encoder.layer.7.attention.output.dense.weight", "encoder.encoder.layer.7.attention.output.dense.bias", "encoder.encoder.layer.7.intermediate.dense.weight", "encoder.encoder.layer.7.intermediate.dense.bias", "encoder.encoder.layer.7.output.dense.weight", "encoder.encoder.layer.7.output.dense.bias", "encoder.encoder.layer.7.layernorm_before.weight", "encoder.encoder.layer.7.layernorm_before.bias", "encoder.encoder.layer.7.layernorm_after.weight", "encoder.encoder.layer.7.layernorm_after.bias", "encoder.encoder.layer.8.attention.attention.query.weight", "encoder.encoder.layer.8.attention.attention.query.bias", "encoder.encoder.layer.8.attention.attention.key.weight", "encoder.encoder.layer.8.attention.attention.key.bias", "encoder.encoder.layer.8.attention.attention.value.weight", "encoder.encoder.layer.8.attention.attention.value.bias", "encoder.encoder.layer.8.attention.output.dense.weight", "encoder.encoder.layer.8.attention.output.dense.bias", "encoder.encoder.layer.8.intermediate.dense.weight", "encoder.encoder.layer.8.intermediate.dense.bias", "encoder.encoder.layer.8.output.dense.weight", "encoder.encoder.layer.8.output.dense.bias", "encoder.encoder.layer.8.layernorm_before.weight", "encoder.encoder.layer.8.layernorm_before.bias", "encoder.encoder.layer.8.layernorm_after.weight", "encoder.encoder.layer.8.layernorm_after.bias", "encoder.encoder.layer.9.attention.attention.query.weight", "encoder.encoder.layer.9.attention.attention.query.bias", "encoder.encoder.layer.9.attention.attention.key.weight", "encoder.encoder.layer.9.attention.attention.key.bias", "encoder.encoder.layer.9.attention.attention.value.weight", "encoder.encoder.layer.9.attention.attention.value.bias", "encoder.encoder.layer.9.attention.output.dense.weight", "encoder.encoder.layer.9.attention.output.dense.bias", "encoder.encoder.layer.9.intermediate.dense.weight", "encoder.encoder.layer.9.intermediate.dense.bias", "encoder.encoder.layer.9.output.dense.weight", "encoder.encoder.layer.9.output.dense.bias", "encoder.encoder.layer.9.layernorm_before.weight", "encoder.encoder.layer.9.layernorm_before.bias", "encoder.encoder.layer.9.layernorm_after.weight", "encoder.encoder.layer.9.layernorm_after.bias", "encoder.layernorm.weight", "encoder.layernorm.bias", "unetr.decoder0.0.block.0.block.weight", "unetr.decoder0.0.block.0.block.bias", "unetr.decoder0.0.block.1.weight", "unetr.decoder0.0.block.1.bias", "unetr.decoder0.0.block.1.running_mean", "unetr.decoder0.0.block.1.running_var", "unetr.decoder0.1.block.0.block.weight", "unetr.decoder0.1.block.0.block.bias", "unetr.decoder0.1.block.1.weight", "unetr.decoder0.1.block.1.bias", "unetr.decoder0.1.block.1.running_mean", "unetr.decoder0.1.block.1.running_var", "unetr.decoder2.0.block.0.block.weight", "unetr.decoder2.0.block.0.block.bias", "unetr.decoder2.0.block.1.block.weight", "unetr.decoder2.0.block.1.block.bias", "unetr.decoder2.0.block.2.weight", "unetr.decoder2.0.block.2.bias", "unetr.decoder2.0.block.2.running_mean", "unetr.decoder2.0.block.2.running_var", "unetr.decoder2.1.block.0.block.weight", "unetr.decoder2.1.block.0.block.bias", "unetr.decoder2.1.block.1.block.weight", "unetr.decoder2.1.block.1.block.bias", "unetr.decoder2.1.block.2.weight", "unetr.decoder2.1.block.2.bias", "unetr.decoder2.1.block.2.running_mean", "unetr.decoder2.1.block.2.running_var", "unetr.decoder2.2.block.0.block.weight", "unetr.decoder2.2.block.0.block.bias", "unetr.decoder2.2.block.1.block.weight", "unetr.decoder2.2.block.1.block.bias", "unetr.decoder2.2.block.2.weight", "unetr.decoder2.2.block.2.bias", "unetr.decoder2.2.block.2.running_mean", "unetr.decoder2.2.block.2.running_var", "unetr.decoder4.0.block.0.block.weight", "unetr.decoder4.0.block.0.block.bias", "unetr.decoder4.0.block.1.block.weight", "unetr.decoder4.0.block.1.block.bias", "unetr.decoder4.0.block.2.weight", "unetr.decoder4.0.block.2.bias", "unetr.decoder4.0.block.2.running_mean", "unetr.decoder4.0.block.2.running_var", "unetr.decoder4.1.block.0.block.weight", "unetr.decoder4.1.block.0.block.bias", "unetr.decoder4.1.block.1.block.weight", "unetr.decoder4.1.block.1.block.bias", "unetr.decoder4.1.block.2.weight", "unetr.decoder4.1.block.2.bias", "unetr.decoder4.1.block.2.running_mean", "unetr.decoder4.1.block.2.running_var", "unetr.decoder6.block.0.block.weight", "unetr.decoder6.block.0.block.bias", "unetr.decoder6.block.1.block.weight", "unetr.decoder6.block.1.block.bias", "unetr.decoder6.block.2.weight", "unetr.decoder6.block.2.bias", "unetr.decoder6.block.2.running_mean", "unetr.decoder6.block.2.running_var", "unetr.decoder8_upsampler.block.weight", "unetr.decoder8_upsampler.block.bias", "unetr.decoder6_upsampler.0.block.0.block.weight", "unetr.decoder6_upsampler.0.block.0.block.bias", "unetr.decoder6_upsampler.0.block.1.weight", "unetr.decoder6_upsampler.0.block.1.bias", "unetr.decoder6_upsampler.0.block.1.running_mean", "unetr.decoder6_upsampler.0.block.1.running_var", "unetr.decoder6_upsampler.1.block.0.block.weight", "unetr.decoder6_upsampler.1.block.0.block.bias", "unetr.decoder6_upsampler.1.block.1.weight", "unetr.decoder6_upsampler.1.block.1.bias", "unetr.decoder6_upsampler.1.block.1.running_mean", "unetr.decoder6_upsampler.1.block.1.running_var", "unetr.decoder6_upsampler.2.block.0.block.weight", "unetr.decoder6_upsampler.2.block.0.block.bias", "unetr.decoder6_upsampler.2.block.1.weight", "unetr.decoder6_upsampler.2.block.1.bias", "unetr.decoder6_upsampler.2.block.1.running_mean", "unetr.decoder6_upsampler.2.block.1.running_var", "unetr.decoder6_upsampler.3.block.weight", "unetr.decoder6_upsampler.3.block.bias", "unetr.decoder4_upsampler.0.block.0.block.weight", "unetr.decoder4_upsampler.0.block.0.block.bias", "unetr.decoder4_upsampler.0.block.1.weight", "unetr.decoder4_upsampler.0.block.1.bias", "unetr.decoder4_upsampler.0.block.1.running_mean", "unetr.decoder4_upsampler.0.block.1.running_var", "unetr.decoder4_upsampler.1.block.0.block.weight", "unetr.decoder4_upsampler.1.block.0.block.bias", "unetr.decoder4_upsampler.1.block.1.weight", "unetr.decoder4_upsampler.1.block.1.bias", "unetr.decoder4_upsampler.1.block.1.running_mean", "unetr.decoder4_upsampler.1.block.1.running_var", "unetr.decoder4_upsampler.2.block.weight", "unetr.decoder4_upsampler.2.block.bias", "unetr.decoder2_upsampler.0.block.0.block.weight", "unetr.decoder2_upsampler.0.block.0.block.bias", "unetr.decoder2_upsampler.0.block.1.weight", "unetr.decoder2_upsampler.0.block.1.bias", "unetr.decoder2_upsampler.0.block.1.running_mean", "unetr.decoder2_upsampler.0.block.1.running_var", "unetr.decoder2_upsampler.1.block.0.block.weight", "unetr.decoder2_upsampler.1.block.0.block.bias", "unetr.decoder2_upsampler.1.block.1.weight", "unetr.decoder2_upsampler.1.block.1.bias", "unetr.decoder2_upsampler.1.block.1.running_mean", "unetr.decoder2_upsampler.1.block.1.running_var", "unetr.decoder2_upsampler.2.block.weight", "unetr.decoder2_upsampler.2.block.bias", "unetr.decoder0_header.0.block.0.block.weight", "unetr.decoder0_header.0.block.0.block.bias", "unetr.decoder0_header.0.block.1.weight", "unetr.decoder0_header.0.block.1.bias", "unetr.decoder0_header.0.block.1.running_mean", "unetr.decoder0_header.0.block.1.running_var", "unetr.decoder0_header.1.block.0.block.weight", "unetr.decoder0_header.1.block.0.block.bias", "unetr.decoder0_header.1.block.1.weight", "unetr.decoder0_header.1.block.1.bias", "unetr.decoder0_header.1.block.1.running_mean", "unetr.decoder0_header.1.block.1.running_var", "unetr.decoder0_header.2.block.weight", "unetr.decoder0_header.2.block.bias". 
	Unexpected key(s) in state_dict: "epoch", "model_state_dict", "optimizer_state_dict". 

Process finished with exit code 1



[Patch 0] ARAND: 0.5549, VOI: 3.5720 (split: 1.3232, merge: 2.2488)
[Patch 1] ARAND: 0.5500, VOI: 3.1415 (split: 1.4677, merge: 1.6738)
[Patch 2] ARAND: 0.3893, VOI: 2.9662 (split: 1.2318, merge: 1.7344)
[Patch 3] ARAND: 0.5120, VOI: 3.4666 (split: 0.8966, merge: 2.5701)
[Patch 4] ARAND: 0.6393, VOI: 4.0402 (split: 2.0701, merge: 1.9701)
[Patch 5] ARAND: 0.6497, VOI: 3.7094 (split: 1.7294, merge: 1.9800)

=== Average Metrics ===
ARAND: 0.5492
VOI:   3.4827 (split: 1.4532, merge: 2.0295)

Process finished with exit code 0

Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/evaluate_segmentation.py", line 24, in <module>
    arand = adapted_rand(pred, gt)[-1]
            ~~~~~~~~~~~~~~~~~~~~~~^^^^
IndexError: invalid index to scalar variable.

Process finished with exit code 1


[Patch 0] pred max: 0.9862, mean: 0.7899, loss: 0.2193
[Patch 0] fragments count: 29
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 29 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 30
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.077364
merged 25 edges
extracting segmentation
[Patch 1] pred max: 0.9997, mean: 0.8206, loss: 0.1929
[Patch 1] fragments count: 45
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 45 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 108
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.0665805
merged 43 edges
extracting segmentation
[Patch 2] pred max: 0.9888, mean: 0.7962, loss: 0.2164
[Patch 2] fragments count: 34
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 34 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 37
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.0811528
merged 29 edges
extracting segmentation
[Patch 3] pred max: 0.9936, mean: 0.8094, loss: 0.2022
[Patch 3] fragments count: 17
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 17 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 25
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.12472
merged 15 edges
extracting segmentation
[Patch 4] pred max: 0.9873, mean: 0.7976, loss: 0.2124
[Patch 4] fragments count: 63
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 63 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 95
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.077
merged 58 edges
extracting segmentation
[Patch 5] pred max: 0.9859, mean: 0.8011, loss: 0.2074
[Patch 5] fragments count: 79
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 79 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 125
merging until threshold 0.7
computing initial scores
merging until 0.7
min edge score 0.06204
merged 74 edges
extracting segmentation
✅ Saved 6 instance segmentations and debug outputs to ./results_affinity3d_waterz
✅ Average test loss: 0.2084



[Patch 0] pred max: 0.9935, mean: 0.7898, loss: 0.2195
[Patch 0] fragments count: 33
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 33 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 66
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.0900373
threshold exceeded
merged 31 edges
extracting segmentation
[Patch 1] pred max: 0.9945, mean: 0.8208, loss: 0.1927
[Patch 1] fragments count: 3
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 3 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 1
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.262083
merged 1 edges
extracting segmentation
[Patch 2] pred max: 0.9892, mean: 0.7962, loss: 0.2164
[Patch 2] fragments count: 27
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 27 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 48
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.0987157
merged 25 edges
extracting segmentation
[Patch 3] pred max: 0.9936, mean: 0.8094, loss: 0.2022
[Patch 3] fragments count: 14
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 14 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 26
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.0891793
merged 12 edges
extracting segmentation
[Patch 4] pred max: 0.9881, mean: 0.7977, loss: 0.2123
[Patch 4] fragments count: 19
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 19 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 44
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.123549
threshold exceeded
merged 17 edges
extracting segmentation
[Patch 5] pred max: 0.9858, mean: 0.8011, loss: 0.2074
[Patch 5] fragments count: 34
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 34 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 85
merging until threshold 0.5
computing initial scores
merging until 0.5
min edge score 0.0776682
threshold exceeded
merged 32 edges
extracting segmentation
✅ Saved 6 instance segmentations and debug outputs to ./results_affinity3d_waterz
✅ Average test loss: 0.2084


[Patch 0] pred max: 0.9864, mean: 0.7898, loss: 0.2194
[Patch 0] fragments count: 34
Re-using already compiled waterz version
Preparing segmentation volume...
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_3d2.py", line 89, in <module>
    seg = next(waterz.agglomerate(affs, [THRESHOLD], fragments=fragments))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "waterz_ee2c27d157b5fd2f8fcb276fc5aa64b2.pyx", line 39, in agglomerate
    cdef WaterzState state = __initialize(affs, segmentation, gt, aff_threshold_low, aff_threshold_high, find_fragments)
  File "waterz_ee2c27d157b5fd2f8fcb276fc5aa64b2.pyx", line 73, in waterz_ee2c27d157b5fd2f8fcb276fc5aa64b2.__initialize
    def __initialize(
ValueError: Buffer dtype mismatch, expected 'uint64_t' but got 'int'


[Patch 0] pred max: 0.9859, mean: 0.7900, loss: 0.2192
[Patch 0] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 1] pred max: 0.9874, mean: 0.8209, loss: 0.1925
[Patch 1] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 2] pred max: 0.9887, mean: 0.7961, loss: 0.2165
[Patch 2] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 3] pred max: 0.9936, mean: 0.8094, loss: 0.2023
[Patch 3] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 4] pred max: 0.9882, mean: 0.7977, loss: 0.2123
[Patch 4] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 5] pred max: 0.9850, mean: 0.8012, loss: 0.2073
[Patch 5] fragments count: 2
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
✅ Saved 6 instance segmentations and debug outputs to ./results_affinity3d_waterz
✅ Average test loss: 0.2083

Process finished with exit code 0



[Patch 0] pred max: 0.9863, mean: 0.7899, loss: 0.2193
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 1] pred max: 0.9899, mean: 0.8206, loss: 0.1930
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 2] pred max: 0.9894, mean: 0.7961, loss: 0.2165
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 3] pred max: 0.9937, mean: 0.8094, loss: 0.2023
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 4] pred max: 0.9896, mean: 0.7977, loss: 0.2124
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
[Patch 5] pred max: 0.9848, mean: 0.8012, loss: 0.2073
Re-using already compiled waterz version
Preparing segmentation volume...
counting regions and sizes...
creating region graph for 2 nodes
creating statistics provider
extracting region graph...
Region graph number of edges: 0
merging until threshold 0.5
computing initial scores
merging until 0.5
merged 0 edges
✅ Saved 6 instance segmentations and debug outputs to ./results_affinity3d_waterz
✅ Average test loss: 0.2085



Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_3d2.py", line 6, in <module>
    from skimage.filters import gaussian
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/skimage/__init__.py", line 124, in <module>
    from ._shared import geometry
  File "geometry.pyx", line 1, in init skimage._shared.geometry
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

Process finished with exit code 1


conda activate pt12  # 或 source activate pt12
pip install -U pip setuptools wheel cython
pip install . --no-build-isolation



Processing /home/guilin/Downloads/waterz-master
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [18 lines of output]
      Traceback (most recent call last):
        File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 389, in <module>
          main()
        File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 373, in main
          json_out["return_val"] = hook(**hook_input["kwargs"])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 143, in get_requires_for_build_wheel
          return hook(config_settings)
                 ^^^^^^^^^^^^^^^^^^^^^
        File "/tmp/pip-build-env-f3jgr4lf/overlay/lib/python3.12/site-packages/setuptools/build_meta.py", line 334, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=[])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File "/tmp/pip-build-env-f3jgr4lf/overlay/lib/python3.12/site-packages/setuptools/build_meta.py", line 304, in _get_build_requires
          self.run_setup()
        File "/tmp/pip-build-env-f3jgr4lf/overlay/lib/python3.12/site-packages/setuptools/build_meta.py", line 320, in run_setup
          exec(code, locals())
        File "<string>", line 4, in <module>
      ModuleNotFoundError: No module named 'Cython'
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 55, in <module>
    for i, (raw, target_aff) in enumerate(test_loader):
           ^^^^^^^^^^^^^^^^^
ValueError: too many values to unpack (expected 2)

Process finished with exit code 1



Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/UNet2/train.py", line 61, in <module>
    model = get_unetr_model(img_size=CROP_SIZE, in_channels=1, out_channels=NUM_CLASSES).to(DEVICE)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/UNet2/model_unetr.py", line 5, in get_unetr_model
    return UNETR(
           ^^^^^^
TypeError: UNETR.__init__() got an unexpected keyword argument 'pos_embed'

Process finished with exit code 1



Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 44, in <module>
    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=True))
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for MAEUNet2SkipSegmentation:
        Missing key(s) in state_dict: "proj2.weight", "proj2.bias", "proj4.weight", "proj4.bias", "proj6.weight", "proj6.bias", "proj8.weight", "proj8.bias", "decoder.up1.weight", "decoder.up1.bias", "decoder.conv1.block.0.weight", "decoder.conv1.block.0.bias", "decoder.conv1.block.2.weight", "decoder.conv1.block.2.bias", "decoder.up2.weight", "decoder.up2.bias", "decoder.conv2.block.0.weight", "decoder.conv2.block.0.bias", "decoder.conv2.block.2.weight", "decoder.conv2.block.2.bias", "decoder.up3.weight", "decoder.up3.bias", "decoder.conv3.block.0.weight", "decoder.conv3.block.0.bias", "decoder.conv3.block.2.weight", "decoder.conv3.block.2.bias", "decoder.up4.weight", "decoder.up4.bias", "decoder.conv4.block.0.weight", "decoder.conv4.block.0.bias", "decoder.conv4.block.2.weight", "decoder.conv4.block.2.bias", "decoder.out.weight", "decoder.out.bias". 
        Unexpected key(s) in state_dict: "unetr.decoder0.0.block.0.block.weight", "unetr.decoder0.0.block.0.block.bias", "unetr.decoder0.0.block.1.weight", "unetr.decoder0.0.block.1.bias", "unetr.decoder0.0.block.1.running_mean", "unetr.decoder0.0.block.1.running_var", "unetr.decoder0.0.block.1.num_batches_tracked", "unetr.decoder0.1.block.0.block.weight", "unetr.decoder0.1.block.0.block.bias", "unetr.decoder0.1.block.1.weight", "unetr.decoder0.1.block.1.bias", "unetr.decoder0.1.block.1.running_mean", "unetr.decoder0.1.block.1.running_var", "unetr.decoder0.1.block.1.num_batches_tracked", "unetr.decoder2.0.block.0.block.weight", "unetr.decoder2.0.block.0.block.bias", "unetr.decoder2.0.block.1.block.weight", "unetr.decoder2.0.block.1.block.bias", "unetr.decoder2.0.block.2.weight", "unetr.decoder2.0.block.2.bias", "unetr.decoder2.0.block.2.running_mean", "unetr.decoder2.0.block.2.running_var", "unetr.decoder2.0.block.2.num_batches_tracked", "unetr.decoder2.1.block.0.block.weight", "unetr.decoder2.1.block.0.block.bias", "unetr.decoder2.1.block.1.block.weight", "unetr.decoder2.1.block.1.block.bias", "unetr.decoder2.1.block.2.weight", "unetr.decoder2.1.block.2.bias", "unetr.decoder2.1.block.2.running_mean", "unetr.decoder2.1.block.2.running_var", "unetr.decoder2.1.block.2.num_batches_tracked", "unetr.decoder2.2.block.0.block.weight", "unetr.decoder2.2.block.0.block.bias", "unetr.decoder2.2.block.1.block.weight", "unetr.decoder2.2.block.1.block.bias", "unetr.decoder2.2.block.2.weight", "unetr.decoder2.2.block.2.bias", "unetr.decoder2.2.block.2.running_mean", "unetr.decoder2.2.block.2.running_var", "unetr.decoder2.2.block.2.num_batches_tracked", "unetr.decoder4.0.block.0.block.weight", "unetr.decoder4.0.block.0.block.bias", "unetr.decoder4.0.block.1.block.weight", "unetr.decoder4.0.block.1.block.bias", "unetr.decoder4.0.block.2.weight", "unetr.decoder4.0.block.2.bias", "unetr.decoder4.0.block.2.running_mean", "unetr.decoder4.0.block.2.running_var", "unetr.decoder4.0.block.2.num_batches_tracked", "unetr.decoder4.1.block.0.block.weight", "unetr.decoder4.1.block.0.block.bias", "unetr.decoder4.1.block.1.block.weight", "unetr.decoder4.1.block.1.block.bias", "unetr.decoder4.1.block.2.weight", "unetr.decoder4.1.block.2.bias", "unetr.decoder4.1.block.2.running_mean", "unetr.decoder4.1.block.2.running_var", "unetr.decoder4.1.block.2.num_batches_tracked", "unetr.decoder6.block.0.block.weight", "unetr.decoder6.block.0.block.bias", "unetr.decoder6.block.1.block.weight", "unetr.decoder6.block.1.block.bias", "unetr.decoder6.block.2.weight", "unetr.decoder6.block.2.bias", "unetr.decoder6.block.2.running_mean", "unetr.decoder6.block.2.running_var", "unetr.decoder6.block.2.num_batches_tracked", "unetr.decoder8_upsampler.block.weight", "unetr.decoder8_upsampler.block.bias", "unetr.decoder6_upsampler.0.block.0.block.weight", "unetr.decoder6_upsampler.0.block.0.block.bias", "unetr.decoder6_upsampler.0.block.1.weight", "unetr.decoder6_upsampler.0.block.1.bias", "unetr.decoder6_upsampler.0.block.1.running_mean", "unetr.decoder6_upsampler.0.block.1.running_var", "unetr.decoder6_upsampler.0.block.1.num_batches_tracked", "unetr.decoder6_upsampler.1.block.0.block.weight", "unetr.decoder6_upsampler.1.block.0.block.bias", "unetr.decoder6_upsampler.1.block.1.weight", "unetr.decoder6_upsampler.1.block.1.bias", "unetr.decoder6_upsampler.1.block.1.running_mean", "unetr.decoder6_upsampler.1.block.1.running_var", "unetr.decoder6_upsampler.1.block.1.num_batches_tracked", "unetr.decoder6_upsampler.2.block.0.block.weight", "unetr.decoder6_upsampler.2.block.0.block.bias", "unetr.decoder6_upsampler.2.block.1.weight", "unetr.decoder6_upsampler.2.block.1.bias", "unetr.decoder6_upsampler.2.block.1.running_mean", "unetr.decoder6_upsampler.2.block.1.running_var", "unetr.decoder6_upsampler.2.block.1.num_batches_tracked", "unetr.decoder6_upsampler.3.block.weight", "unetr.decoder6_upsampler.3.block.bias", "unetr.decoder4_upsampler.0.block.0.block.weight", "unetr.decoder4_upsampler.0.block.0.block.bias", "unetr.decoder4_upsampler.0.block.1.weight", "unetr.decoder4_upsampler.0.block.1.bias", "unetr.decoder4_upsampler.0.block.1.running_mean", "unetr.decoder4_upsampler.0.block.1.running_var", "unetr.decoder4_upsampler.0.block.1.num_batches_tracked", "unetr.decoder4_upsampler.1.block.0.block.weight", "unetr.decoder4_upsampler.1.block.0.block.bias", "unetr.decoder4_upsampler.1.block.1.weight", "unetr.decoder4_upsampler.1.block.1.bias", "unetr.decoder4_upsampler.1.block.1.running_mean", "unetr.decoder4_upsampler.1.block.1.running_var", "unetr.decoder4_upsampler.1.block.1.num_batches_tracked", "unetr.decoder4_upsampler.2.block.weight", "unetr.decoder4_upsampler.2.block.bias", "unetr.decoder2_upsampler.0.block.0.block.weight", "unetr.decoder2_upsampler.0.block.0.block.bias", "unetr.decoder2_upsampler.0.block.1.weight", "unetr.decoder2_upsampler.0.block.1.bias", "unetr.decoder2_upsampler.0.block.1.running_mean", "unetr.decoder2_upsampler.0.block.1.running_var", "unetr.decoder2_upsampler.0.block.1.num_batches_tracked", "unetr.decoder2_upsampler.1.block.0.block.weight", "unetr.decoder2_upsampler.1.block.0.block.bias", "unetr.decoder2_upsampler.1.block.1.weight", "unetr.decoder2_upsampler.1.block.1.bias", "unetr.decoder2_upsampler.1.block.1.running_mean", "unetr.decoder2_upsampler.1.block.1.running_var", "unetr.decoder2_upsampler.1.block.1.num_batches_tracked", "unetr.decoder2_upsampler.2.block.weight", "unetr.decoder2_upsampler.2.block.bias", "unetr.decoder0_header.0.block.0.block.weight", "unetr.decoder0_header.0.block.0.block.bias", "unetr.decoder0_header.0.block.1.weight", "unetr.decoder0_header.0.block.1.bias", "unetr.decoder0_header.0.block.1.running_mean", "unetr.decoder0_header.0.block.1.running_var", "unetr.decoder0_header.0.block.1.num_batches_tracked", "unetr.decoder0_header.1.block.0.block.weight", "unetr.decoder0_header.1.block.0.block.bias", "unetr.decoder0_header.1.block.1.weight", "unetr.decoder0_header.1.block.1.bias", "unetr.decoder0_header.1.block.1.running_mean", "unetr.decoder0_header.1.block.1.running_var", "unetr.decoder0_header.1.block.1.num_batches_tracked", "unetr.decoder0_header.2.block.weight", "unetr.decoder0_header.2.block.bias". 





Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unetR.py", line 49, in forward
    output = self.unetr(x,  [z2, z4, z6, z8])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/UNetR.py", line 267, in forward
    z2 = self.proj_feat(hidden_states_out[0], self.embed_dim, grid_size)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/UNetR.py", line 255, in proj_feat
    x = x.view(new_view)
        ^^^^^^^^^^^^^^^^
RuntimeError: shape '[2, 2, 20, 20, 768]' is invalid for input of size 1230336

Process finished with exit code 1


Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unetR.py", line 48, in forward
    output = self.unetr(x, z12, [z3, z6, z9, z12])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: UNETR.forward() takes 2 positional arguments but 4 were given

Process finished with exit code 1



Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_continue.py", line 119, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unetR.py", line 48, in forward
    output = self.unetr(x, z12, [z3, z6, z9, z12])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: UNETR.forward() takes 2 positional arguments but 4 were given

Process finished with exit code 1



Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inspect_encoder.py", line 44, in <module>
    output = model(dummy_input)
             ^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 94, in forward
    out = self.encoder(pixel_values=x, output_hidden_states=True, return_dict=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 823, in forward
    embedding_output, mask, ids_restore = self.embeddings(
                                          ^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 315, in forward
    embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 363, in forward
    raise ValueError(
ValueError: Make sure that the channel dimension of the pixel values match with the one set in the configuration.




/home/guilin/PycharmProjects/MAE3d/inspect_encoder.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(checkpoint_path, map_location=DEVICE)
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inspect_encoder.py", line 24, in <module>
    model.load_state_dict(state_dict["model_state_dict"])
                          ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
KeyError: 'model_state_dict'

Process finished with exit code 1


Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 105, in forward
    return self.decoder(main, [skip1, skip2, skip3, skip4])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 53, in forward
    return self.out(x)
           ^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 725, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 720, in _conv_forward
    return F.conv3d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [3, 32, 1, 1, 1], expected input[2, 64, 32, 320, 320] to have 32 channels, but got 64 channels instead

Process finished with exit code 1



Epoch 1/300 [Train]:   0%|          | 0/93 [00:00<?, ?it/s]Total params: 108903715, Trainable: 33953443
Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 104, in forward
    return self.decoder(main, [skip1, skip2, skip3, skip4])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 38, in forward
    x = torch.cat([x, skips[3]], dim=1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list.

Process finished with exit code 1



x = self.up1(x)
x = torch.cat([x, F.interpolate(skips[0], size=x.shape[2:], mode="trilinear", align_corners=False)], dim=1)
x = self.conv1(x)

x = self.up2(x)
x = torch.cat([x, F.interpolate(skips[1], size=x.shape[2:], mode="trilinear", align_corners=False)], dim=1)
x = self.conv2(x)

x = self.up3(x)
x = torch.cat([x, F.interpolate(skips[2], size=x.shape[2:], mode="trilinear", align_corners=False)], dim=1)
x = self.conv3(x)


Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 124, in forward
    return self.decoder(main, [skip1, skip2, skip3])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 40, in forward
    x = torch.cat([x, skips[0]], dim=1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 2 for tensor number 1 in the list.

Process finished with exit code 1


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Total params: 108713155, Trainable: 33762883
Epoch 1/300 [Train]:   0%|          | 0/93 [00:00<?, ?it/s]x_embed = self.encoder.embeddings(x)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
x_embed = blk(x_embed)为tuple，转为tensor
Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 124, in forward
    return self.decoder(main, [skip1, skip2, skip3])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 40, in forward
    x = torch.cat([x, skips[0]], dim=1)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 16 for tensor number 1 in the list.

Process finished with exit code 1



Total params: 108713155, Trainable: 33762883
Epoch 1/300 [Train]:   0%|          | 0/93 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 104, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 96, in forward
    x_embed = blk(x_embed)
              ^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 611, in forward
    self.layernorm_before(hidden_states),  # in ViTMAE, layernorm is applied before self-attention
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/functional.py", line 2900, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
TypeError: layer_norm(): argument 'input' (position 1) must be Tensor, not tuple

Process finished with exit code 1



Some weights of ViTMAEForPreTraining were not initialized from the model checkpoint at /home/guilin/PycharmProjects/MAE3d/output/vitmae3d and are newly initialized because the shapes did not match:
- decoder.decoder_pos_embed: found shape torch.Size([1, 801, 384]) in the checkpoint and torch.Size([1, 201, 384]) in the model instantiated
- vit.embeddings.position_embeddings: found shape torch.Size([1, 801, 768]) in the checkpoint and torch.Size([1, 201, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/300 [Train]:   0%|          | 0/372 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 101, in <module>
    loss = criterion(outputs, targets) * weight
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 46, in forward
    return (focal_loss * self.channel_weights).mean()
            ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!


[Patch 0] pred max: 0.8334, mean: 0.7179, loss: 0.3466
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 1] pred max: 0.8334, mean: 0.7179, loss: 0.3445
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 2] pred max: 0.8334, mean: 0.7179, loss: 0.3521
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 3] pred max: 0.8334, mean: 0.7179, loss: 0.3565
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 4] pred max: 0.8334, mean: 0.7179, loss: 0.3605
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 5] pred max: 0.8334, mean: 0.7179, loss: 0.3745
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 6] pred max: 0.8334, mean: 0.7179, loss: 0.3657
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 7] pred max: 0.8334, mean: 0.7179, loss: 0.3394
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 8] pred max: 0.8334, mean: 0.7179, loss: 0.3258
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 9] pred max: 0.8334, mean: 0.7179, loss: 0.3635
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 10] pred max: 0.8334, mean: 0.7179, loss: 0.3663
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 11] pred max: 0.8334, mean: 0.7179, loss: 0.3601
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 12] pred max: 0.8334, mean: 0.7179, loss: 0.3598
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 13] pred max: 0.8334, mean: 0.7179, loss: 0.3698
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 14] pred max: 0.8334, mean: 0.7179, loss: 0.3592
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 15] pred max: 0.8334, mean: 0.7179, loss: 0.3690
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 16] pred max: 0.8334, mean: 0.7179, loss: 0.3647
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 17] pred max: 0.8334, mean: 0.7179, loss: 0.3643
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 18] pred max: 0.8334, mean: 0.7179, loss: 0.3417
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 19] pred max: 0.8334, mean: 0.7179, loss: 0.3303
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 20] pred max: 0.8334, mean: 0.7179, loss: 0.3556
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 21] pred max: 0.8334, mean: 0.7179, loss: 0.3673
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 22] pred max: 0.8334, mean: 0.7179, loss: 0.3564
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 23] pred max: 0.8334, mean: 0.7179, loss: 0.3673
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 24] pred max: 0.8334, mean: 0.7179, loss: 0.3614
z_aff ratio: 0.9688
y_aff ratio: 0.9938
x_aff ratio: 0.9938
✅ Saved 25 instance segmentations, raw patches, and GT patches to ./results_affinity3d1
✅ Average test loss: 0.3569


[Patch 0] pred max: 0.8334, mean: 0.7179, loss: 0.3466
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 1] pred max: 0.8334, mean: 0.7179, loss: 0.3445
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 2] pred max: 0.8334, mean: 0.7179, loss: 0.3521
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 3] pred max: 0.8334, mean: 0.7179, loss: 0.3565
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 4] pred max: 0.8334, mean: 0.7179, loss: 0.3605
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 5] pred max: 0.8334, mean: 0.7179, loss: 0.3745
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 6] pred max: 0.8334, mean: 0.7179, loss: 0.3657
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 7] pred max: 0.8334, mean: 0.7179, loss: 0.3394
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 8] pred max: 0.8334, mean: 0.7179, loss: 0.3258
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 9] pred max: 0.8334, mean: 0.7179, loss: 0.3635
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 10] pred max: 0.8334, mean: 0.7179, loss: 0.3663
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 11] pred max: 0.8334, mean: 0.7179, loss: 0.3601
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 12] pred max: 0.8334, mean: 0.7179, loss: 0.3598
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 13] pred max: 0.8334, mean: 0.7179, loss: 0.3698
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 14] pred max: 0.8334, mean: 0.7179, loss: 0.3592
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 15] pred max: 0.8334, mean: 0.7179, loss: 0.3690
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 16] pred max: 0.8334, mean: 0.7179, loss: 0.3647
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 17] pred max: 0.8334, mean: 0.7179, loss: 0.3643
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 18] pred max: 0.8334, mean: 0.7179, loss: 0.3417
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 19] pred max: 0.8334, mean: 0.7179, loss: 0.3303
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 20] pred max: 0.8334, mean: 0.7179, loss: 0.3556
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 21] pred max: 0.8334, mean: 0.7179, loss: 0.3673
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 22] pred max: 0.8334, mean: 0.7179, loss: 0.3564
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 23] pred max: 0.8334, mean: 0.7179, loss: 0.3673
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
[Patch 24] pred max: 0.8334, mean: 0.7179, loss: 0.3614
z_aff ratio: 0.0000
y_aff ratio: 0.9938
x_aff ratio: 0.9938
✅ Saved 25 instance segmentations, raw patches, and GT patches to ./results_affinity3d1
✅ Average test loss: 0.3569



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Some weights of ViTMAEForPreTraining were not initialized from the model checkpoint at /home/guilin/PycharmProjects/MAE3d/output/vitmae3d and are newly initialized because the shapes did not match:
- decoder.decoder_pos_embed: found shape torch.Size([1, 801, 384]) in the checkpoint and torch.Size([1, 201, 384]) in the model instantiated
- vit.embeddings.position_embeddings: found shape torch.Size([1, 801, 768]) in the checkpoint and torch.Size([1, 201, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/300 [Train]:  48%|████▊     | 180/372 [00:25<00:26,  7.16it/s]



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 60, in <module>
    model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 48, in __init__
    self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 272, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4480, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4930, in _load_pretrained_model
    model_to_load.load_state_dict(state_dict, strict=False, assign=assign_params)
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for ViTMAEForPreTraining:
	size mismatch for vit.embeddings.position_embeddings: copying a param with shape torch.Size([1, 801, 768]) from checkpoint, the shape in current model is torch.Size([1, 201, 768]).
	size mismatch for decoder.decoder_pos_embed: copying a param with shape torch.Size([1, 801, 384]) from checkpoint, the shape in current model is torch.Size([1, 201, 384]).

Process finished with exit code 1



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/cremi_dataset.py 
📊 比较多种 patch size 下的亲和图连接比例 (sample_C_20160501.hdf, 每种前 50 个 patch)...

🧩 Patch size: (32, 320, 320)
  ➜ 方向 z+: 连接比例 = 0.7930 (129927568/163840000)
  ➜ 方向 y+: 连接比例 = 0.9807 (160676523/163840000)
  ➜ 方向 x+: 连接比例 = 0.9823 (160944973/163840000)
------------------------------------------------------------
🧩 Patch size: (32, 160, 160)
  ➜ 方向 z+: 连接比例 = 0.7656 (31358967/40960000)
  ➜ 方向 y+: 连接比例 = 0.9770 (40016375/40960000)
  ➜ 方向 x+: 连接比例 = 0.9792 (40108912/40960000)
------------------------------------------------------------
🧩 Patch size: (32, 96, 96)
  ➜ 方向 z+: 连接比例 = 0.7688 (11336733/14745600)
  ➜ 方向 y+: 连接比例 = 0.9735 (14354359/14745600)
  ➜ 方向 x+: 连接比例 = 0.9750 (14376458/14745600)
------------------------------------------------------------
🧩 Patch size: (32, 64, 64)
  ➜ 方向 z+: 连接比例 = 0.7740 (5072734/6553600)
  ➜ 方向 y+: 连接比例 = 0.9691 (6350900/6553600)
  ➜ 方向 x+: 连接比例 = 0.9696 (6354457/6553600)
------------------------------------------------------------
🧩 Patch size: (64, 160, 160)
  ➜ 方向 z+: 连接比例 = 0.8156 (66813111/81920000)
  ➜ 方向 y+: 连接比例 = 0.9776 (80088706/81920000)
  ➜ 方向 x+: 连接比例 = 0.9794 (80235413/81920000)
------------------------------------------------------------

Process finished with exit code 0



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/cremi_dataset.py 
📊 正在统计 sample_C_20160501.hdf 中前 64 个 patch 的亲和图连接比例...
➡️ 方向 z+: 连接比例 = 0.7977 (167292814/209715200)
➡️ 方向 y+: 连接比例 = 0.9802 (205566076/209715200)
➡️ 方向 x+: 连接比例 = 0.9820 (205949488/209715200)

Process finished with exit code 0


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
[Patch 0] pred max: 0.8565, mean: 0.7426, loss: 0.3449
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
[Patch 1] pred max: 0.8575, mean: 0.7711, loss: 0.2875
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
[Patch 2] pred max: 0.8533, mean: 0.7607, loss: 0.3135
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
[Patch 3] pred max: 0.8531, mean: 0.7624, loss: 0.3038
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
[Patch 4] pred max: 0.8581, mean: 0.7507, loss: 0.3274
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
[Patch 5] pred max: 0.8580, mean: 0.7524, loss: 0.3255
z_aff ratio: 0.0000
y_aff ratio: 0.0000
x_aff ratio: 0.0000
✅ Saved 6 instance segmentations, raw patches, and GT patches to ./results_affinity3d
✅ Average test loss: 0.3171



raw_np = raw[0, 0].detach().cpu().numpy() if isinstance(raw, torch.Tensor) else raw[0, 0]



Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 105, in <module>
    raw_np = raw_tensor.detach().cpu().numpy()  # 转成 numpy
             ^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'detach'

Process finished with exit code 1


# === 还原图像（反标准化）
raw_tensor = raw[0, 0]  # Tensor 类型 [1, D, H, W]
raw_np = raw_tensor.detach().cpu().numpy()  # 转成 numpy
raw_restore = (raw_np * STD + MEAN) * 255.0
raw_uint8 = raw_restore.clip(0, 255).astype(np.uint8)

tifffile.imwrite(os.path.join(SAVE_DIR, f"raw_patch{i}.tif"), raw_uint8)


Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 103, in <module>
    raw_np = raw[0, 0].numpy()
             ^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'numpy'. Did you mean: 'dump'?


raw_np = raw[0, 0].numpy()
raw_restore = (raw_np * STD + MEAN) * 255.0
raw_uint8 = raw_restore.clip(0, 255).astype(np.uint8)

tifffile.imwrite(os.path.join(SAVE_DIR, f"raw_patch{i}.tif"), raw_uint8)


tifffile.imwrite(f"debug_prob_z_aff_patch{i}.tif", pred[0].astype(np.float32))  # 不加阈值

/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
[Patch 0] pred max: 0.8588, mean: 0.7427, loss: 0.3448
z_aff ratio: 0.9687
y_aff ratio: 0.9969
x_aff ratio: 0.9969
[Patch 1] pred max: 0.8579, mean: 0.7712, loss: 0.2875
z_aff ratio: 0.9688
y_aff ratio: 0.9969
x_aff ratio: 0.9969
[Patch 2] pred max: 0.8536, mean: 0.7607, loss: 0.3134
z_aff ratio: 0.9688
y_aff ratio: 0.9969
x_aff ratio: 0.9969
[Patch 3] pred max: 0.8539, mean: 0.7624, loss: 0.3038
z_aff ratio: 0.9688
y_aff ratio: 0.9969
x_aff ratio: 0.9969
[Patch 4] pred max: 0.8589, mean: 0.7507, loss: 0.3274
z_aff ratio: 0.9687
y_aff ratio: 0.9969
x_aff ratio: 0.9969
[Patch 5] pred max: 0.8572, mean: 0.7525, loss: 0.3254
z_aff ratio: 0.9688
y_aff ratio: 0.9969
x_aff ratio: 0.9969
✅ Saved 6 instance segmentations, raw patches, and GT patches to ./results_affinity3d
✅ Average test loss: 0.3171

Process finished with exit code 0


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))
z_aff ratio: 0.9687
y_aff ratio: 0.9969
x_aff ratio: 0.9969
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 86, in <module>
    aff_mask[:-1] &= z_aff[:-1]      # z+
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'bitwise_and' output from dtype('uint8') to dtype('bool') with casting rule 'same_kind'

Process finished with exit code 1


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
[Patch 0] pred max: 0.8751, mean: 0.7398, loss: 0.3473
z_aff ratio: 0.0000
y_aff ratio: 0.6068
x_aff ratio: 0.9967
[Patch 1] pred max: 0.8799, mean: 0.7769, loss: 0.2806
z_aff ratio: 0.0000
y_aff ratio: 0.9966
x_aff ratio: 0.9969
[Patch 2] pred max: 0.8686, mean: 0.7608, loss: 0.3129
z_aff ratio: 0.0000
y_aff ratio: 0.9967
x_aff ratio: 0.9969
[Patch 3] pred max: 0.8813, mean: 0.7688, loss: 0.2952
z_aff ratio: 0.0000
y_aff ratio: 0.9965
x_aff ratio: 0.9969
[Patch 4] pred max: 0.8723, mean: 0.7482, loss: 0.3293
z_aff ratio: 0.0000
y_aff ratio: 0.7090
x_aff ratio: 0.9862
[Patch 5] pred max: 0.8553, mean: 0.7539, loss: 0.3240
z_aff ratio: 0.0000
y_aff ratio: 0.9944
x_aff ratio: 0.9969
✅ Saved 6 instance segmentations, raw patches, and GT patches to ./results_affinity3d
✅ Average test loss: 0.3149


print(f"z_aff ratio: {z_aff.sum() / z_aff.size:.4f}")
print(f"y_aff ratio: {y_aff.sum() / y_aff.size:.4f}")
print(f"x_aff ratio: {x_aff.sum() / x_aff.size:.4f}")


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
[Patch 0] pred max: 0.8547, mean: 0.7405, loss: 0.3469
[Patch 1] pred max: 0.8465, mean: 0.7675, loss: 0.2922
[Patch 2] pred max: 0.8497, mean: 0.7591, loss: 0.3150
[Patch 3] pred max: 0.8496, mean: 0.7603, loss: 0.3057
[Patch 4] pred max: 0.8570, mean: 0.7489, loss: 0.3294
[Patch 5] pred max: 0.8557, mean: 0.7507, loss: 0.3274
✅ Saved 6 instance segmentations, raw patches, and GT patches to ./results_affinity3d
✅ Average test loss: 0.3194

Process finished with exit code 0



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
[Patch 0] pred max: 0.8547, mean: 0.7404, loss: 0.3470
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py", line 77, in <module>
    tifffile.imwrite(os.path.join(SAVE_DIR, f"debug_z_aff_patch{i}.tif"), z_aff)
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/tifffile/tifffile.py", line 1415, in imwrite
    with TiffWriter(
         ^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/tifffile/tifffile.py", line 1743, in __init__
    self._fh = FileHandle(file, mode=mode, size=0)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/tifffile/tifffile.py", line 14695, in __init__
    self.open()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/tifffile/tifffile.py", line 14714, in open
    self._fh = open(self._file, self._mode, encoding=None)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/guilin/PycharmProjects/MAE3d/results_affinity3d/debug_z_aff_patch0.tif'


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py 
yes
/home/guilin/PycharmProjects/MAE3d/inference_affinity_3d.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))


import sys; print('Python %s on %s' % (sys.version, sys.platform))
/home/guilin/miniconda3/envs/pt12/bin/python -X pycache_prefix=/home/guilin/.cache/JetBrains/PyCharmCE2024.3/cpython-cache /snap/pycharm-community/465/plugins/python-ce/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 32957 --file /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
Connected to pydev debugger (build 243.26053.29)
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Input shape: torch.Size([2, 1, 32, 320, 320])
[DEBUG] Config image_size: (32, 320, 320), patch_size: [16, 16, 16]
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Patch grid D×H×W = 2×20×20 → expected patches: 800, got: 800
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/snap/pycharm-community/465/plugins/python-ce/helpers/pydev/pydevd.py", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/snap/pycharm-community/465/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 88, in <module>
    loss = criterion(outputs, targets)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 36, in forward
    bce_loss = self.bce(input, target)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/loss.py", line 819, in forward
    return F.binary_cross_entropy_with_logits(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/functional.py", line 3624, in binary_cross_entropy_with_logits
    raise ValueError(
ValueError: Target size (torch.Size([2, 3, 32, 320, 320])) must be the same as input size (torch.Size([2, 3, 8, 80, 80]))
python-BaseException




yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]crop shape: crop shape: (32, 320, 320)(32, 320, 320)

crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Input shape: torch.Size([2, 1, 32, 320, 320])
[DEBUG] Config image_size: [32, 320, 320], patch_size: [16, 16, 16]
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Patch grid D×H×W = 2×20×20 → expected patches: 800, got: 200
⚠️ Position embeddings mismatched — reinitializing...
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 64, in forward
    raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")
ValueError: Mismatch: input patch count 200 does not match expected 800 from image_size [32, 320, 320] and patch_size [16, 16, 16]

Process finished with exit code 1




/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Input shape: torch.Size([2, 1, 32, 320, 320])
[DEBUG] Config image_size: [32, 320, 320], patch_size: [16, 16, 16]
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
[DEBUG] Patch grid D×H×W = 2×20×20 → expected patches: 800, got: 200
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 57, in forward
    raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")
ValueError: Mismatch: input patch count 200 does not match expected 800 from image_size [32, 320, 320] and patch_size [16, 16, 16]

Process finished with exit code 1




/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]crop shape: (32, 320, 320)
crop shape:crop shape:  (32, 320, 320)(32, 320, 320)

crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
crop shape: (32, 320, 320)
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

Process finished with exit code 1




/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

Process finished with exit code 1




报错：/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_affinity_3d.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

# train_affinity_3d.py：
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_affinity_3d_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 320, 320)
BATCH_SIZE = 2
NUM_EPOCHS = 100
NUM_CLASSES = 3  # z+, y+, x+ affinity
LR = 1e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return (focal_weight * bce_loss).mean()

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()

# mae3d_unet_finetune.py：
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit

        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)

# cremi_affinity_3d_dataset.py：
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
import random

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 320, 320), mean=127.91 / 255, std=28 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.D, self.H, self.W = self.raw.shape

    def __len__(self):
        return 10000  # number of random crops per epoch

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        zd, yh, xw = self.crop_size
        z = random.randint(0, self.D - zd)
        y = random.randint(0, self.H - yh)
        x = random.randint(0, self.W - xw)

        raw_crop = self.raw[z:z+zd, y:y+yh, x:x+xw].astype(np.float32)
        label_crop = self.labels[z:z+zd, y:y+yh, x:x+xw].astype(np.int64)

        # 归一化 + 标准化
        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 51, in forward
    assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"
           ^^^^^^^^^^^^^^
AssertionError: Mismatch: got 200 patches but expected 2x20x20

Process finished with exit code 1


/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 48, in forward
    x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: shape '[2, 768, 2, 20, 20]' is invalid for input of size 307200



/home/guilin/miniconda3/envs/pt12/bin/python /home/guilin/PycharmProjects/MAE3d/train_segmentation.py 
yes
Epoch 1/100 [Train]:   0%|          | 0/14500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/train_segmentation.py", line 85, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/mae3d_unet_finetune.py", line 45, in forward
    features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 823, in forward
    embedding_output, mask, ids_restore = self.embeddings(
                                          ^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 322, in forward
    embeddings = embeddings + position_embeddings[:, 1:, :]
                 ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (200) must match the size of tensor b (800) at non-singleton dimension 1




final_reconstruction[mask_cpu] = reconstructed[mask_cpu]
                                     ~~~~~~~~~~~~~^^^^^^^^^^
IndexError: boolean index did not match indexed array along dimension 0; dimension is 32 but corresponding boolean dimension is 800
(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1780000
yes
Reconstruction loss: 0.4297
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 63, in <module>
    final_reconstruction[mask_cpu] = reconstructed[mask_cpu]
                                     ~~~~~~~~~~~~~^^^^^^^^^^
IndexError: boolean index did not match indexed array along dimension 0; dimension is 32 but corresponding boolean dimension is 800




(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1670000
yes
Reconstruction loss: 0.4615
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 58, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1129, in unpatchify
    patchified_pixel_values = torch.einsum("ndhwpqc->ncdhwpq", patchified_pixel_values)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (7) does not match the number of dimensions (8) for operand 0 and no ellipsis was given


(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-1670000
yes
Reconstruction loss: 0.4652
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 58, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1112, in unpatchify
    num_patches_d = original_depth // pd
                    ~~~~~~~~~~~~~~~^^~~~
TypeError: unsupported operand type(s) for //: 'tuple' and 'int'



(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-50000
yes
Reconstruction loss: 0.6117
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 57, in <module>
    masked_volume = model.unpatchify(masked_patchified)[0, 0].cpu().numpy() * STD + MEAN
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1112, in unpatchify
    num_patches_d = original_depth // pd
                    ~~~~~~~~~~~~~~~^^~~~
TypeError: unsupported operand type(s) for //: 'list' and 'int'



(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/MAE3d$ python inference.py
Loading model from: /home/guilin/PycharmProjects/MAE3d/output/vitmae3d/checkpoint-50000
yes
Reconstruction loss: 0.6272
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/inference.py", line 53, in <module>
    masked_patchified[0][mask.bool()] = 0  # 将被 mask 的 patch 置为 0
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
IndexError: The shape of the mask [1, 800] at index 0 does not match the shape of the indexed tensor [800, 4096] at index 0


Traceback (most recent call last):                                                                                                                                                                                         
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3162, in _determine_best_metric
    metric_value = metrics[metric_to_check]
                   ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'eval_accuracy'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 216, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2620, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3094, in _maybe_log_save_evaluate
    is_new_best_metric = self._determine_best_metric(metrics=metrics, trial=trial)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3164, in _determine_best_metric
    raise KeyError(
KeyError: "The `metric_for_best_model` training argument is set to 'eval_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Consider changing the `metric_for_best_model` via the TrainingArguments."




Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 197, in main
    f.write(f"{name:60s} | shape: {tuple(param.shape):20s} | requires_grad={param.requires_grad}\n")
                                  ^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: unsupported format string passed to tuple.__format__


warnings.warn(
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 225, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 133, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/hf_argparser.py", line 358, in parse_args_into_dataclasses
    obj = dtype(**inputs)
          ^^^^^^^^^^^^^^^
  File "<string>", line 136, in __init__
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/training_args.py", line 1678, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: IntervalStrategy.STEPS
- Save strategy: SaveStrategy.EPOCH


tensorboard --logdir ./output_dir/tb_samples


AssertionError: size of input tensor and input format are different.         tensor shape: (1, 320), input_format: CHW



0%|                                                                                                                                                                            | 499/4176700 [01:03<148:04:00,  7.83it/s](pt12) guilin@guilin-System-Product-Name:~/data_proccess$ python unzip.py ./EM_pretrain_data
Extracting FAFB_crop_hdf_4.zip to ./EM_pretrain_data/FAFB_crop_hdf_4...
Extracting Kasthuri2015_hdf_5.zip to ./EM_pretrain_data/Kasthuri2015_hdf_5...
Extracting Kasthuri2015_hdf_9.zip to ./EM_pretrain_data/Kasthuri2015_hdf_9...
Extracting Kasthuri2015_hdf_8.zip to ./EM_pretrain_data/Kasthuri2015_hdf_8...
Extracting Kasthuri2015_hdf_2.zip to ./EM_pretrain_data/Kasthuri2015_hdf_2...
Extracting FIB-25_hdf_6.zip to ./EM_pretrain_data/FIB-25_hdf_6...
Extracting Kasthuri2015_hdf_3.zip to ./EM_pretrain_data/Kasthuri2015_hdf_3...
Traceback (most recent call last):
  File "/home/guilin/data_proccess/unzip.py", line 52, in <module>
    extract_all_in_dir(args.dir)
  File "/home/guilin/data_proccess/unzip.py", line 44, in extract_all_in_dir
    extract_archive(fpath, out_dir)
  File "/home/guilin/data_proccess/unzip.py", line 9, in extract_archive
    with zipfile.ZipFile(archive_path, 'r') as zip_ref:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1349, in __init__
    self._RealGetContents()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/zipfile/__init__.py", line 1416, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
{'loss': 0.9422, 'grad_norm': 0.2779630422592163, 'learning_rate': 3.906246120865152e-06, 'epoch': 0.0}                                                                                                                    
{'eval_runtime': 380.9952, 'eval_samples_per_second': 29.368, 'eval_steps_per_second': 29.368, 'epoch': 0.0}                                                                                                               
{'loss': 0.9468, 'grad_norm': 0.21853122115135193, 'learning_rate': 3.906242241730305e-06, 'epoch': 0.0}                                                                                                                   
{'eval_runtime': 388.9377, 'eval_samples_per_second': 28.768, 'eval_steps_per_second': 28.768, 'epoch': 0.0}                                                                                                               
{'loss': 1.018, 'grad_norm': 0.20022796094417572, 'learning_rate': 3.906238362595458e-06, 'epoch': 0.0}                                                                                                                    
  0%|                                                                                                                                                                          | 30/10069900 [13:10<51206:24:19, 18.31s/it]
 36%|███████████████████████████████████████████████████████████████▊                                                                                                                 | 4037/11189 [02:15<04:14, 28.07it/s]



FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:__main__:Loading data...
yes
INFO:__main__:Start training...
  0%|                                                                                                                                                                                         | 0/10069900 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 144, in <module>
    main()
  File "/home/guilin/PycharmProjects/MAE3d/run_mae_3d.py", line 135, in main
    trainer.train()
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3718, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/trainer.py", line 3783, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1216, in forward
    loss = self.forward_loss(pixel_values, logits, mask, interpolate_pos_encoding=interpolate_pos_encoding)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1152, in forward_loss
    target = self.patchify(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/PycharmProjects/MAE3d/vitmae3d.py", line 1084, in patchify
    patchified_pixel_values = torch.einsum("ncdhwpq->ndhwpqc", patchified_pixel_values)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (7) does not match the number of dimensions (8) for operand 0 and no ellipsis was given
  0%| 
