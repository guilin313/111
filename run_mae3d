python run_mae_3d.py \
  --train_dir ./data/train \
  --val_dir ./data/val \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \
  --num_train_epochs 100 \
  --output_dir ./output/vitmae3d \
  --logging_dir ./output/vitmae3d/logs \
  --base_learning_rate 1e-3 \
  --logging_steps 10 \
  --save_strategy="epoch" \           # <<< 改这里
  --evaluation_strategy="epoch" \     # <<< 改这里
  --save_total_limit=3



import os
import sys
import logging
import random
import torch
import json
import tifffile
import numpy as np
from torch.utils.data import Dataset
from torchvision.transforms import Compose
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import HfArgumentParser, Trainer, TrainingArguments
from transformers.trainer_utils import EvalPrediction
from vitmae3d import ViTMAEForPreTraining, ViTMAEConfig
from torch.utils.tensorboard import SummaryWriter

logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    config_path: Optional[str] = field(default=None)
    model_path: Optional[str] = field(default=None)
    mask_ratio: float = field(default=0.75)
    norm_pix_loss: bool = field(default=True)

@dataclass
class DataTrainingArguments:
    train_dir: str = field(metadata={"help": "Directory containing .tif training volumes."})
    val_dir: str = field(metadata={"help": "Directory containing .tif validation volumes."})
    max_train_samples: Optional[int] = field(default=None)
    max_eval_samples: Optional[int] = field(default=None)

@dataclass
class CustomTrainingArguments(TrainingArguments):
    base_learning_rate: float = field(
        default=1e-3, metadata={"help": "Absolute LR = base_lr * batch_size / 256"}
    )

def log_dataset_samples(dataset, writer, tag_prefix="train", num_samples=5):
    for idx in range(min(num_samples, len(dataset))):
        data = dataset[idx]["pixel_values"]  # shape: [1, D, H, W]
        image = data[data.shape[0] // 2]   # middle slice [H, W]
        writer.add_image(f"{tag_prefix}_slice_{idx}", image.unsqueeze(0), global_step=0)

class VolumeDataset(Dataset):
    def __init__(self, file_list, transform=None):
        self.file_list = file_list
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        volume = tifffile.imread(self.file_list[idx]).astype(np.float32)
        volume = torch.tensor(volume)  # [D, H, W]
        if self.transform:
            volume = self.transform(volume)
        return {"pixel_values": volume}

class RandomCrop3D:
    def __init__(self, size):
        self.size = size  # (D, H, W)

    def __call__(self, vol):
        d, h, w = vol.shape
        zd, yd, xd = self.size
        z = random.randint(0, d - zd)
        y = random.randint(0, h - yd)
        x = random.randint(0, w - xd)
        return vol[z:z+zd, y:y+yd, x:x+xd]

class Normalize3D:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, vol):
        return (vol / 255.0 - self.mean) / self.std

def collate_fn(examples):
    batch = torch.stack([ex["pixel_values"] for ex in examples])
    return {"pixel_values": batch.unsqueeze(1)}  # [B, 1, D, H, W]

def gather_tif_files(root_dir):
    files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for f in filenames:
            if f.endswith(".tif"):
                files.append(os.path.join(dirpath, f))
    return files

# ==== 这里是新加的 compute_metrics 函数 ====
def compute_mae_metrics(eval_pred: EvalPrediction):
    logits, labels = eval_pred.predictions, eval_pred.label_ids
    if isinstance(logits, tuple):  # some models return a tuple
        logits = logits[0]
    loss = ((logits - labels) ** 2).mean()
    return {"eval_loss": loss.item()}
# ==========================================

def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(level=logging.INFO)
    logger.info("Loading data...")

    train_files = gather_tif_files(data_args.train_dir)
    val_files = gather_tif_files(data_args.val_dir)

    if data_args.max_train_samples:
        train_files = train_files[:data_args.max_train_samples]
    if data_args.max_eval_samples:
        val_files = val_files[:data_args.max_eval_samples]

    transform = Compose([
        RandomCrop3D((32, 320, 320)),
        Normalize3D(143.510583/255, 45.286453/255),
    ])

    train_dataset = VolumeDataset(train_files, transform)
    val_dataset = VolumeDataset(val_files, transform)

    writer = SummaryWriter(log_dir=training_args.logging_dir)
    log_dataset_samples(train_dataset, writer, tag_prefix="train", num_samples=5)
    log_dataset_samples(val_dataset, writer, tag_prefix="val", num_samples=3)
    writer.close()
    logger.info("✅ Wrote sample slices to TensorBoard.")

    config = ViTMAEConfig(
        image_size=(32, 320, 320),
        patch_size=(16, 16, 16),
        num_channels=1,
        hidden_size=768,
        num_hidden_layers=10,
        num_attention_heads=8,
        intermediate_size=3072,
        decoder_hidden_size=384,
        decoder_num_hidden_layers=6,
        decoder_num_attention_heads=4,
        decoder_intermediate_size=1536,
        mask_ratio=model_args.mask_ratio,
        norm_pix_loss=model_args.norm_pix_loss,
    )

    if model_args.model_path:
        model = ViTMAEForPreTraining.from_pretrained(model_args.model_path, config=config)
    else:
        model = ViTMAEForPreTraining(config)

    total_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
    training_args.learning_rate = training_args.base_learning_rate * total_batch_size / 256

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
        compute_metrics=compute_mae_metrics,  # <<< 加了这行！
    )

    logger.info("Start training...")
    trainer.train()
    trainer.save_model()

    logger.info("Evaluating...")
    metrics = trainer.evaluate()
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)
    trainer.save_state()

if __name__ == "__main__":
    main()




{
      "epoch": 25.930445393532644,
      "eval_runtime": 234.9197,
      "eval_samples_per_second": 37.149,
      "eval_steps_per_second": 9.288,
      "step": 510000
    },



python run_mae_3d.py \
  --train_dir ./data/train \
  --val_dir ./data/val \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \
  --num_train_epochs 100 \
  --output_dir ./output/vitmae3d \
  --logging_dir ./output/vitmae3d/logs \
  --base_learning_rate 1e-3 \
  --logging_steps 10 \
  --save_strategy="steps" \
  --save_steps=10000 \
  --eval_steps 10000 \
  --evaluation_strategy steps \
  --save_total_limit 3


class ViTMAEForPreTraining(ViTMAEPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.vit = ViTMAEModel(config)
        self.decoder = ViTMAEDecoder(config, num_patches=self.vit.embeddings.num_patches)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.vit.embeddings.patch_embeddings

    def _prune_heads(self, heads_to_prune):
        """
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
        class PreTrainedModel
        """
        for layer, heads in heads_to_prune.items():
            self.encoder.layer[layer].attention.prune_heads(heads)

    def patchify(self, pixel_values, interpolate_pos_encoding: bool = False):
        """
        Args:
            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
                Pixel values.
            interpolate_pos_encoding (`bool`, *optional*, default `False`):
                interpolation flag passed during the forward pass.

        Returns:
            `torch.FloatTensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:
                Patchified pixel values.
        """
        patch_size, num_channels = self.config.patch_size, self.config.num_channels
        # sanity checks
        pd,ph,pw = self.config.patch_size
        if not interpolate_pos_encoding and (
            pixel_values.shape[2] %pd !=0 or
            pixel_values.shape[3] %ph !=0 or
            pixel_values.shape[4] %pw !=0
        ):
            raise ValueError("Input size must be divisible by the patch size")
        if pixel_values.shape[1] != num_channels:
            raise ValueError(
                "Make sure the number of channels of the pixel values is equal to the one set in the configuration"
            )

        # patchify
        batch_size = pixel_values.shape[0]
        num_patches_d = pixel_values.shape[2] // pd
        num_patches_h = pixel_values.shape[3] // ph
        num_patches_w = pixel_values.shape[4] // pw

        patchified_pixel_values = pixel_values.reshape(
            batch_size, num_channels,
            num_patches_d,pd,
            num_patches_h, ph,
            num_patches_w, pw
        )
        patch_volume = patch_size[0] * patch_size[1] * patch_size[2]
        output_dim = patch_volume * num_channels
        patchified_pixel_values = torch.einsum("ncdphqkw->ndhkpqwc", patchified_pixel_values)
        patchified_pixel_values = patchified_pixel_values.reshape(
            batch_size, num_patches_d * num_patches_h * num_patches_w, output_dim
        )
        return patchified_pixel_values

    def unpatchify(self, patchified_pixel_values, original_image_size: Optional[Tuple[int, int]] = None):
        """
        Args:
            patchified_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:
                Patchified pixel values.
            original_image_size (`Tuple[int, int]`, *optional*):
                Original image size.

        Returns:
            `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`:
                Pixel values.
        """
        patch_size, num_channels = self.config.patch_size, self.config.num_channels
        original_image_size = (
            original_image_size
            if original_image_size is not None
            else tuple(self.config.image_size)
        )
        pd, ph, pw = self.config.patch_size
        original_depth, original_height, original_width = original_image_size
        num_patches_d = original_depth // pd
        num_patches_h = original_height // ph
        num_patches_w = original_width // pw
        # sanity check
        if num_patches_d * num_patches_h * num_patches_w != patchified_pixel_values.shape[1]:
            raise ValueError(
                f"The number of patches in the patchified pixel values {patchified_pixel_values.shape[1]}, does not match the number of patches on original image {num_patches_h}*{num_patches_w}"
            )

        # unpatchify
        batch_size = patchified_pixel_values.shape[0]
        patchified_pixel_values = patchified_pixel_values.reshape(
            batch_size,
            num_patches_d,num_patches_h,num_patches_w,
            pd,ph,pw,
            num_channels,
        )
        patchified_pixel_values = torch.einsum("ndhkpqwc->ncdphqkw", patchified_pixel_values)

        pixel_values = patchified_pixel_values.reshape(
            batch_size,
            num_channels,
            num_patches_d * pd,
            num_patches_h * ph,
            num_patches_w * pw,
        )
        return pixel_values

    def forward_loss(self, pixel_values, pred, mask, interpolate_pos_encoding: bool = False):
        """
        Args:
            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
                Pixel values.
            pred (`torch.FloatTensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:
                Predicted pixel values.
            mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
                Tensor indicating which patches are masked (1) and which are not (0).
            interpolate_pos_encoding (`bool`, *optional*, default `False`):
                interpolation flag passed during the forward pass.

        Returns:
            `torch.FloatTensor`: Pixel reconstruction loss.
        """
        target = self.patchify(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)
        if self.config.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.0e-6) ** 0.5

        loss = (pred - target) ** 2
        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
        return loss

    @add_start_docstrings_to_model_forward(VIT_MAE_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=ViTMAEForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        noise: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
    ) -> Union[Tuple, ViTMAEForPreTrainingOutput]:
        r"""
        Returns:

        Examples:

        ```python
        >>> from transformers import AutoImageProcessor, ViTMAEForPreTraining
        >>> from PIL import Image
        >>> import requests

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
        >>> model = ViTMAEForPreTraining.from_pretrained("facebook/vit-mae-base")

        >>> inputs = image_processor(images=image, return_tensors="pt")
        >>> outputs = model(**inputs)
        >>> loss = outputs.loss
        >>> mask = outputs.mask
        >>> ids_restore = outputs.ids_restore
        ```"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.vit(
            pixel_values,
            noise=noise,
            head_mask=head_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            interpolate_pos_encoding=interpolate_pos_encoding,
        )

        latent = outputs.last_hidden_state
        ids_restore = outputs.ids_restore
        mask = outputs.mask

        decoder_outputs = self.decoder(latent, ids_restore, interpolate_pos_encoding=interpolate_pos_encoding)
        logits = decoder_outputs.logits  # shape (batch_size, num_patches, patch_size*patch_size*num_channels)

        loss = self.forward_loss(pixel_values, logits, mask, interpolate_pos_encoding=interpolate_pos_encoding)

        if not return_dict:
            output = (logits, mask, ids_restore) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return ViTMAEForPreTrainingOutput(
            loss=loss,
            logits=logits,
            mask=mask,
            ids_restore=ids_restore,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )





#!/usr/bin/env python
# coding=utf-8

import os
import sys
import logging
import random
import torch
import json
import tifffile
import numpy as np
from torch.utils.data import Dataset
from torchvision.transforms import Compose
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import HfArgumentParser, Trainer, TrainingArguments
from vitmae3d import ViTMAEForPreTraining, ViTMAEConfig
from torch.utils.tensorboard import SummaryWriter

logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    config_path: Optional[str] = field(default=None)
    model_path: Optional[str] = field(default=None)
    mask_ratio: float = field(default=0.75)
    norm_pix_loss: bool = field(default=True)

@dataclass
class DataTrainingArguments:
    train_dir: str = field(metadata={"help": "Directory containing .tif training volumes."})
    val_dir: str = field(metadata={"help": "Directory containing .tif validation volumes."})
    max_train_samples: Optional[int] = field(default=None)
    max_eval_samples: Optional[int] = field(default=None)

@dataclass
class CustomTrainingArguments(TrainingArguments):
    base_learning_rate: float = field(
        default=1e-3, metadata={"help": "Absolute LR = base_lr * batch_size / 256"}
    )

def log_dataset_samples(dataset, writer, tag_prefix="train", num_samples=5):
    for idx in range(min(num_samples, len(dataset))):
        data = dataset[idx]["pixel_values"]  # shape: [1, D, H, W]
        image = data[data.shape[0] // 2]   # 取中间层 [H, W]
        writer.add_image(f"{tag_prefix}_slice_{idx}", image.unsqueeze(0), global_step=0)

class VolumeDataset(Dataset):
    def __init__(self, file_list,  transform=None):
        self.file_list = file_list
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        volume = tifffile.imread(self.file_list[idx]).astype(np.float32)

        volume = torch.tensor(volume)  # [D, H, W]
        if self.transform:
            volume = self.transform(volume)
        return {"pixel_values": volume}

class RandomCrop3D:
    def __init__(self, size):
        self.size = size  # tuple: (D, H, W)

    def __call__(self, vol):
        d, h, w = vol.shape
        zd, yd, xd = self.size
        z = random.randint(0, d - zd)
        y = random.randint(0, h - yd)
        x = random.randint(0, w - xd)
        return vol[z:z+zd, y:y+yd, x:x+xd]

class Normalize3D:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, vol):
        return (vol / 255.0 - self.mean) / self.std

def collate_fn(examples):
    batch = torch.stack([ex["pixel_values"] for ex in examples])  # [B, D, H, W]
    return {"pixel_values": batch.unsqueeze(1)}  # [B, 1, D, H, W]

def gather_tif_files(root_dir):
    files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for f in filenames:
            if f.endswith(".tif"):
                files.append(os.path.join(dirpath, f))
    return files

def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(level=logging.INFO)
    logger.info("Loading data...")

    train_files = gather_tif_files(data_args.train_dir)
    val_files = gather_tif_files(data_args.val_dir)

    if data_args.max_train_samples:
        train_files = train_files[:data_args.max_train_samples]
    if data_args.max_eval_samples:
        val_files = val_files[:data_args.max_eval_samples]

    transform = Compose([
        RandomCrop3D((32, 320, 320)),
        Normalize3D(143.510583/255, 45.286453/255),
    ])

    train_dataset = VolumeDataset(train_files, transform)
    val_dataset = VolumeDataset(val_files, transform)

    sample = train_dataset[0]["pixel_values"]
    print("train_dataset[0]shape:", sample.shape)

    writer = SummaryWriter(log_dir=os.path.join(training_args.output_dir, "tb_samples"))
    log_dataset_samples(train_dataset, writer, tag_prefix="train", num_samples=5)
    log_dataset_samples(val_dataset, writer, tag_prefix="val", num_samples=3)
    writer.close()
    logger.info("✅ Wrote sample slices to TensorBoard.")

    config = ViTMAEConfig(
        image_size=(32, 320, 320),
        patch_size=(16, 16, 16),
        num_channels=1,
        hidden_size=768,
        num_hidden_layers=10,
        num_attention_heads=8,
        intermediate_size=3072,
        decoder_hidden_size=384,
        decoder_num_hidden_layers=6,
        decoder_num_attention_heads=4,
        decoder_intermediate_size=1536,
        mask_ratio=model_args.mask_ratio,
        norm_pix_loss=model_args.norm_pix_loss
    )

    if model_args.model_path:
        model = ViTMAEForPreTraining.from_pretrained(model_args.model_path, config=config)
    else:
        model = ViTMAEForPreTraining(config)

    param_report_path = os.path.join(training_args.output_dir, "model_params.txt")
    with open(param_report_path, "w") as f:
        total_params = 0
        trainable_params = 0
        f.write("Model Parameters:\n")
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad:
                trainable_params += param_count
            f.write(f"{name:60s} | shape: {str(tuple(param.shape)):20s} | requires_grad={param.requires_grad}\n")
        total_mb = total_params * 4 / 1024 / 1024  # float32 占 4 字节
        f.write(f"\n✅ Total parameters: {total_params:,} (~{total_mb:.2f} MB)\n")
        f.write(f"🟢 Trainable parameters: {trainable_params:,}\n")
        f.write(f"🟡 Non-trainable parameters: {total_params - trainable_params:,}\n")
    logger.info(f"✅ Model parameter details saved to {param_report_path}")

    total_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
    training_args.learning_rate = training_args.base_learning_rate * total_batch_size / 256

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
    )

    logger.info("Start training...")
    trainer.train()
    trainer.save_model()

    logger.info("Evaluating...")
    metrics = trainer.evaluate()
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

if __name__ == "__main__":
    main()




#!/usr/bin/env python
# coding=utf-8

import os
import sys
import logging
import random
import torch
import json
import tifffile
import numpy as np
from torch.utils.data import Dataset
from torchvision.transforms import Compose
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import HfArgumentParser, Trainer, TrainingArguments
from vitmae3d import ViTMAEForPreTraining, ViTMAEConfig
from torch.utils.tensorboard import SummaryWriter

logger = logging.getLogger(__name__)

def compute_mean_std(root_dir, cache_file="mean_std_cache.json"):
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r") as f:
                cached = json.load(f)
            if cached.get("dir") == os.path.abspath(root_dir):
                logger.info("Loaded mean/std from cache.")
                return cached["mean"], cached["std"]
        except Exception as e:
            logger.warning(f"Failed to load mean/std cache: {e}")

    all_voxels = []
    for root, _, files in os.walk(root_dir):
        for fname in files:
            if fname.endswith(".tif"):
                fpath = os.path.join(root, fname)
                try:
                    vol = tifffile.imread(fpath).astype(np.float32)
                    all_voxels.append(vol.flatten())
                except Exception as e:
                    print(f"Warning: failed to read {fpath}: {e}")
    all_voxels = np.concatenate(all_voxels)
    mean = float(np.mean(all_voxels))
    std = float(np.std(all_voxels))

    try:
        with open(cache_file, "w") as f:
            json.dump({"dir": os.path.abspath(root_dir), "mean": mean, "std": std}, f)
    except Exception as e:
        logger.warning(f"Failed to write mean/std cache: {e}")

    return mean, std

@dataclass
class ModelArguments:
    config_path: Optional[str] = field(default=None)
    model_path: Optional[str] = field(default=None)
    mask_ratio: float = field(default=0.75)
    norm_pix_loss: bool = field(default=True)

@dataclass
class DataTrainingArguments:
    train_dir: str = field(metadata={"help": "Directory containing .tif training volumes."})
    val_dir: str = field(metadata={"help": "Directory containing .tif validation volumes."})
    max_train_samples: Optional[int] = field(default=None)
    max_eval_samples: Optional[int] = field(default=None)

@dataclass
class CustomTrainingArguments(TrainingArguments):
    base_learning_rate: float = field(
        default=1e-3, metadata={"help": "Absolute LR = base_lr * batch_size / 256"}
    )

def log_dataset_samples(dataset, writer, tag_prefix="train", num_samples=5):
    for idx in range(min(num_samples, len(dataset))):
        data = dataset[idx]["pixel_values"]  # shape: [1, D, H, W]
        image = data[data.shape[0] // 2]   # 取中间层 [H, W]
        writer.add_image(f"{tag_prefix}_slice_{idx}", image.unsqueeze(0), global_step=0)

class VolumeDataset(Dataset):
    def __init__(self, file_list,  transform=None):
        self.file_list = file_list
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        volume = tifffile.imread(self.file_list[idx]).astype(np.float32)
#        volume = volume / 255.0
#        volume = (volume - self.mean) / self.std

        volume = torch.tensor(volume)  # [D, H, W]
        if self.transform:
            volume = self.transform(volume)
        return {"pixel_values": volume}

class RandomCrop3D:
    def __init__(self, size):
        self.size = size  # tuple: (D, H, W)

    def __call__(self, vol):
        d, h, w = vol.shape
        zd, yd, xd = self.size
        z = random.randint(0, d - zd)
        y = random.randint(0, h - yd)
        x = random.randint(0, w - xd)
        return vol[z:z+zd, y:y+yd, x:x+xd]

class Normalize3D:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, vol):
        return (vol / 255.0 - self.mean) / self.std

def collate_fn(examples):
    batch = torch.stack([ex["pixel_values"] for ex in examples])  # [B, D, H, W]
    return {"pixel_values": batch.unsqueeze(1)}  # [B, 1, D, H, W]

def gather_tif_files(root_dir):
    files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for f in filenames:
            if f.endswith(".tif"):
                files.append(os.path.join(dirpath, f))
    return files

def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(level=logging.INFO)
    logger.info("Loading data...")

#    mean, std = compute_mean_std(data_args.train_dir)
 #   logger.info(f"computed mean: {mean:.4f}, std:{std:.4f}")

    train_files = gather_tif_files(data_args.train_dir)
    val_files = gather_tif_files(data_args.val_dir)

    if data_args.max_train_samples:
        train_files = train_files[:data_args.max_train_samples]
    if data_args.max_eval_samples:
        val_files = val_files[:data_args.max_eval_samples]

    transform = Compose([
        RandomCrop3D((32, 320, 320)),
        Normalize3D(143.510583/255, 45.286453/255),
    ])

    train_dataset = VolumeDataset(train_files, transform)
    val_dataset = VolumeDataset(val_files, transform)

    sample = train_dataset[0]["pixel_values"]
    print("train_dataset[0]shape:", sample.shape)

    writer = SummaryWriter(log_dir=os.path.join(training_args.output_dir, "tb_samples"))
    log_dataset_samples(train_dataset, writer, tag_prefix="train", num_samples=5)
    log_dataset_samples(val_dataset, writer, tag_prefix="val", num_samples=3)
    writer.close()
    logger.info("✅ Wrote sample slices to TensorBoard.")

    config = ViTMAEConfig(
        image_size=(32, 320, 320),
        patch_size=(16, 16, 16),
        num_channels=1,
        hidden_size=768,
        num_hidden_layers=10,
        num_attention_heads=8,
        intermediate_size=3072,
        decoder_hidden_size=384,
        decoder_num_hidden_layers=6,
        decoder_num_attention_heads=4,
        decoder_intermediate_size=1536,
        mask_ratio=model_args.mask_ratio,
        norm_pix_loss=model_args.norm_pix_loss
    )

    if model_args.model_path:
        model = ViTMAEForPreTraining.from_pretrained(model_args.model_path, config=config)
    else:
        model = ViTMAEForPreTraining(config)

    param_report_path = os.path.join(training_args.output_dir, "model_params.txt")
    with open(param_report_path, "w") as f:
        total_params = 0
        trainable_params = 0
        f.write("Model Parameters:\n")
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad:
                trainable_params += param_count
            f.write(f"{name:60s} | shape: {str(tuple(param.shape)):20s} | requires_grad={param.requires_grad}\n")
        total_mb = total_params * 4 / 1024 / 1024  # float32 占 4 字节
        f.write(f"\n✅ Total parameters: {total_params:,} (~{total_mb:.2f} MB)\n")
        f.write(f"🟢 Trainable parameters: {trainable_params:,}\n")
        f.write(f"🟡 Non-trainable parameters: {total_params - trainable_params:,}\n")
    logger.info(f"✅ Model parameter details saved to {param_report_path}")

    total_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
    training_args.learning_rate = training_args.base_learning_rate * total_batch_size / 256

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
    )

    logger.info("Start training...")
    trainer.train()
    trainer.save_model()

    logger.info("Evaluating...")
    metrics = trainer.evaluate()
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

if __name__ == "__main__":
    main()



param_report_path = os.path.join(training_args.output_dir, "model_params.txt")
    with open(param_report_path, "w") as f:
        total_params = 0
        trainable_params = 0
        f.write("Model Parameters:\n")
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad:
                trainable_params += param_count
            f.write(f"{name:60s} | shape: {tuple(param.shape):20s} | requires_grad={param.requires_grad}\n")
        total_mb = total_params * 4 / 1024 / 1024  # float32 占 4 字节
        f.write(f"\n✅ Total parameters: {total_params:,} (~{total_mb:.2f} MB)\n")
        f.write(f"🟢 Trainable parameters: {trainable_params:,}\n")
        f.write(f"🟡 Non-trainable parameters: {total_params - trainable_params:,}\n")
    logger.info(f"✅ Model parameter details saved to {param_report_path}")


param_report_path = os.path.join(training_args.output_dir, "model_params.txt")
    with open(param_report_path, "w") as f:
        total_params = 0
        trainable_params = 0
        f.write("Model Parameters:\n")
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad:
                trainable_params += param_count
            f.write(f"{name:60s} | shape: {tuple(param.shape):20s} | requires_grad={param.requires_grad}\n")
        f.write(f"\n✅ Total parameters: {total_params:,}\n")
        f.write(f"🟢 Trainable parameters: {trainable_params:,}\n")
        f.write(f"🟡 Non-trainable parameters: {total_params - trainable_params:,}\n")
    logger.info(f"✅ Model parameter details saved to {param_report_path}")




#!/usr/bin/env python
# coding=utf-8

import os
import sys
import logging
import random
import torch
import tifffile
import numpy as np
from torch.utils.data import Dataset
from torchvision.transforms import Compose
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import HfArgumentParser, Trainer, TrainingArguments
from vitmae3d import ViTMAEForPreTraining, ViTMAEConfig

logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    config_path: Optional[str] = field(default=None)
    model_path: Optional[str] = field(default=None)
    mask_ratio: float = field(default=0.75)
    norm_pix_loss: bool = field(default=True)

@dataclass
class DataTrainingArguments:
    train_dir: str = field(metadata={"help": "Directory containing .tif training volumes."})
    val_dir: str = field(metadata={"help": "Directory containing .tif validation volumes."})
    max_train_samples: Optional[int] = field(default=None)
    max_eval_samples: Optional[int] = field(default=None)

@dataclass
class CustomTrainingArguments(TrainingArguments):
    base_learning_rate: float = field(
        default=1e-3, metadata={"help": "Absolute LR = base_lr * batch_size / 256"}
    )

class VolumeDataset(Dataset):
    def __init__(self, file_list, transform=None):
        self.file_list = file_list
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        volume = tifffile.imread(self.file_list[idx]).astype(np.float32)
        volume = torch.tensor(volume)  # [D, H, W]
        if self.transform:
            volume = self.transform(volume)
        return {"pixel_values": volume}

class RandomCrop3D:
    def __init__(self, size):
        self.size = size  # tuple: (D, H, W)

    def __call__(self, vol):
        d, h, w = vol.shape
        zd, yd, xd = self.size
        z = random.randint(0, d - zd)
        y = random.randint(0, h - yd)
        x = random.randint(0, w - xd)
        return vol[z:z+zd, y:y+yd, x:x+xd]

class Normalize3D:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, vol):
        return (vol - self.mean) / self.std

def collate_fn(examples):
    batch = torch.stack([ex["pixel_values"] for ex in examples])  # [B, D, H, W]
    return {"pixel_values": batch.unsqueeze(1)}  # [B, 1, D, H, W]

def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(level=logging.INFO)
    logger.info("Loading data...")

    train_files = [os.path.join(data_args.train_dir, f) for f in os.listdir(data_args.train_dir) if f.endswith(".tif")]
    val_files = [os.path.join(data_args.val_dir, f) for f in os.listdir(data_args.val_dir) if f.endswith(".tif")]

    if data_args.max_train_samples:
        train_files = train_files[:data_args.max_train_samples]
    if data_args.max_eval_samples:
        val_files = val_files[:data_args.max_eval_samples]

    transform = Compose([
        RandomCrop3D((32, 320, 320)),
        Normalize3D(0.5, 0.2),
    ])

    train_dataset = VolumeDataset(train_files, transform)
    val_dataset = VolumeDataset(val_files, transform)

    config = ViTMAEConfig(
        image_size=(32, 320, 320),
        patch_size=(16, 16, 16),
        num_channels=1,
        hidden_size=768,
        num_hidden_layers=8,
        num_attention_heads=6,
        intermediate_size=768,
        decoder_hidden_size=384,
        decoder_num_hidden_layers=4,
        decoder_num_attention_heads=6,
        decoder_intermediate_size=1536,
        mask_ratio=model_args.mask_ratio,
        norm_pix_loss=model_args.norm_pix_loss
    )

    if model_args.model_path:
        model = ViTMAEForPreTraining.from_pretrained(model_args.model_path, config=config)
    else:
        model = ViTMAEForPreTraining(config)

    total_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
    training_args.learning_rate = training_args.base_learning_rate * total_batch_size / 256

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
    )

    logger.info("Start training...")
    trainer.train()
    trainer.save_model()

    logger.info("Evaluating...")
    metrics = trainer.evaluate()
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

if __name__ == "__main__":
    main()
