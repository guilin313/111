total = sum(p.numel() for p in model.parameters())
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total params: {total}, Trainable: {trainable}")

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 160, 160)
BATCH_SIZE = 2
NUM_EPOCHS = 300
NUM_CLASSES = 3
LR = 5e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss with no channel_weights (insert if needed) ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=0.25, channel_weights=(1.0, 1.0, 1.0)):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.register_buffer(
            'channel_weights',
            torch.tensor(channel_weights).view(1, 3, 1, 1, 1)
        )

    def forward(self, input, target):
        # input: [B, 3, D, H, W]
        # target: same shape
        bce_loss = nn.functional.binary_cross_entropy_with_logits(input, target, reduction='none')
        prob = torch.sigmoid(input)
        focal_weight = self.alpha * (1 - prob) ** self.gamma * target + \
                       (1 - self.alpha) * prob ** self.gamma * (1 - target)
        focal_loss = focal_weight * bce_loss
        return (focal_loss * self.channel_weights).mean()

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
config.image_size = CROP_SIZE
config.mask_ratio = 0.0
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(
    gamma=2.0,
    alpha=0.25,
    channel_weights=(3.0, 1.0, 1.0)  # 强调 z+ 通道
)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets, boundary in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)
        boundary = boundary.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        weight = 1.0 + 4.0 * boundary.unsqueeze(1)
        loss = criterion(outputs, targets) * weight
        loss = loss.mean()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets, boundary in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            boundary = boundary.to(DEVICE)

            outputs = model(inputs)
            weight = 1.0 + 4.0 * boundary.unsqueeze(1)
            loss = criterion(outputs, targets) * weight
            loss = loss.mean()
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d2", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d2/mae3d_unet_epoch{epoch+1}.pth")

writer.close()



import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 160, 160)
BATCH_SIZE = 2
NUM_EPOCHS = 300
NUM_CLASSES = 3
LR = 5e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss with no channel_weights (insert if needed) ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return focal_weight * bce_loss

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
config.image_size = CROP_SIZE
config.mask_ratio = 0.0
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets, boundary in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)
        boundary = boundary.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        weight = 1.0 + 4.0 * boundary.unsqueeze(1)
        loss = criterion(outputs, targets) * weight
        loss = loss.mean()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets, boundary in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            boundary = boundary.to(DEVICE)

            outputs = model(inputs)
            weight = 1.0 + 4.0 * boundary.unsqueeze(1)
            loss = criterion(outputs, targets) * weight
            loss = loss.mean()
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d1", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d1/mae3d_unet_epoch{epoch+1}.pth")

writer.close()


import torch
import torch.optim as optim
from torch.nn.parallel import DataParallel
from dataset import get_dataloader
from model import MAE3D
from config import args
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np

# 自定义 ViT 编码器 & 解码器
encoder = ...  # 你的 ViT 3D 编码器
decoder = ...  # 你的 ViT 3D 解码器

# 初始化模型
model = MAE3D(encoder, decoder, args)
model = DataParallel(model).cuda()

# 训练参数
optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epochs)
dataloader = get_dataloader("/path/to/your/tif_data", batch_size=args.batch_size)

# AMP 训练
scaler = torch.cuda.amp.GradScaler()
accumulation_steps = 4  # 梯度累积步数

# TensorBoard 记录
writer = SummaryWriter(log_dir="runs/mae3d")

# 训练循环
for epoch in range(args.num_epochs):
    model.train()
    epoch_loss = 0.0
    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch+1}/{args.num_epochs}")
    correct, total = 0, 0
    
    for i, (batch) in progress_bar:
        batch = batch.cuda()
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():  # 混合精度计算
            loss, original, recon, masked = model(batch, return_image=True)
            loss = loss / accumulation_steps  # 进行梯度累积
        
        scaler.scale(loss).backward()  # 反向传播
        
        if (i + 1) % accumulation_steps == 0:
            scaler.step(optimizer)  # 更新参数
            scaler.update()  # 更新 Scaler
            optimizer.zero_grad()
        
        epoch_loss += loss.item() * accumulation_steps  # 还原 Loss 计算
        progress_bar.set_postfix(loss=loss.item() * accumulation_steps)
        
        # 计算精度
        recon_error = torch.abs(original - recon).mean(dim=(1, 2, 3))  # 计算每个样本的重建误差
        batch_accuracy = torch.exp(-recon_error).mean().item()  # 转换为精度
        correct += batch_accuracy * batch.size(0)
        total += batch.size(0)
    
    scheduler.step()  # 学习率衰减
    avg_loss = epoch_loss / len(dataloader)
    accuracy = correct / total  # 计算平均精度
    print(f"Epoch [{epoch+1}/{args.num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
    writer.add_scalar("Loss/train", avg_loss, epoch)
    writer.add_scalar("Accuracy/train", accuracy, epoch)

# 关闭 TensorBoard 记录
writer.close()

# 保存模型
torch.save(model.state_dict(), "mae3d_model.pth")
