
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 160, 160)
BATCH_SIZE = 2
NUM_EPOCHS = 300
NUM_CLASSES = 3
LR = 5e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss with no channel_weights (insert if needed) ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return focal_weight * bce_loss

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
config.image_size = CROP_SIZE
config.mask_ratio = 0.0
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets, boundary in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)
        boundary = boundary.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        weight = 1.0 + 4.0 * boundary.unsqueeze(1)
        loss = criterion(outputs, targets) * weight
        loss = loss.mean()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets, boundary in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            boundary = boundary.to(DEVICE)

            outputs = model(inputs)
            weight = 1.0 + 4.0 * boundary.unsqueeze(1)
            loss = criterion(outputs, targets) * weight
            loss = loss.mean()
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()



# cremi_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 320, 320), mean=127.91 / 255, std=28 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.volume_shape = self.raw.shape  # (D, H, W)
        self.starts = self.compute_sliding_window_starts(self.volume_shape, self.crop_size)

    def compute_sliding_window_starts(self, volume_shape, crop_size):
        starts = []
        for size, patch in zip(volume_shape, crop_size):
            pos = list(range(0, size - patch + 1, patch))
            if (size - patch) % patch != 0:
                pos.append(size - patch)
            starts.append(pos)
        return [(z, y, x) for z in starts[0] for y in starts[1] for x in starts[2]]

    def __len__(self):
        return len(self.starts)

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        z, y, x = self.starts[idx]
        dz, dy, dx = self.crop_size

        raw_crop = self.raw[z:z+dz, y:y+dy, x:x+dx].astype(np.float32)
        label_crop = self.labels[z:z+dz, y:y+dy, x:x+dx].astype(np.int64)

        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor

# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from statsmodels.sandbox.panel.sandwich_covariance_generic import kernel

from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 256, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(256, 256,3, padding=1), nn.ReLU(),
            nn.Conv3d(256, 256,3, padding=1),nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv3 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.up4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.conv4 = nn.Sequential(
            nn.Conv3d(32, 32, 3, padding=1), nn.ReLU(),
            nn.Conv3d(32, 32, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(32, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        x = self.up3(x)
        x = self.conv3(x)
        x = self.up4(x)
        x = self.conv4(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.encoder = self.mae.vit
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        #print(f"[DEBUG] Input shape: {x.shape}")
        #print(f"[DEBUG] Config image_size: {self.config.image_size}, patch_size: {self.patch_size}")

        # 使用 MAE 模型并关闭 mask_ratio
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        #print(f"[DEBUG] Patch grid D×H×W = {D}×{H}×{W} → expected patches: {D*H*W}, got: {N}")

        if self.encoder.embeddings.position_embeddings.shape[1] != N + 1:
            print("⚠️ Position embeddings mismatched — reinitializing...")
            self.encoder.embeddings.position_embeddings = nn.Parameter(
                torch.zeros(1, N + 1, self.hidden_dim)
            )
            nn.init.trunc_normal_(self.encoder.embeddings.position_embeddings, std=0.02)

        if D * H * W != N:
            raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)

# train_affinity_3d.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_A_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_B_20160501.hdf",
    "/home/guilin/PycharmProjects/MAE3d/segementation_data/sample_C_20160501.hdf",
]
CHECKPOINT = "/home/guilin/PycharmProjects/MAE3d/output/vitmae3d"
CROP_SIZE = (32, 320, 320)
BATCH_SIZE = 2
NUM_EPOCHS = 300
NUM_CLASSES = 3  # z+, y+, x+ affinity
LR = 5e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return (focal_weight * bce_loss).mean()

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
config.image_size = CROP_SIZE
config.mask_ratio = 0.0
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()










#!/usr/bin/env python
# coding=utf-8

import os
import sys
import logging
import random
import torch
import json
import tifffile
import numpy as np
from torch.utils.data import Dataset
from torchvision.transforms import Compose
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import HfArgumentParser, Trainer, TrainingArguments
from vitmae3d import ViTMAEForPreTraining, ViTMAEConfig
from torch.utils.tensorboard import SummaryWriter

logger = logging.getLogger(__name__)

def compute_mean_std(root_dir, cache_file="mean_std_cache.json"):
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r") as f:
                cached = json.load(f)
            if cached.get("dir") == os.path.abspath(root_dir):
                logger.info("Loaded mean/std from cache.")
                return cached["mean"], cached["std"]
        except Exception as e:
            logger.warning(f"Failed to load mean/std cache: {e}")

    all_voxels = []
    for root, _, files in os.walk(root_dir):
        for fname in files:
            if fname.endswith(".tif"):
                fpath = os.path.join(root, fname)
                try:
                    vol = tifffile.imread(fpath).astype(np.float32)
                    all_voxels.append(vol.flatten())
                except Exception as e:
                    print(f"Warning: failed to read {fpath}: {e}")
    all_voxels = np.concatenate(all_voxels)
    mean = float(np.mean(all_voxels))
    std = float(np.std(all_voxels))

    try:
        with open(cache_file, "w") as f:
            json.dump({"dir": os.path.abspath(root_dir), "mean": mean, "std": std}, f)
    except Exception as e:
        logger.warning(f"Failed to write mean/std cache: {e}")

    return mean, std

@dataclass
class ModelArguments:
    config_path: Optional[str] = field(default=None)
    model_path: Optional[str] = field(default=None)
    mask_ratio: float = field(default=0.75)
    norm_pix_loss: bool = field(default=True)

@dataclass
class DataTrainingArguments:
    train_dir: str = field(metadata={"help": "Directory containing .tif training volumes."})
    val_dir: str = field(metadata={"help": "Directory containing .tif validation volumes."})
    max_train_samples: Optional[int] = field(default=None)
    max_eval_samples: Optional[int] = field(default=None)

@dataclass
class CustomTrainingArguments(TrainingArguments):
    base_learning_rate: float = field(
        default=1e-3, metadata={"help": "Absolute LR = base_lr * batch_size / 256"}
    )

def log_dataset_samples(dataset, writer, tag_prefix="train", num_samples=5):
    for idx in range(min(num_samples, len(dataset))):
        data = dataset[idx]["pixel_values"]  # shape: [1, D, H, W]
        image = data[data.shape[0] // 2]   # 取中间层 [H, W]
        writer.add_image(f"{tag_prefix}_slice_{idx}", image.unsqueeze(0), global_step=0)

class VolumeDataset(Dataset):
    def __init__(self, file_list,  transform=None):
        self.file_list = file_list
        self.transform = transform

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        volume = tifffile.imread(self.file_list[idx]).astype(np.float32)
#        volume = volume / 255.0
#        volume = (volume - self.mean) / self.std

        volume = torch.tensor(volume)  # [D, H, W]
        if self.transform:
            volume = self.transform(volume)
        return {"pixel_values": volume}

class RandomCrop3D:
    def __init__(self, size):
        self.size = size  # tuple: (D, H, W)

    def __call__(self, vol):
        d, h, w = vol.shape
        zd, yd, xd = self.size
        z = random.randint(0, d - zd)
        y = random.randint(0, h - yd)
        x = random.randint(0, w - xd)
        return vol[z:z+zd, y:y+yd, x:x+xd]

class Normalize3D:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, vol):
        return (vol / 255.0 - self.mean) / self.std

def collate_fn(examples):
    batch = torch.stack([ex["pixel_values"] for ex in examples])  # [B, D, H, W]
    return {"pixel_values": batch.unsqueeze(1)}  # [B, 1, D, H, W]

def gather_tif_files(root_dir):
    files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for f in filenames:
            if f.endswith(".tif"):
                files.append(os.path.join(dirpath, f))
    return files

def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, CustomTrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    logging.basicConfig(level=logging.INFO)
    logger.info("Loading data...")

#    mean, std = compute_mean_std(data_args.train_dir)
 #   logger.info(f"computed mean: {mean:.4f}, std:{std:.4f}")

    train_files = gather_tif_files(data_args.train_dir)
    val_files = gather_tif_files(data_args.val_dir)

    if data_args.max_train_samples:
        train_files = train_files[:data_args.max_train_samples]
    if data_args.max_eval_samples:
        val_files = val_files[:data_args.max_eval_samples]

    transform = Compose([
        RandomCrop3D((32, 320, 320)),
        Normalize3D(143.510583/255, 45.286453/255),
    ])

    train_dataset = VolumeDataset(train_files, transform)
    val_dataset = VolumeDataset(val_files, transform)

    sample = train_dataset[0]["pixel_values"]
    print("train_dataset[0]shape:", sample.shape)

    writer = SummaryWriter(log_dir=os.path.join(training_args.output_dir, "tb_samples"))
    log_dataset_samples(train_dataset, writer, tag_prefix="train", num_samples=5)
    log_dataset_samples(val_dataset, writer, tag_prefix="val", num_samples=3)
    writer.close()
    logger.info("✅ Wrote sample slices to TensorBoard.")

    config = ViTMAEConfig(
        image_size=(32, 320, 320),
        patch_size=(16, 16, 16),
        num_channels=1,
        hidden_size=768,
        num_hidden_layers=10,
        num_attention_heads=8,
        intermediate_size=3072,
        decoder_hidden_size=384,
        decoder_num_hidden_layers=6,
        decoder_num_attention_heads=4,
        decoder_intermediate_size=1536,
        mask_ratio=model_args.mask_ratio,
        norm_pix_loss=model_args.norm_pix_loss
    )

    if model_args.model_path:
        model = ViTMAEForPreTraining.from_pretrained(model_args.model_path, config=config)
    else:
        model = ViTMAEForPreTraining(config)

    param_report_path = os.path.join(training_args.output_dir, "model_params.txt")
    with open(param_report_path, "w") as f:
        total_params = 0
        trainable_params = 0
        f.write("Model Parameters:\n")
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad:
                trainable_params += param_count
            f.write(f"{name:60s} | shape: {str(tuple(param.shape)):20s} | requires_grad={param.requires_grad}\n")
        total_mb = total_params * 4 / 1024 / 1024  # float32 占 4 字节
        f.write(f"\n✅ Total parameters: {total_params:,} (~{total_mb:.2f} MB)\n")
        f.write(f"🟢 Trainable parameters: {trainable_params:,}\n")
        f.write(f"🟡 Non-trainable parameters: {total_params - trainable_params:,}\n")
    logger.info(f"✅ Model parameter details saved to {param_report_path}")

    total_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size
    training_args.learning_rate = training_args.base_learning_rate * total_batch_size / 256

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
    )

    logger.info("Start training...")
    trainer.train()
    trainer.save_model()

    logger.info("Evaluating...")
    metrics = trainer.evaluate()
    trainer.log_metrics("eval", metrics)
    trainer.save_metrics("eval", metrics)

if __name__ == "__main__":
    main()

#命令行：
python run_mae_3d.py \
  --train_dir ./data/train \
  --val_dir ./data/val \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \
  --num_train_epochs 100 \
  --output_dir ./output/vitmae3d \
  --logging_dir ./output/vitmae3d/logs \
  --base_learning_rate 1e-3 \
  --logging_steps 10 \
  --save_strategy="steps" \
  --save_steps=10000 \
  --eval_steps 10000 \
  --evaluation_strategy steps \
  --save_total_limit 3
