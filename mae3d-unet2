# train_affinity_3d.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_affinity_3d_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATH = "./data/sample_C_20160501.hdf"
CHECKPOINT = "./output/vitmae3d/checkpoint-100000"
CROP_SIZE = (32, 160, 160)
BATCH_SIZE = 2
NUM_EPOCHS = 100
NUM_CLASSES = 3  # z+, y+, x+ affinity
LR = 1e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return (focal_weight * bce_loss).mean()

# === Dataset ===
dataset = CREMIAffinity3DDataset(H5_PATH, crop_size=CROP_SIZE)
val_ratio = 0.1
val_size = int(len(dataset) * val_ratio)
train_size = len(dataset) - val_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()



# cremi_affinity_3d_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
import random

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 160, 160), mean=143.51 / 255, std=45.29 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.D, self.H, self.W = self.raw.shape

    def __len__(self):
        return 10000  # number of random crops per epoch

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        zd, yh, xw = self.crop_size
        z = random.randint(0, self.D - zd)
        y = random.randint(0, self.H - yh)
        x = random.randint(0, self.W - xw)

        raw_crop = self.raw[z:z+zd, y:y+yh, x:x+xw].astype(np.float32)
        label_crop = self.labels[z:z+zd, y:y+yh, x:x+xw].astype(np.int64)

        # 归一化 + 标准化
        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor




# mae3d_unet_finetune.py
import torch
import torch.nn as nn
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, kernel_size=1)

    def forward(self, x):  # x: [B, C, D, H, W]
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)  # [B, num_classes, D, H, W]

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=3):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit  # ViT-MAE 编码器部分

        self.patch_size = config.patch_size
        self.grid_size = (
            config.image_size[0] // self.patch_size[0],
            config.image_size[1] // self.patch_size[1],
            config.image_size[2] // self.patch_size[2],
        )
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(self.hidden_dim, num_classes)

    def forward(self, x):  # 输入：[B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # 去除 CLS token → [B, N, C]
        x = features.transpose(1, 2)   # [B, C, N]
        x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        return self.decoder(x)  # 输出：[B, num_classes, D, H, W]



# train_segmentation.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "./data/sample_A_20160501.hdf",
    "./data/sample_B_20160501.hdf",
    "./data/sample_C_20160501.hdf",
]
CHECKPOINT = "./output/vitmae3d/checkpoint-100000"
BATCH_SIZE = 2
NUM_EPOCHS = 50
NUM_CLASSES = 2
LR = 1e-4
LOG_DIR = "./logs_seg"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Data ===
datasets = [CREMIDataset(h5_path) for h5_path in H5_PATHS]
dataset = ConcatDataset(datasets)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Loss & Optimizer ===
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=LR)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for i, (inputs, targets) in enumerate(tqdm(dataloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)  # [B, C, H, W]
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(dataloader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

    # Optional: Save model
    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints/mae3d_unet_epoch{epoch+1}.pth")

writer.close()



# cremi_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset

class CREMIDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids", transform=None):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.transform = transform

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.label = f[label_key][()]

        assert self.raw.shape == self.label.shape, "Raw and label volume must have same shape"
        self.length = self.raw.shape[0]  # 默认按 Z 轴切片（或 patch）

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        image = self.raw[idx].astype(np.float32) / 255.0  # normalize
        label = self.label[idx].astype(np.int64)

        if self.transform:
            image, label = self.transform(image, label)

        image = torch.from_numpy(image).unsqueeze(0)  # [1, H, W]
        label = torch.from_numpy(label)              # [H, W] → 可适配 3D later

        return image, label







# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        # 加载预训练 MAE 模型
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit  # encoder 输出 shape: [B, N, hidden_size]

        self.patch_size = config.patch_size
        self.grid_size = (
            config.image_size[0] // self.patch_size[0],
            config.image_size[1] // self.patch_size[1],
            config.image_size[2] // self.patch_size[2]
        )
        self.hidden_dim = config.hidden_size

        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # 去掉 CLS token
        x = features.transpose(1, 2)  # [B, C, N]
        x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        return self.decoder(x)  # [B, num_classes, D', H', W']

# 示例：初始化模型（需指定 config 和 checkpoint 路径）
# from transformers import ViTMAEConfig
# config = ViTMAEConfig.from_pretrained("./output/vitmae3d/checkpoint-100000")
# model = MAEUNet2Segmentation("./output/vitmae3d/checkpoint-100000", config, num_classes=3)
