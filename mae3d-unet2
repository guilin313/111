import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

# === ÊÆãÂ∑ÆÂùó ===
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv3d(channels, channels, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(channels, channels, 3, padding=1),
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.relu(x + self.block(x))

# === UNet-style decoderÔºàÊîØÊåÅÊ∑±Â±Ç skipÔºâ===
class MAEUNetDecoderWithDeepSkip(nn.Module):
    def __init__(self, encoder_dim, skip_dims, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 256, 2, stride=2)
        self.conv1 = ResidualBlock(256 + skip_dims[0])

        self.up2 = nn.ConvTranspose3d(256 + skip_dims[0], 128, 2, stride=2)
        self.conv2 = ResidualBlock(128 + skip_dims[1])

        self.up3 = nn.ConvTranspose3d(128 + skip_dims[1], 64, 2, stride=2)
        self.conv3 = ResidualBlock(64 + skip_dims[2])

        self.up4 = nn.ConvTranspose3d(64 + skip_dims[2], 32, 2, stride=2)
        self.conv4 = ResidualBlock(32)

        self.out = nn.Conv3d(32, num_classes, 1)

    def forward(self, x, skips):
        x = self.up1(x)
        x = torch.cat([x, skips[0]], dim=1)
        x = self.conv1(x)

        x = self.up2(x)
        x = torch.cat([x, skips[1]], dim=1)
        x = self.conv2(x)

        x = self.up3(x)
        x = torch.cat([x, skips[2]], dim=1)
        x = self.conv3(x)

        x = self.up4(x)
        x = self.conv4(x)

        return self.out(x)

# === ‰∏ªÊ®°ÂûãÁ±ªÔºàViT-MAE + UNet decoder + skip + ÂÜªÁªì encoderÔºâ===
class MAEUNetDeepSkipSegmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=3):
        super().__init__()
        self.patch_size = tuple(config.patch_size)
        self.hidden_dim = config.hidden_size
        self.config = config

        self.mae = ViTMAEForPreTraining.from_pretrained(
            pretrained_path,
            config=config,
            ignore_mismatched_sizes=True
        )
        self.encoder = self.mae.vit

        # ‚úÖ ÂÜªÁªì ViT-MAE ÁºñÁ†ÅÂô®ÂèÇÊï∞
        for param in self.encoder.parameters():
            param.requires_grad = False

        skip_dims = [256, 128, 64]
        self.decoder = MAEUNetDecoderWithDeepSkip(
            encoder_dim=self.hidden_dim,
            skip_dims=skip_dims,
            num_classes=num_classes
        )

        self.proj3 = nn.Conv3d(self.hidden_dim, 256, 1)
        self.proj6 = nn.Conv3d(self.hidden_dim, 128, 1)
        self.proj9 = nn.Conv3d(self.hidden_dim, 64, 1)

    def forward(self, x):
        B = x.shape[0]
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        x_embed = self.encoder.embeddings(x)
        hidden_states = []
        for i, blk in enumerate(self.encoder.encoder.layer):
            x_embed = blk(x_embed)
            if i in [3, 6, 9]:
                hidden_states.append(x_embed)

        def to_3d(feat):
            feat = feat[:, 1:, :]  # remove cls token
            return feat.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)

        main = to_3d(x_embed)
        skip1 = F.interpolate(self.proj3(to_3d(hidden_states[0])), scale_factor=8.0, mode="trilinear", align_corners=False)
        skip2 = F.interpolate(self.proj6(to_3d(hidden_states[1])), scale_factor=4.0, mode="trilinear", align_corners=False)
        skip3 = F.interpolate(self.proj9(to_3d(hidden_states[2])), scale_factor=2.0, mode="trilinear", align_corners=False)

        return self.decoder(main, [skip1, skip2, skip3])



import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

# === üîπ AttentionFusion Ê®°ÂùóÔºöÂ∞Ü skip Âíå decoder ÁâπÂæÅÈÄöËøáÊ≥®ÊÑèÂäõËûçÂêà ===
class AttentionFusion(nn.Module):
    def __init__(self, main_dim, skip_dim, out_dim):
        super().__init__()
        self.q = nn.Conv3d(main_dim, out_dim, 1)
        self.k = nn.Conv3d(skip_dim, out_dim, 1)
        self.v = nn.Conv3d(skip_dim, out_dim, 1)
        self.out = nn.Conv3d(out_dim, out_dim, 1)

    def forward(self, main, skip):
        q = self.q(main)
        k = self.k(skip)
        v = self.v(skip)
        attn = torch.softmax((q * k).sum(dim=1, keepdim=True), dim=1)  # [B, 1, D, H, W]
        fused = attn * v
        return self.out(fused + main)  # ÊÆãÂ∑ÆÂä†ÂõûÊù•

# === üîπ Decoder BlockÔºåÂ∏¶‰∏äÈááÊ†∑ + attention ËûçÂêà skip + conv3d ===
class AttentionDecoderBlock(nn.Module):
    def __init__(self, in_dim, skip_dim, out_dim):
        super().__init__()
        self.upsample = nn.ConvTranspose3d(in_dim, out_dim, 2, stride=2)
        self.fusion = AttentionFusion(out_dim, skip_dim, out_dim)
        self.conv = nn.Sequential(
            nn.Conv3d(out_dim, out_dim, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_dim, out_dim, 3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip):
        x = self.upsample(x)
        x = self.fusion(x, skip)
        return self.conv(x)

# === üîπ ÂÖ®ÈÉ®Ëß£Á†ÅÂô®ÁªìÊûÑÔºà4‰∏™Èò∂ÊÆµÔºâ ===
class MAEAttentionSkipDecoder(nn.Module):
    def __init__(self, encoder_dim, skip_dims, num_classes):
        super().__init__()
        self.block1 = AttentionDecoderBlock(encoder_dim, skip_dims[0], 256)
        self.block2 = AttentionDecoderBlock(256, skip_dims[1], 128)
        self.block3 = AttentionDecoderBlock(128, skip_dims[2], 64)
        self.block4 = nn.Sequential(
            nn.ConvTranspose3d(64, 32, 2, stride=2),
            nn.ReLU(inplace=True),
            nn.Conv3d(32, 32, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.out = nn.Conv3d(32, num_classes, 1)

    def forward(self, x, skips):
        x = self.block1(x, skips[0])
        x = self.block2(x, skips[1])
        x = self.block3(x, skips[2])
        x = self.block4(x)
        return self.out(x)

# === üîπ ÊÄª‰ΩìÊ®°ÂûãÁªìÊûÑÔºöViT-MAE + Ê≥®ÊÑèÂäõ skip Ëß£Á†ÅÂô® ===
class MAEUNetAttentionSegmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=3):
        super().__init__()
        self.patch_size = tuple(config.patch_size)
        self.hidden_dim = config.hidden_size
        self.config = config

        self.mae = ViTMAEForPreTraining.from_pretrained(
            pretrained_path,
            config=config,
            ignore_mismatched_sizes=True
        )
        self.encoder = self.mae.vit

        # ‰∏≠Èó¥Â±Ç skip Êò†Â∞ÑÊäïÂΩ±
        self.proj3 = nn.Conv3d(self.hidden_dim, 256, 1)
        self.proj6 = nn.Conv3d(self.hidden_dim, 128, 1)
        self.proj9 = nn.Conv3d(self.hidden_dim, 64, 1)

        self.decoder = MAEAttentionSkipDecoder(
            encoder_dim=self.hidden_dim,
            skip_dims=[256, 128, 64],
            num_classes=num_classes
        )

    def forward(self, x):
        B = x.shape[0]
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        x_embed = self.encoder.embeddings(x)  # patch embedding
        hidden_states = []
        for i, blk in enumerate(self.encoder.encoder.layer):
            x_embed = blk(x_embed)
            if i in [3, 6, 9]:
                hidden_states.append(x_embed)

        def to_3d(feat):
            feat = feat[:, 1:, :]
            return feat.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)

        main = to_3d(x_embed)
        skip1 = F.interpolate(self.proj3(to_3d(hidden_states[0])), scale_factor=8.0, mode="trilinear", align_corners=False)
        skip2 = F.interpolate(self.proj6(to_3d(hidden_states[1])), scale_factor=4.0, mode="trilinear", align_corners=False)
        skip3 = F.interpolate(self.proj9(to_3d(hidden_states[2])), scale_factor=2.0, mode="trilinear", align_corners=False)

        return self.decoder(main, [skip1, skip2, skip3])



# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from statsmodels.sandbox.panel.sandwich_covariance_generic import kernel

from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 256, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(256, 256,3, padding=1), nn.ReLU(),
            nn.Conv3d(256, 256,3, padding=1),nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv3 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.up4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.conv4 = nn.Sequential(
            nn.Conv3d(32, 32, 3, padding=1), nn.ReLU(),
            nn.Conv3d(32, 32, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(32, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        x = self.up3(x)
        x = self.conv3(x)
        x = self.up4(x)
        x = self.conv4(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(
            pretrained_path,
            config=config,
            #ignore_mismatched_sizes=True  # ‚Üê Ëá™Âä®ÂøΩÁï• shape ‰∏ç‰∏ÄËá¥ÁöÑÂ±Ç
        )
        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.encoder = self.mae.vit
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        #print(f"[DEBUG] Input shape: {x.shape}")
        #print(f"[DEBUG] Config image_size: {self.config.image_size}, patch_size: {self.patch_size}")

        # ‰ΩøÁî® MAE Ê®°ÂûãÂπ∂ÂÖ≥Èó≠ mask_ratio
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        if D * H * W != N:
            raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)




self.mae = ViTMAEForPreTraining.from_pretrained(
    pretrained_path,
    config=config,
    ignore_mismatched_sizes=True  # ‚Üê Ëá™Âä®ÂøΩÁï• shape ‰∏ç‰∏ÄËá¥ÁöÑÂ±Ç
)

self.mae = ViTMAEForPreTraining(config)
state_dict = torch.load(os.path.join(pretrained_path, "pytorch_model.bin"), map_location="cpu")
# Âà†Èô§ position embedding Áõ∏ÂÖ≥ÁöÑ keysÔºàÈò≤Ê≠¢Â∞∫ÂØ∏‰∏çÂåπÈÖçÔºâ
for k in list(state_dict.keys()):
    if "position_embeddings" in k or "decoder_pos_embed" in k:
        print(f"‚ö†Ô∏è Removing mismatched key: {k}")
        del state_dict[k]
_ = self.mae.load_state_dict(state_dict, strict=False)


# === Logging Setup ===
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)



# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        print(f"[DEBUG] Input shape: {x.shape}")
        print(f"[DEBUG] Config image_size: {self.config.image_size}, patch_size: {self.patch_size}")

        # ‰ΩøÁî® MAE Ê®°ÂûãÂπ∂ÂÖ≥Èó≠ mask_ratio
        features = self.mae(pixel_values=x, mask_ratio=0.0).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        print(f"[DEBUG] Patch grid D√óH√óW = {D}√ó{H}√ó{W} ‚Üí expected patches: {D*H*W}, got: {N}")

        if self.mae.vit.embeddings.position_embeddings.shape[1] != N + 1:
            print("‚ö†Ô∏è Position embeddings mismatched ‚Äî reinitializing...")
            self.mae.vit.embeddings.position_embeddings = nn.Parameter(
                torch.zeros(1, N + 1, self.hidden_dim)
            )
            nn.init.trunc_normal_(self.mae.vit.embeddings.position_embeddings, std=0.02)

        if D * H * W != N:
            raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)



# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit

        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        print(f"[DEBUG] Input shape: {x.shape}")
        print(f"[DEBUG] Config image_size: {self.config.image_size}, patch_size: {self.patch_size}")

        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        print(f"[DEBUG] Patch grid D√óH√óW = {D}√ó{H}√ó{W} ‚Üí expected patches: {D*H*W}, got: {N}")

        if D * H * W != N:
            raise ValueError(f"Mismatch: input patch count {N} does not match expected {D*H*W} from image_size {self.config.image_size} and patch_size {self.patch_size}")

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)


# cremi_affinity_3d_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
import random

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 320, 320), mean=127.91 / 255, std=28 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.D, self.H, self.W = self.raw.shape

    def __len__(self):
        return 10000  # number of random crops per epoch

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        zd, yh, xw = self.crop_size  # ‚úÖ Á°Æ‰øù‰ΩøÁî®‰º†ÂÖ• crop_size
        assert zd <= self.D and yh <= self.H and xw <= self.W, "Crop size must fit inside the volume"

        z = random.randint(0, self.D - zd)
        y = random.randint(0, self.H - yh)
        x = random.randint(0, self.W - xw)

        raw_crop = self.raw[z:z+zd, y:y+yh, x:x+xw].astype(np.float32)
        label_crop = self.labels[z:z+zd, y:y+yh, x:x+xw].astype(np.int64)

        assert raw_crop.shape == self.crop_size, f"Crop shape mismatch: got {raw_crop.shape}, expected {self.crop_size}"

        # ÂΩí‰∏ÄÂåñ + Ê†áÂáÜÂåñ
        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor



# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit

        self.patch_size = config.patch_size
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)
        self.config = config

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # [B, N, C]

        N = features.shape[1]  # num_patches = D*H*W
        Pd, Ph, Pw = self.patch_size
        D = self.config.image_size[0] // Pd
        H = self.config.image_size[1] // Ph
        W = self.config.image_size[2] // Pw

        assert D * H * W == N, f"Mismatch: got {N} patches but expected {D}x{H}x{W}"

        x = features.transpose(1, 2).reshape(B, self.hidden_dim, D, H, W)
        return self.decoder(x)



# train_affinity_3d.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_affinity_3d_dataset import CREMIAffinity3DDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "./data/sample_A_20160501.hdf",
    "./data/sample_B_20160501.hdf",
    "./data/sample_C_20160501.hdf",
]
CHECKPOINT = "./output/vitmae3d/checkpoint-100000"
CROP_SIZE = (32, 160, 160)
BATCH_SIZE = 2
NUM_EPOCHS = 100
NUM_CLASSES = 3  # z+, y+, x+ affinity
LR = 1e-4
LOG_DIR = "./logs_affinity3d"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Focal Loss ===
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, input, target):
        bce_loss = self.bce(input, target)
        prob = torch.sigmoid(input)
        focal_weight = (1 - prob) ** self.gamma * target + prob ** self.gamma * (1 - target)
        return (focal_weight * bce_loss).mean()

# === Dataset ===
dataset_a = CREMIAffinity3DDataset(H5_PATHS[0], crop_size=CROP_SIZE)
dataset_b = CREMIAffinity3DDataset(H5_PATHS[1], crop_size=CROP_SIZE)
dataset_c = CREMIAffinity3DDataset(H5_PATHS[2], crop_size=CROP_SIZE)

val_ratio = 0.1
val_size = int(len(dataset_c) * val_ratio)
train_size = len(dataset_c) - val_size
train_c, val_c = random_split(dataset_c, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_dataset = ConcatDataset([dataset_a, dataset_b, train_c])
val_dataset = val_c

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Optimizer, Scheduler, Loss ===
optimizer = optim.AdamW(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
criterion = FocalLoss(gamma=2.0)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Accuracy Metric ===
def affinity_accuracy(preds, targets):
    preds = torch.sigmoid(preds) > 0.5
    targets = targets > 0.5
    correct = (preds == targets).float().mean()
    return correct.item()

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]"):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    writer.add_scalar("Loss/train", avg_train_loss, epoch)

    # === Validation ===
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    with torch.no_grad():
        for inputs, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]"):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
            val_acc += affinity_accuracy(outputs, targets)

    avg_val_loss = val_loss / len(val_loader)
    avg_val_acc = val_acc / len(val_loader)
    writer.add_scalar("Loss/val", avg_val_loss, epoch)
    writer.add_scalar("Metric/AffinityAcc", avg_val_acc, epoch)
    writer.add_scalar("LR", scheduler.get_last_lr()[0], epoch)
    print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AffinityAcc: {avg_val_acc:.4f}")

    scheduler.step()

    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints_affinity3d", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints_affinity3d/mae3d_unet_epoch{epoch+1}.pth")

writer.close()




# cremi_affinity_3d_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
import random

class CREMIAffinity3DDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids",
                 crop_size=(32, 160, 160), mean=143.51 / 255, std=45.29 / 255):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.crop_size = crop_size
        self.mean = mean
        self.std = std

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.labels = f[label_key][()]

        assert self.raw.shape == self.labels.shape, "Raw and label volume must have same shape"
        self.D, self.H, self.W = self.raw.shape

    def __len__(self):
        return 10000  # number of random crops per epoch

    def compute_affinities(self, ids):
        affinities = np.zeros((3, *ids.shape), dtype=np.uint8)
        affinities[0, :-1] = (ids[1:] == ids[:-1])     # z+
        affinities[1, :, :-1] = (ids[:, 1:] == ids[:, :-1])  # y+
        affinities[2, :, :, :-1] = (ids[:, :, 1:] == ids[:, :, :-1])  # x+
        return affinities

    def __getitem__(self, idx):
        zd, yh, xw = self.crop_size
        z = random.randint(0, self.D - zd)
        y = random.randint(0, self.H - yh)
        x = random.randint(0, self.W - xw)

        raw_crop = self.raw[z:z+zd, y:y+yh, x:x+xw].astype(np.float32)
        label_crop = self.labels[z:z+zd, y:y+yh, x:x+xw].astype(np.int64)

        # ÂΩí‰∏ÄÂåñ + Ê†áÂáÜÂåñ
        raw_crop = (raw_crop / 255.0 - self.mean) / self.std
        affinity = self.compute_affinities(label_crop)

        raw_tensor = torch.from_numpy(raw_crop).unsqueeze(0)  # [1, D, H, W]
        aff_tensor = torch.from_numpy(affinity.astype(np.float32))  # [3, D, H, W]
        return raw_tensor, aff_tensor




# mae3d_unet_finetune.py
import torch
import torch.nn as nn
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, kernel_size=3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, kernel_size=3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, kernel_size=1)

    def forward(self, x):  # x: [B, C, D, H, W]
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)  # [B, num_classes, D, H, W]

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=3):
        super().__init__()
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit  # ViT-MAE ÁºñÁ†ÅÂô®ÈÉ®ÂàÜ

        self.patch_size = config.patch_size
        self.grid_size = (
            config.image_size[0] // self.patch_size[0],
            config.image_size[1] // self.patch_size[1],
            config.image_size[2] // self.patch_size[2],
        )
        self.hidden_dim = config.hidden_size
        self.decoder = MAEUNet2Decoder(self.hidden_dim, num_classes)

    def forward(self, x):  # ËæìÂÖ•Ôºö[B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # ÂéªÈô§ CLS token ‚Üí [B, N, C]
        x = features.transpose(1, 2)   # [B, C, N]
        x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        return self.decoder(x)  # ËæìÂá∫Ôºö[B, num_classes, D, H, W]



# train_segmentation.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from cremi_dataset import CREMIDataset
from mae3d_unet_finetune import MAEUNet2Segmentation
from transformers import ViTMAEConfig

# === Config ===
H5_PATHS = [
    "./data/sample_A_20160501.hdf",
    "./data/sample_B_20160501.hdf",
    "./data/sample_C_20160501.hdf",
]
CHECKPOINT = "./output/vitmae3d/checkpoint-100000"
BATCH_SIZE = 2
NUM_EPOCHS = 50
NUM_CLASSES = 2
LR = 1e-4
LOG_DIR = "./logs_seg"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === Data ===
datasets = [CREMIDataset(h5_path) for h5_path in H5_PATHS]
dataset = ConcatDataset(datasets)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)

# === Model ===
config = ViTMAEConfig.from_pretrained(CHECKPOINT)
model = MAEUNet2Segmentation(CHECKPOINT, config, num_classes=NUM_CLASSES).to(DEVICE)

# === Loss & Optimizer ===
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=LR)

# === Logging ===
writer = SummaryWriter(LOG_DIR)

# === Training Loop ===
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for i, (inputs, targets) in enumerate(tqdm(dataloader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")):
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)  # [B, C, H, W]
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(dataloader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

    # Optional: Save model
    if (epoch + 1) % 10 == 0:
        os.makedirs("./checkpoints", exist_ok=True)
        torch.save(model.state_dict(), f"./checkpoints/mae3d_unet_epoch{epoch+1}.pth")

writer.close()



# cremi_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset

class CREMIDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids", transform=None):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.transform = transform

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.label = f[label_key][()]

        assert self.raw.shape == self.label.shape, "Raw and label volume must have same shape"
        self.length = self.raw.shape[0]  # ÈªòËÆ§Êåâ Z ËΩ¥ÂàáÁâáÔºàÊàñ patchÔºâ

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        image = self.raw[idx].astype(np.float32) / 255.0  # normalize
        label = self.label[idx].astype(np.int64)

        if self.transform:
            image, label = self.transform(image, label)

        image = torch.from_numpy(image).unsqueeze(0)  # [1, H, W]
        label = torch.from_numpy(label)              # [H, W] ‚Üí ÂèØÈÄÇÈÖç 3D later

        return image, label







# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉ MAE Ê®°Âûã
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit  # encoder ËæìÂá∫ shape: [B, N, hidden_size]

        self.patch_size = config.patch_size
        self.grid_size = (
            config.image_size[0] // self.patch_size[0],
            config.image_size[1] // self.patch_size[1],
            config.image_size[2] // self.patch_size[2]
        )
        self.hidden_dim = config.hidden_size

        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # ÂéªÊéâ CLS token
        x = features.transpose(1, 2)  # [B, C, N]
        x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        return self.decoder(x)  # [B, num_classes, D', H', W']

# Á§∫‰æãÔºöÂàùÂßãÂåñÊ®°ÂûãÔºàÈúÄÊåáÂÆö config Âíå checkpoint Ë∑ØÂæÑÔºâ
# from transformers import ViTMAEConfig
# config = ViTMAEConfig.from_pretrained("./output/vitmae3d/checkpoint-100000")
# model = MAEUNet2Segmentation("./output/vitmae3d/checkpoint-100000", config, num_classes=3)
