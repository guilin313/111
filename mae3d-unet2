# cremi_dataset.py
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset

class CREMIDataset(Dataset):
    def __init__(self, h5_path, raw_key="volumes/raw", label_key="volumes/labels/neuron_ids", transform=None):
        super().__init__()
        self.h5_path = h5_path
        self.raw_key = raw_key
        self.label_key = label_key
        self.transform = transform

        with h5py.File(h5_path, "r") as f:
            self.raw = f[raw_key][()]
            self.label = f[label_key][()]

        assert self.raw.shape == self.label.shape, "Raw and label volume must have same shape"
        self.length = self.raw.shape[0]  # 默认按 Z 轴切片（或 patch）

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        image = self.raw[idx].astype(np.float32) / 255.0  # normalize
        label = self.label[idx].astype(np.int64)

        if self.transform:
            image, label = self.transform(image, label)

        image = torch.from_numpy(image).unsqueeze(0)  # [1, H, W]
        label = torch.from_numpy(label)              # [H, W] → 可适配 3D later

        return image, label







# mae3d_unet_finetune.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from vitmae3d import ViTMAEForPreTraining

class MAEUNet2Decoder(nn.Module):
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        self.up1 = nn.ConvTranspose3d(encoder_dim, 128, kernel_size=2, stride=2)
        self.conv1 = nn.Sequential(
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU(),
            nn.Conv3d(128, 128, 3, padding=1), nn.ReLU()
        )
        self.up2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)
        self.conv2 = nn.Sequential(
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Conv3d(64, 64, 3, padding=1), nn.ReLU()
        )
        self.out = nn.Conv3d(64, num_classes, 1)

    def forward(self, x):
        x = self.up1(x)
        x = self.conv1(x)
        x = self.up2(x)
        x = self.conv2(x)
        return self.out(x)

class MAEUNet2Segmentation(nn.Module):
    def __init__(self, pretrained_path, config, num_classes=2):
        super().__init__()
        # 加载预训练 MAE 模型
        self.mae = ViTMAEForPreTraining.from_pretrained(pretrained_path, config=config)
        self.encoder = self.mae.vit  # encoder 输出 shape: [B, N, hidden_size]

        self.patch_size = config.patch_size
        self.grid_size = (
            config.image_size[0] // self.patch_size[0],
            config.image_size[1] // self.patch_size[1],
            config.image_size[2] // self.patch_size[2]
        )
        self.hidden_dim = config.hidden_size

        self.decoder = MAEUNet2Decoder(encoder_dim=self.hidden_dim, num_classes=num_classes)

    def forward(self, x):  # x: [B, 1, D, H, W]
        B = x.shape[0]
        features = self.encoder(pixel_values=x).last_hidden_state  # [B, N+1, C]
        features = features[:, 1:, :]  # 去掉 CLS token
        x = features.transpose(1, 2)  # [B, C, N]
        x = x.reshape(B, self.hidden_dim, *self.grid_size)  # [B, C, D, H, W]
        return self.decoder(x)  # [B, num_classes, D', H', W']

# 示例：初始化模型（需指定 config 和 checkpoint 路径）
# from transformers import ViTMAEConfig
# config = ViTMAEConfig.from_pretrained("./output/vitmae3d/checkpoint-100000")
# model = MAEUNet2Segmentation("./output/vitmae3d/checkpoint-100000", config, num_classes=3)
