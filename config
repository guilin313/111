(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/mae2dpretrain$ /bin/bash /home/guilin/PycharmProjects/mae2dpretrain/script.sh
Traceback (most recent call last):
  File "/home/guilin/PycharmProjects/mae2dpretrain/run_mae.py", line 46, in <module>
    check_min_version("4.51.0.dev0")
  File "/home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/transformers/utils/__init__.py", line 295, in check_min_version
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.50.0.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
(pt12) guilin@guilin-System-Product-Name:~/PycharmProjects/mae2dpretrain$ pip install git+https://github.com/huggingface/transformers
DEPRECATION: Loading egg at /home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-7p29hjy6
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-7p29hjy6


https://blog.csdn.net/weixin_45508265/article/details/130287752

from transformers import ViTMAEConfig
from vitmae3d import ViTMAEForPreTraining

config = ViTMAEConfig(
    image_size=(32, 320, 320),
    patch_size=(4, 40, 40),
    num_channels=1,
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    decoder_hidden_size=512,
    decoder_num_hidden_layers=4,
    decoder_num_attention_heads=8,
    decoder_intermediate_size=2048,
    mask_ratio=0.75,
    norm_pix_loss=True
)
model = ViTMAEForPreTraining(config)


class Args:
    input_size = (32, 320, 320)
    patch_size = (4, 16, 16)
    in_chans = 1
    mask_ratio = 0.75
    encoder_embed_dim = 768
    encoder_depth = 12
    encoder_num_heads = 12
    decoder_embed_dim = 512
    decoder_depth = 8
    decoder_num_heads = 8
    pos_embed_type = 'sincos'
    patchembed = "PatchEmbed3D"
    batch_size = 4
    lr = 1e-4
    weight_decay = 1e-4
    num_epochs = 100

args = Args()


class ViT3DEncoder(nn.Module):
    def __init__(self, patch_size, in_chans, embed_dim, depth, num_heads, embed_layer=PatchEmbed3D):
        super().__init__()
        self.patch_embed = embed_layer(img_size=(32, 320, 320),  # 根据你的数据调整
                                       patch_size=patch_size,
                                       in_chans=in_chans,
                                       embed_dim=embed_dim)
        self.num_patches = self.patch_embed.num_patches
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x, pos_embed):
        x = self.patch_embed(x)  # [B, N, C]
        x += pos_embed
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return x

class ViT3DDecoder(nn.Module):
    def __init__(self, patch_size, num_classes, embed_dim, depth, num_heads):
        super().__init__()
        self.num_patches = (32 // patch_size[0]) * (320 // patch_size[1]) * (320 // patch_size[2])
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        x = self.head(x)
        return x
