Model Parameters:
vit.embeddings.cls_token                                     | shape: (1, 1, 768)          | requires_grad=True
vit.embeddings.position_embeddings                           | shape: (1, 801, 768)        | requires_grad=False
vit.embeddings.patch_embeddings.projection.weight            | shape: (768, 1, 16, 16, 16) | requires_grad=True
vit.embeddings.patch_embeddings.projection.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.0.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.0.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.0.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.0.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.0.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.0.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.0.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.0.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.1.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.1.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.1.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.1.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.1.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.1.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.1.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.1.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.2.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.2.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.2.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.2.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.2.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.2.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.2.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.2.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.3.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.3.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.3.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.3.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.3.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.3.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.3.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.3.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.4.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.4.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.4.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.4.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.4.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.4.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.4.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.4.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.5.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.5.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.5.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.5.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.5.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.5.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.5.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.5.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.6.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.6.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.6.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.6.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.6.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.6.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.6.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.6.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.7.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.7.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.7.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.7.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.7.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.7.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.7.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.7.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.8.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.8.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.8.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.8.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.8.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.8.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.8.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.8.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.attention.attention.query.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.9.attention.attention.query.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.attention.attention.key.weight           | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.9.attention.attention.key.bias             | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.attention.attention.value.weight         | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.9.attention.attention.value.bias           | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.attention.output.dense.weight            | shape: (768, 768)           | requires_grad=True
vit.encoder.layer.9.attention.output.dense.bias              | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.intermediate.dense.weight                | shape: (3072, 768)          | requires_grad=True
vit.encoder.layer.9.intermediate.dense.bias                  | shape: (3072,)              | requires_grad=True
vit.encoder.layer.9.output.dense.weight                      | shape: (768, 3072)          | requires_grad=True
vit.encoder.layer.9.output.dense.bias                        | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.layernorm_before.weight                  | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.layernorm_before.bias                    | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.layernorm_after.weight                   | shape: (768,)               | requires_grad=True
vit.encoder.layer.9.layernorm_after.bias                     | shape: (768,)               | requires_grad=True
vit.layernorm.weight                                         | shape: (768,)               | requires_grad=True
vit.layernorm.bias                                           | shape: (768,)               | requires_grad=True
decoder.mask_token                                           | shape: (1, 1, 384)          | requires_grad=True
decoder.decoder_pos_embed                                    | shape: (1, 801, 384)        | requires_grad=False
decoder.decoder_embed.weight                                 | shape: (384, 768)           | requires_grad=True
decoder.decoder_embed.bias                                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.0.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.0.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.0.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.0.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.0.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.0.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.0.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.0.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.1.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.1.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.1.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.1.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.1.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.1.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.1.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.1.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.2.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.2.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.2.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.2.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.2.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.2.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.2.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.2.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.3.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.3.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.3.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.3.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.3.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.3.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.3.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.3.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.4.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.4.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.4.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.4.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.4.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.4.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.4.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.4.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.attention.attention.query.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.5.attention.attention.query.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.attention.attention.key.weight      | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.5.attention.attention.key.bias        | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.attention.attention.value.weight    | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.5.attention.attention.value.bias      | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.attention.output.dense.weight       | shape: (384, 384)           | requires_grad=True
decoder.decoder_layers.5.attention.output.dense.bias         | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.intermediate.dense.weight           | shape: (1536, 384)          | requires_grad=True
decoder.decoder_layers.5.intermediate.dense.bias             | shape: (1536,)              | requires_grad=True
decoder.decoder_layers.5.output.dense.weight                 | shape: (384, 1536)          | requires_grad=True
decoder.decoder_layers.5.output.dense.bias                   | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.layernorm_before.weight             | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.layernorm_before.bias               | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.layernorm_after.weight              | shape: (384,)               | requires_grad=True
decoder.decoder_layers.5.layernorm_after.bias                | shape: (384,)               | requires_grad=True
decoder.decoder_norm.weight                                  | shape: (384,)               | requires_grad=True
decoder.decoder_norm.bias                                    | shape: (384,)               | requires_grad=True
decoder.decoder_pred.weight                                  | shape: (4096, 384)          | requires_grad=True
decoder.decoder_pred.bias                                    | shape: (4096,)              | requires_grad=True

✅ Total parameters: 87,470,464 (~333.67 MB)
🟢 Trainable parameters: 86,547,712
🟡 Non-trainable parameters: 922,752





***** eval metrics *****
  epoch                   =      100.0
  eval_loss               =     0.1382
  eval_runtime            = 0:00:24.54
  eval_samples_per_second =    305.553
  eval_steps_per_second   =     38.214



pip install --no-cache-dir git+https://github.com/huggingface/transformers


(pt12) guilin@guilin-System-Product-Name:~$ pip uninstall transformers -y
Found existing installation: transformers 4.50.0
Uninstalling transformers-4.50.0:
  Successfully uninstalled transformers-4.50.0
(pt12) guilin@guilin-System-Product-Name:~$ pip install git+https://github.com/huggingface/transformers
DEPRECATION: Loading egg at /home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6715u09u
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u


(pt12) guilin@guilin-System-Product-Name:~$ pip install git+https://github.com/huggingface/transformers
DEPRECATION: Loading egg at /home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6715u09u
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u
  error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)
  error: 4069 bytes of body are still expected
  fetch-pack: unexpected disconnect while reading sideband packet
  fatal: early EOF
  fatal: fetch-pack: invalid index-pack output
  error: subprocess-exited-with-error
  
  × git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u did not run successfully.
  │ exit code: 128
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.

[notice] A new release of pip is available: 24.3.1 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
error: subprocess-exited-with-error

× git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u did not run successfully.
│ exit code: 128
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.





https://blog.csdn.net/weixin_45508265/article/details/130287752

from transformers import ViTMAEConfig
from vitmae3d import ViTMAEForPreTraining

config = ViTMAEConfig(
    image_size=(32, 320, 320),
    patch_size=(4, 40, 40),
    num_channels=1,
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    decoder_hidden_size=512,
    decoder_num_hidden_layers=4,
    decoder_num_attention_heads=8,
    decoder_intermediate_size=2048,
    mask_ratio=0.75,
    norm_pix_loss=True
)
model = ViTMAEForPreTraining(config)


class Args:
    input_size = (32, 320, 320)
    patch_size = (4, 16, 16)
    in_chans = 1
    mask_ratio = 0.75
    encoder_embed_dim = 768
    encoder_depth = 12
    encoder_num_heads = 12
    decoder_embed_dim = 512
    decoder_depth = 8
    decoder_num_heads = 8
    pos_embed_type = 'sincos'
    patchembed = "PatchEmbed3D"
    batch_size = 4
    lr = 1e-4
    weight_decay = 1e-4
    num_epochs = 100

args = Args()


class ViT3DEncoder(nn.Module):
    def __init__(self, patch_size, in_chans, embed_dim, depth, num_heads, embed_layer=PatchEmbed3D):
        super().__init__()
        self.patch_embed = embed_layer(img_size=(32, 320, 320),  # 根据你的数据调整
                                       patch_size=patch_size,
                                       in_chans=in_chans,
                                       embed_dim=embed_dim)
        self.num_patches = self.patch_embed.num_patches
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x, pos_embed):
        x = self.patch_embed(x)  # [B, N, C]
        x += pos_embed
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return x

class ViT3DDecoder(nn.Module):
    def __init__(self, patch_size, num_classes, embed_dim, depth, num_heads):
        super().__init__()
        self.num_patches = (32 // patch_size[0]) * (320 // patch_size[1]) * (320 // patch_size[2])
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        x = self.head(x)
        return x
