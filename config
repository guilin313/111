***** eval metrics *****
  epoch                   =      100.0
  eval_loss               =     0.1382
  eval_runtime            = 0:00:24.54
  eval_samples_per_second =    305.553
  eval_steps_per_second   =     38.214



pip install --no-cache-dir git+https://github.com/huggingface/transformers


(pt12) guilin@guilin-System-Product-Name:~$ pip uninstall transformers -y
Found existing installation: transformers 4.50.0
Uninstalling transformers-4.50.0:
  Successfully uninstalled transformers-4.50.0
(pt12) guilin@guilin-System-Product-Name:~$ pip install git+https://github.com/huggingface/transformers
DEPRECATION: Loading egg at /home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6715u09u
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u


(pt12) guilin@guilin-System-Product-Name:~$ pip install git+https://github.com/huggingface/transformers
DEPRECATION: Loading egg at /home/guilin/miniconda3/envs/pt12/lib/python3.12/site-packages/huggingface_hub-0.29.2-py3.8.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6715u09u
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u
  error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)
  error: 4069 bytes of body are still expected
  fetch-pack: unexpected disconnect while reading sideband packet
  fatal: early EOF
  fatal: fetch-pack: invalid index-pack output
  error: subprocess-exited-with-error
  
  × git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u did not run successfully.
  │ exit code: 128
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.

[notice] A new release of pip is available: 24.3.1 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
error: subprocess-exited-with-error

× git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6715u09u did not run successfully.
│ exit code: 128
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.





https://blog.csdn.net/weixin_45508265/article/details/130287752

from transformers import ViTMAEConfig
from vitmae3d import ViTMAEForPreTraining

config = ViTMAEConfig(
    image_size=(32, 320, 320),
    patch_size=(4, 40, 40),
    num_channels=1,
    hidden_size=768,
    num_attention_heads=12,
    num_hidden_layers=12,
    decoder_hidden_size=512,
    decoder_num_hidden_layers=4,
    decoder_num_attention_heads=8,
    decoder_intermediate_size=2048,
    mask_ratio=0.75,
    norm_pix_loss=True
)
model = ViTMAEForPreTraining(config)


class Args:
    input_size = (32, 320, 320)
    patch_size = (4, 16, 16)
    in_chans = 1
    mask_ratio = 0.75
    encoder_embed_dim = 768
    encoder_depth = 12
    encoder_num_heads = 12
    decoder_embed_dim = 512
    decoder_depth = 8
    decoder_num_heads = 8
    pos_embed_type = 'sincos'
    patchembed = "PatchEmbed3D"
    batch_size = 4
    lr = 1e-4
    weight_decay = 1e-4
    num_epochs = 100

args = Args()


class ViT3DEncoder(nn.Module):
    def __init__(self, patch_size, in_chans, embed_dim, depth, num_heads, embed_layer=PatchEmbed3D):
        super().__init__()
        self.patch_embed = embed_layer(img_size=(32, 320, 320),  # 根据你的数据调整
                                       patch_size=patch_size,
                                       in_chans=in_chans,
                                       embed_dim=embed_dim)
        self.num_patches = self.patch_embed.num_patches
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x, pos_embed):
        x = self.patch_embed(x)  # [B, N, C]
        x += pos_embed
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return x

class ViT3DDecoder(nn.Module):
    def __init__(self, patch_size, num_classes, embed_dim, depth, num_heads):
        super().__init__()
        self.num_patches = (32 // patch_size[0]) * (320 // patch_size[1]) * (320 // patch_size[2])
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))
        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        x = self.head(x)
        return x
